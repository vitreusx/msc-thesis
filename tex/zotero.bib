@article{abdarReviewUncertaintyQuantification2021,
  title = {A {{Review}} of {{Uncertainty Quantification}} in {{Deep Learning}}: {{Techniques}}, {{Applications}} and {{Challenges}}},
  shorttitle = {A {{Review}} of {{Uncertainty Quantification}} in {{Deep Learning}}},
  author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
  year = {2021},
  month = dec,
  journal = {Information Fusion},
  volume = {76},
  eprint = {2011.06225},
  primaryclass = {cs},
  pages = {243--297},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.05.008},
  urldate = {2024-07-06},
  abstract = {Uncertainty quantification (UQ) plays a pivotal role in reduction of uncertainties during both optimization and decision making processes. It can be applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two most widely-used UQ methods in the literature. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning. Moreover, we also investigate the application of these methods in reinforcement learning (RL). Then, we outline a few important applications of UQ methods. Finally, we briefly highlight the fundamental research challenges faced by UQ methods and discuss the future research directions in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/PRBS9RQG/Abdar et al. - 2021 - A Review of Uncertainty Quantification in Deep Lea.pdf;/home/james/Zotero/storage/RG57VJCU/2011.html}
}

@misc{adaptiveagentteamHumanTimescaleAdaptationOpenEnded2023,
  title = {Human-{{Timescale Adaptation}} in an {{Open-Ended Task Space}}},
  author = {Adaptive Agent Team and Bauer, Jakob and Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Bhoopchand, Avishkar and {Bradley-Schmieg}, Nathalie and Chang, Michael and Clay, Natalie and Collister, Adrian and Dasagi, Vibhavari and Gonzalez, Lucy and Gregor, Karol and Hughes, Edward and Kashem, Sheleem and {Loks-Thompson}, Maria and Openshaw, Hannah and {Parker-Holder}, Jack and Pathak, Shreya and {Perez-Nieves}, Nicolas and Rakicevic, Nemanja and Rockt{\"a}schel, Tim and Schroecker, Yannick and Sygnowski, Jakub and Tuyls, Karl and York, Sarah and Zacherl, Alexander and Zhang, Lei},
  year = {2023},
  month = jan,
  number = {arXiv:2301.07608},
  eprint = {2301.07608},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-24},
  abstract = {Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/james/Documents/zotero/Adaptive Agent Team et al_2023_Human-Timescale Adaptation in an Open-Ended Task Space.pdf;/home/james/Zotero/storage/HJZWBTN6/2301.html}
}

@misc{agarwalDeepReinforcementLearning2022,
  title = {Deep {{Reinforcement Learning}} at the {{Edge}} of the {{Statistical Precipice}}},
  author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G.},
  year = {2022},
  month = jan,
  number = {arXiv:2108.13264},
  eprint = {2108.13264},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.13264},
  urldate = {2024-11-28},
  abstract = {Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/james/Zotero/storage/XXICT83J/Agarwal et al. - 2022 - Deep Reinforcement Learning at the Edge of the Statistical Precipice.pdf;/home/james/Zotero/storage/ZT6G3NAW/2108.html}
}

@article{ahnCanNotSay,
  title = {Do {{As I Can}}, {{Not As I Say}}: {{Grounding Language}} in {{Robotic Affordances}}},
  author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
  abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's ``hands and eyes,'' while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website, the video, and open sourced code in a tabletop domain can be found at say-can.github.io.},
  langid = {english},
  file = {/home/james/Documents/zotero/Ahn et al_Do As I Can, Not As I Say2.pdf}
}

@misc{aitchisonAtari5DistillingArcade2022,
  title = {Atari-5: {{Distilling}} the {{Arcade Learning Environment}} down to {{Five Games}}},
  shorttitle = {Atari-5},
  author = {Aitchison, Matthew and Sweetser, Penny and Hutter, Marcus},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02019},
  eprint = {2210.02019},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.02019},
  urldate = {2024-08-17},
  abstract = {The Arcade Learning Environment (ALE) has become an essential benchmark for assessing the performance of reinforcement learning algorithms. However, the computational cost of generating results on the entire 57-game dataset limits ALE's use and makes the reproducibility of many results infeasible. We propose a novel solution to this problem in the form of a principled methodology for selecting small but representative subsets of environments within a benchmark suite. We applied our method to identify a subset of five ALE games, called Atari-5, which produces 57-game median score estimates within 10\% of their true values. Extending the subset to 10-games recovers 80\% of the variance for log-scores for all games within the 57-game set. We show this level of compression is possible due to a high degree of correlation between many of the games in ALE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/3ZTJGMVB/Aitchison et al. - 2022 - Atari-5 Distilling the Arcade Learning Environmen.pdf;/home/james/Zotero/storage/R2847YWA/2210.html}
}

@misc{aminSurveyExplorationMethods2021,
  title = {A {{Survey}} of {{Exploration Methods}} in {{Reinforcement Learning}}},
  author = {Amin, Susan and Gomrokchi, Maziar and Satija, Harsh and {van Hoof}, Herke and Precup, Doina},
  year = {2021},
  month = sep,
  number = {arXiv:2109.00157},
  eprint = {2109.00157},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.00157},
  urldate = {2023-03-04},
  abstract = {Exploration is an essential component of reinforcement learning algorithms, where agents need to learn how to predict and control unknown and often stochastic environments. Reinforcement learning agents depend crucially on exploration to obtain informative data for the learning process as the lack of enough information could hinder effective learning. In this article, we provide a survey of modern exploration methods in (Sequential) reinforcement learning, as well as a taxonomy of exploration methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Exploration,RL},
  file = {/home/james/Documents/zotero/Amin et al_2021_A Survey of Exploration Methods in Reinforcement Learning2.pdf;/home/james/Zotero/storage/JH5V9XUG/2109.html}
}

@misc{andrychowiczHindsightExperienceReplay2018,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  year = {2018},
  month = feb,
  number = {arXiv:1707.01495},
  eprint = {1707.01495},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.01495},
  urldate = {2023-03-04},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Andrychowicz et al_2018_Hindsight Experience Replay2.pdf;/home/james/Zotero/storage/XFDQXZ3C/1707.html}
}

@misc{andrychowiczWhatMattersOnPolicy2020,
  title = {What {{Matters In On-Policy Reinforcement Learning}}? {{A Large-Scale Empirical Study}}},
  shorttitle = {What {{Matters In On-Policy Reinforcement Learning}}?},
  author = {Andrychowicz, Marcin and Raichuk, Anton and Sta{\'n}czyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, L{\'e}onard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and Gelly, Sylvain and Bachem, Olivier},
  year = {2020},
  month = jun,
  number = {arXiv:2006.05990},
  eprint = {2006.05990},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.05990},
  urldate = {2024-07-06},
  abstract = {In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement {$>$}50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/WUZ623UB/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf;/home/james/Zotero/storage/MEE9AR58/2006.html}
}

@misc{aubretSurveyIntrinsicMotivation2019,
  title = {A Survey on Intrinsic Motivation in Reinforcement Learning},
  author = {Aubret, Arthur and Matignon, Laetitia and Hassas, Salima},
  year = {2019},
  month = nov,
  number = {arXiv:1908.06976},
  eprint = {1908.06976},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.06976},
  urldate = {2023-03-04},
  abstract = {The reinforcement learning (RL) research area is very active, with an important number of new contributions; especially considering the emergent field of deep RL (DRL). However a number of scientific and technical challenges still need to be addressed, amongst which we can mention the ability to abstract actions or the difficulty to explore the environment which can be addressed by intrinsic motivation (IM). In this article, we provide a survey on the role of intrinsic motivation in DRL. We categorize the different kinds of intrinsic motivations and detail for each category, its advantages and limitations with respect to the mentioned challenges. Additionnally, we conduct an in-depth investigation of substantial current research questions, that are currently under study or not addressed at all in the considered research area of DRL. We choose to survey these research works, from the perspective of learning how to achieve tasks. We suggest then, that solving current challenges could lead to a larger developmental architecture which may tackle most of the tasks. We describe this developmental architecture on the basis of several building blocks composed of a RL algorithm and an IM module compressing information.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Exploration,RL},
  file = {/home/james/Documents/zotero/Aubret et al_2019_A survey on intrinsic motivation in reinforcement learning2.pdf;/home/james/Zotero/storage/QTDIVN9U/1908.html}
}

@misc{AutoencoderBetaVAELilLog,
  title = {From {{Autoencoder}} to {{Beta-VAE}} {\textbar} {{Lil}}'{{Log}}},
  urldate = {2023-03-04},
  howpublished = {https://lilianweng.github.io/posts/2018-08-12-vae/}
}

@misc{badiaAgent57OutperformingAtari2020,
  title = {Agent57: {{Outperforming}} the {{Atari Human Benchmark}}},
  shorttitle = {Agent57},
  author = {Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Blundell, Charles},
  year = {2020},
  month = mar,
  number = {arXiv:2003.13350},
  eprint = {2003.13350},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.13350},
  urldate = {2023-07-21},
  abstract = {Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Badia et al_2020_Agent57.pdf;/home/james/Zotero/storage/S3TXE54Z/2003.html}
}

@misc{baiFFHQUVNormalizedFacial2022,
  title = {{{FFHQ-UV}}: {{Normalized Facial UV-Texture Dataset}} for {{3D Face Reconstruction}}},
  shorttitle = {{{FFHQ-UV}}},
  author = {Bai, Haoran and Kang, Di and Zhang, Haoxian and Pan, Jinshan and Bao, Linchao},
  year = {2022},
  month = nov,
  number = {arXiv:2211.13874},
  eprint = {2211.13874},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-26},
  abstract = {We present a large-scale facial UV-texture dataset that contains over 50,000 high-quality texture UV-maps with even illuminations, neutral expressions, and cleaned facial regions, which are desired characteristics for rendering realistic 3D face models under different lighting conditions. The dataset is derived from a large-scale face image dataset namely FFHQ, with the help of our fully automatic and robust UV-texture production pipeline. Our pipeline utilizes the recent advances in StyleGAN-based facial image editing approaches to generate multi-view normalized face images from single-image inputs. An elaborated UV-texture extraction, correction, and completion procedure is then applied to produce high-quality UV-maps from the normalized face images. Compared with existing UV-texture datasets, our dataset has more diverse and higher-quality texture maps. We further train a GAN-based texture decoder as the nonlinear texture basis for parametric fitting based 3D face reconstruction. Experiments show that our method improves the reconstruction accuracy over state-of-the-art approaches, and more importantly, produces high-quality texture maps that are ready for realistic renderings. The dataset, code, and pre-trained texture decoder are publicly available at https://github.com/csbhr/FFHQ-UV.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Bai et al_2022_FFHQ-UV2.pdf}
}

@misc{balduzziShatteredGradientsProblem2018,
  title = {The {{Shattered Gradients Problem}}: {{If}} Resnets Are the Answer, Then What Is the Question?},
  shorttitle = {The {{Shattered Gradients Problem}}},
  author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, J. P. and Ma, Kurt Wan-Duo and McWilliams, Brian},
  year = {2018},
  month = jun,
  number = {arXiv:1702.08591},
  eprint = {1702.08591},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-02},
  abstract = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite wellchosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new ``looks linear'' (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/7NJH6IKR/Balduzzi et al. - 2018 - The Shattered Gradients Problem If resnets are th.pdf}
}

@misc{ballAugmentedWorldModels2021,
  title = {Augmented {{World Models Facilitate Zero-Shot Dynamics Generalization From}} a {{Single Offline Environment}}},
  author = {Ball, Philip J. and Lu, Cong and {Parker-Holder}, Jack and Roberts, Stephen},
  year = {2021},
  month = aug,
  number = {arXiv:2104.05632},
  eprint = {2104.05632},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-30},
  abstract = {Reinforcement learning from large-scale offline datasets provides us with the ability to learn policies without potentially unsafe or impractical exploration. Significant progress has been made in the past few years in dealing with the challenge of correcting for differing behavior between the data collection and learned policies. However, little attention has been paid to potentially changing dynamics when transferring a policy to the online setting, where performance can be up to 90\% reduced for existing methods. In this paper we address this problem with Augmented World Models (AugWM). We augment a learned dynamics model with simple transformations that seek to capture potential changes in physical properties of the robot, leading to more robust policies. We not only train our policy in this new setting, but also provide it with the sampled augmentation as a context, allowing it to adapt to changes in the environment. At test time we learn the context in a self-supervised fashion by approximating the augmentation which corresponds to the new environment. We rigorously evaluate our approach on over 100 different changed dynamics settings, and show that this simple approach can significantly improve the zero-shot generalization of a recent state-of-the-art baseline, often achieving successful policies where the baseline fails.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Ball et al_2021_Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a.pdf;/home/james/Zotero/storage/5C6CWU9A/2104.html}
}

@misc{baninoPonderNetLearningPonder2021,
  title = {{{PonderNet}}: {{Learning}} to {{Ponder}}},
  shorttitle = {{{PonderNet}}},
  author = {Banino, Andrea and Balaguer, Jan and Blundell, Charles},
  year = {2021},
  month = sep,
  number = {arXiv:2107.05407},
  eprint = {2107.05407},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.05407},
  urldate = {2023-03-04},
  abstract = {In standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Banino et al_2021_PonderNet.pdf;/home/james/Zotero/storage/BIGHNRWQ/2107.html}
}

@misc{barhamPathwaysAsynchronousDistributed2022,
  title = {Pathways: {{Asynchronous Distributed Dataflow}} for {{ML}}},
  shorttitle = {Pathways},
  author = {Barham, Paul and Chowdhery, Aakanksha and Dean, Jeff and Ghemawat, Sanjay and Hand, Steven and Hurt, Dan and Isard, Michael and Lim, Hyeontaek and Pang, Ruoming and Roy, Sudip and Saeta, Brennan and Schuh, Parker and Sepassi, Ryan and Shafey, Laurent El and Thekkath, Chandramohan A. and Wu, Yonghui},
  year = {2022},
  month = mar,
  number = {arXiv:2203.12533},
  eprint = {2203.12533},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.12533},
  urldate = {2023-03-04},
  abstract = {We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous parallel computations on thousands of accelerators while coordinating data transfers over their dedicated interconnects. Pathways makes use of a novel asynchronous distributed dataflow design that lets the control plane execute in parallel despite dependencies in the data plane. This design, with careful engineering, allows Pathways to adopt a single-controller model that makes it easier to express complex new parallelism patterns. We demonstrate that Pathways can achieve performance parity ({\textasciitilde}100\% accelerator utilization) with state-of-the-art systems when running SPMD computations over 2048 TPUs, while also delivering throughput comparable to the SPMD case for Transformer models that are pipelined across 16 stages, or sharded across two islands of accelerators connected over a data center network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Barham et al_2022_Pathways.pdf;/home/james/Zotero/storage/2SEIQCCS/2203.html}
}

@misc{beckSurveyMetaReinforcementLearning2023,
  title = {A {{Survey}} of {{Meta-Reinforcement Learning}}},
  author = {Beck, Jacob and Vuorio, Risto and Liu, Evan Zheran and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
  year = {2023},
  month = jan,
  number = {arXiv:2301.08028},
  eprint = {2301.08028},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.08028},
  urldate = {2023-03-12},
  abstract = {While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Meta-Learning,RL},
  file = {/home/james/Documents/zotero/Beck et al_2023_A Survey of Meta-Reinforcement Learning2.pdf;/home/james/Zotero/storage/H5CITUHK/2301.html}
}

@misc{bellemareArcadeLearningEnvironment2013,
  title = {The {{Arcade Learning Environment}}: {{An Evaluation Platform}} for {{General Agents}}},
  shorttitle = {The {{Arcade Learning Environment}}},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  year = {2013},
  month = jun,
  number = {arXiv:1207.4708},
  eprint = {1207.4708},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1207.4708},
  urldate = {2024-11-23},
  abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/Zotero/storage/LSTXSQPY/Bellemare et al. - 2013 - The Arcade Learning Environment An Evaluation Platform for General Agents.pdf;/home/james/Zotero/storage/ZUSAMYHY/1207.html}
}

@misc{bellemareDistributionalPerspectiveReinforcement2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  number = {arXiv:1707.06887},
  eprint = {1707.06887},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-07-23},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf;/home/james/Zotero/storage/GS93URP8/1707.html}
}

@misc{bengioFlowNetworkBased2021,
  title = {Flow {{Network}} Based {{Generative Models}} for {{Non-Iterative Diverse Candidate Generation}}},
  author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  year = {2021},
  month = nov,
  number = {arXiv:2106.04399},
  eprint = {2106.04399},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-13},
  abstract = {This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Bengio et al_2021_Flow Network based Generative Models for Non-Iterative Diverse Candidate.pdf;/home/james/Zotero/storage/V68YRKPG/2106.html}
}

@misc{blattmannSemiParametricNeuralImage2022,
  title = {Semi-{{Parametric Neural Image Synthesis}}},
  author = {Blattmann, Andreas and Rombach, Robin and Oktay, Kaan and M{\"u}ller, Jonas and Ommer, Bj{\"o}rn},
  year = {2022},
  month = oct,
  number = {arXiv:2204.11824},
  eprint = {2204.11824},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.11824},
  urldate = {2023-03-04},
  abstract = {Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Much of this success is due to the scalability of these architectures and hence caused by a dramatic increase in model complexity and in the computational resources invested in training these models. Our work questions the underlying paradigm of compressing large training data into ever growing parametric representations. We rather present an orthogonal, semi-parametric approach. We complement comparably small diffusion or autoregressive models with a separate image database and a retrieval strategy. During training we retrieve a set of nearest neighbors from this external database for each training instance and condition the generative model on these informative samples. While the retrieval approach is providing the (local) content, the model is focusing on learning the composition of scenes based on this content. As demonstrated by our experiments, simply swapping the database for one with different contents transfers a trained model post-hoc to a novel domain. The evaluation shows competitive performance on tasks which the generative model has not been trained on, such as class-conditional synthesis, zero-shot stylization or text-to-image synthesis without requiring paired text-image data. With negligible memory and computational overhead for the external database and retrieval we can significantly reduce the parameter count of the generative model and still outperform the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Blattmann et al_2022_Semi-Parametric Neural Image Synthesis.pdf;/home/james/Zotero/storage/LKG4RMSL/2204.html}
}

@misc{borgeaudImprovingLanguageModels2022,
  title = {Improving Language Models by Retrieving from Trillions of Tokens},
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and van den Driessche, George and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
  year = {2022},
  month = feb,
  number = {arXiv:2112.04426},
  eprint = {2112.04426},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.04426},
  urldate = {2023-03-04},
  abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\${\textbackslash}times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Borgeaud et al_2022_Improving language models by retrieving from trillions of tokens.pdf;/home/james/Zotero/storage/3UF9VFYJ/2112.html}
}

@misc{brockHighPerformanceLargeScaleImage2021,
  title = {High-{{Performance Large-Scale Image Recognition Without Normalization}}},
  author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
  year = {2021},
  month = feb,
  number = {arXiv:2102.06171},
  eprint = {2102.06171},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-14},
  abstract = {Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5\%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2\%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/FWJJXRFD/Brock et al. - 2021 - High-Performance Large-Scale Image Recognition Wit.pdf;/home/james/Zotero/storage/2WKBB47U/2102.html}
}

@misc{brohanRT1RoboticsTransformer2023,
  title = {{{RT-1}}: {{Robotics Transformer}} for {{Real-World Control}} at {{Scale}}},
  shorttitle = {{{RT-1}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  year = {2023},
  month = aug,
  number = {arXiv:2212.06817},
  eprint = {2212.06817},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.06817},
  urldate = {2024-11-24},
  abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Zotero/storage/PS6RZ8ZS/Brohan et al. - 2023 - RT-1 Robotics Transformer for Real-World Control at Scale.pdf}
}

@misc{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  year = {2021},
  month = may,
  number = {arXiv:2104.13478},
  eprint = {2104.13478},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.13478},
  urldate = {2023-03-04},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Bronstein et al_2021_Geometric Deep Learning.pdf;/home/james/Zotero/storage/ID2BKJZA/2104.html}
}

@misc{byravanImaginedValueGradients2019,
  title = {Imagined {{Value Gradients}}: {{Model-Based Policy Optimization}} with {{Transferable Latent Dynamics Models}}},
  shorttitle = {Imagined {{Value Gradients}}},
  author = {Byravan, Arunkumar and Springenberg, Jost Tobias and Abdolmaleki, Abbas and Hafner, Roland and Neunert, Michael and Lampe, Thomas and Siegel, Noah and Heess, Nicolas and Riedmiller, Martin},
  year = {2019},
  month = oct,
  number = {arXiv:1910.04142},
  eprint = {1910.04142},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.04142},
  urldate = {2024-11-26},
  abstract = {Humans are masters at quickly learning many complex tasks, relying on an approximate understanding of the dynamics of their environments. In much the same way, we would like our learning agents to quickly adapt to new tasks. In this paper, we explore how model-based Reinforcement Learning (RL) can facilitate transfer to new tasks. We develop an algorithm that learns an action-conditional, predictive model of expected future observations, rewards and values from which a policy can be derived by following the gradient of the estimated value along imagined trajectories. We show how robust policy optimization can be achieved in robot manipulation tasks even with approximate models that are learned directly from vision and proprioception. We evaluate the efficacy of our approach in a transfer learning scenario, re-using previously learned models on tasks with different reward structures and visual distractors, and show a significant improvement in learning speed compared to strong off-policy baselines. Videos with results can be found at https://sites.google.com/view/ivg-corl19},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/james/Zotero/storage/F8894SKH/Byravan et al. - 2019 - Imagined Value Gradients Model-Based Policy Optimization with Transferable Latent Dynamics Models.pdf;/home/james/Zotero/storage/EIPU6XZ9/1910.html}
}

@misc{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  year = {2021},
  month = may,
  number = {arXiv:2104.14294},
  eprint = {2104.14294},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.14294},
  urldate = {2023-03-04},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Caron et al_2021_Emerging Properties in Self-Supervised Vision Transformers.pdf;/home/james/Zotero/storage/6SMUUSXE/2104.html}
}

@misc{charpentierPosteriorNetworkUncertainty2020,
  title = {Posterior {{Network}}: {{Uncertainty Estimation}} without {{OOD Samples}} via {{Density-Based Pseudo-Counts}}},
  shorttitle = {Posterior {{Network}}},
  author = {Charpentier, Bertrand and Z{\"u}gner, Daniel and G{\"u}nnemann, Stephan},
  year = {2020},
  month = oct,
  number = {arXiv:2006.09239},
  eprint = {2006.09239},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.09239},
  urldate = {2024-07-09},
  abstract = {Accurate estimation of aleatoric and epistemic uncertainty is crucial to build safe and reliable systems. Traditional approaches, such as dropout and ensemble methods, estimate uncertainty by sampling probability predictions from different submodels, which leads to slow uncertainty estimation at inference time. Recent works address this drawback by directly predicting parameters of prior distributions over the probability predictions with a neural network. While this approach has demonstrated accurate uncertainty estimation, it requires defining arbitrary target parameters for in-distribution data and makes the unrealistic assumption that out-of-distribution (OOD) data is known at training time. In this work we propose the Posterior Network (PostNet), which uses Normalizing Flows to predict an individual closed-form posterior distribution over predicted probabilites for any input sample. The posterior distributions learned by PostNet accurately reflect uncertainty for in- and out-of-distribution data -- without requiring access to OOD data at training time. PostNet achieves state-of-the art results in OOD detection and in uncertainty calibration under dataset shifts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/2GDXUYI8/Charpentier et al. - 2020 - Posterior Network Uncertainty Estimation without .pdf;/home/james/Zotero/storage/FBLDAEM6/2006.html}
}

@misc{chenDecisionTransformerReinforcement2021,
  title = {Decision {{Transformer}}: {{Reinforcement Learning}} via {{Sequence Modeling}}},
  shorttitle = {Decision {{Transformer}}},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01345},
  eprint = {2106.01345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.01345},
  urldate = {2023-03-04},
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Chen et al_2021_Decision Transformer.pdf;/home/james/Zotero/storage/MJ5XQL7Y/2106.html}
}

@misc{chenExploringSimpleSiamese2020,
  title = {Exploring {{Simple Siamese Representation Learning}}},
  author = {Chen, Xinlei and He, Kaiming},
  year = {2020},
  month = nov,
  number = {arXiv:2011.10566},
  eprint = {2011.10566},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.10566},
  urldate = {2024-11-24},
  abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/9FVXCAVE/Chen and He - 2020 - Exploring Simple Siamese Representation Learning.pdf;/home/james/Zotero/storage/5PPK88RE/2011.html}
}

@misc{chenRandomizedEnsembledDouble2021,
  title = {Randomized {{Ensembled Double Q-Learning}}: {{Learning Fast Without}} a {{Model}}},
  shorttitle = {Randomized {{Ensembled Double Q-Learning}}},
  author = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
  year = {2021},
  month = mar,
  number = {arXiv:2101.05982},
  eprint = {2101.05982},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.05982},
  urldate = {2024-08-05},
  abstract = {Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio {$>>$} 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio {$>>$} 1.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/IA993PZ3/Chen et al. - 2021 - Randomized Ensembled Double Q-Learning Learning F.pdf;/home/james/Zotero/storage/92T8ZRHT/2101.html}
}

@inproceedings{chopraLearningSimilarityMetric2005,
  title = {Learning a Similarity Metric Discriminatively, with Application to Face Verification},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
  year = {2005},
  month = jun,
  volume = {1},
  pages = {539-546 vol. 1},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2005.202},
  urldate = {2024-11-24},
  abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
  keywords = {Artificial neural networks,Character generation,Drives,Face recognition,Glass,Robustness,Spatial databases,Support vector machine classification,Support vector machines,System testing},
  file = {/home/james/Zotero/storage/IUFWXFXS/1467314.html}
}

@misc{chuaDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}} in a {{Handful}} of {{Trials}} Using {{Probabilistic Dynamics Models}}},
  author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  year = {2018},
  month = nov,
  number = {arXiv:1805.12114},
  eprint = {1805.12114},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.12114},
  urldate = {2023-03-04},
  abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Chua et al_2018_Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics.pdf;/home/james/Zotero/storage/G256PNY5/1805.html}
}

@misc{chungThinkerLearningPlan2023,
  title = {Thinker: {{Learning}} to {{Plan}} and {{Act}}},
  shorttitle = {Thinker},
  author = {Chung, Stephen and Anokhin, Ivan and Krueger, David},
  year = {2023},
  month = jul,
  number = {arXiv:2307.14993},
  eprint = {2307.14993},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-30},
  abstract = {We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. The algorithm's generality opens a new research direction on how a world model can be used in reinforcement learning and how planning can be seamlessly integrated into an agent's decision-making process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6,I.2.8,I.5.1},
  file = {/home/james/Documents/zotero/Chung et al_2023_Thinker.pdf;/home/james/Zotero/storage/LP32C936/2307.html}
}

@misc{colasAutotelicAgentsIntrinsically2022,
  title = {Autotelic {{Agents}} with {{Intrinsically Motivated Goal-Conditioned Reinforcement Learning}}: A {{Short Survey}}},
  shorttitle = {Autotelic {{Agents}} with {{Intrinsically Motivated Goal-Conditioned Reinforcement Learning}}},
  author = {Colas, C{\'e}dric and Karch, Tristan and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  year = {2022},
  month = jul,
  number = {arXiv:2012.09830},
  eprint = {2012.09830},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.09830},
  urldate = {2023-07-22},
  abstract = {Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by \$autotelic\$ \$agents\$: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning (RL) methods has been leading to the emergence of a new field: \$developmental\$ \$reinforcement\$ \$learning\$. Developmental RL is concerned with the use of deep RL algorithms to tackle a developmental problem -- the \$intrinsically\$ \$motivated\$ \$acquisition\$ \$of\$ \$open\$-\$ended\$ \$repertoires\$ \$of\$ \$skills\$. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard RL algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental RL and proposes a computational framework based on goal-conditioned RL to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Colas et al_2022_Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement.pdf;/home/james/Zotero/storage/GWIVJHAR/2012.html}
}

@misc{cranmerDiscoveringSymbolicModels2020,
  title = {Discovering {{Symbolic Models}} from {{Deep Learning}} with {{Inductive Biases}}},
  author = {Cranmer, Miles and {Sanchez-Gonzalez}, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
  year = {2020},
  month = nov,
  number = {arXiv:2006.11287},
  eprint = {2006.11287},
  primaryclass = {astro-ph, physics:physics, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11287},
  urldate = {2023-03-04},
  abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Cranmer et al_2020_Discovering Symbolic Models from Deep Learning with Inductive Biases.pdf;/home/james/Zotero/storage/WEAUEDQ9/2006.html}
}

@misc{cundySequenceMatchImitationLearning2023,
  title = {{{SequenceMatch}}: {{Imitation Learning}} for {{Autoregressive Sequence Modelling}} with {{Backtracking}}},
  shorttitle = {{{SequenceMatch}}},
  author = {Cundy, Chris and Ermon, Stefano},
  year = {2023},
  month = jun,
  number = {arXiv:2306.05426},
  eprint = {2306.05426},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.05426},
  urldate = {2023-07-20},
  abstract = {In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented without adversarial training or major architectural changes. We identify the SequenceMatch-\${\textbackslash}chi{\textasciicircum}2\$ divergence as a more suitable training objective for autoregressive models which are used for generation. We show that empirically, SequenceMatch training leads to improvements over MLE on text generation with language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Cundy_Ermon_2023_SequenceMatch.pdf;/home/james/Zotero/storage/W5CCE5MW/2306.html}
}

@article{deboerTutorialCrossEntropyMethod2005,
  title = {A {{Tutorial}} on the {{Cross-Entropy Method}}},
  author = {{de Boer}, Pieter-Tjerk and Kroese, Dirk P. and Mannor, Shie and Rubinstein, Reuven Y.},
  year = {2005},
  month = feb,
  journal = {Annals of Operations Research},
  volume = {134},
  number = {1},
  pages = {19--67},
  issn = {1572-9338},
  doi = {10.1007/s10479-005-5724-z},
  urldate = {2024-11-28},
  abstract = {The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modifications, and discuss applications in combinatorial optimization and machine learning.},
  langid = {english},
  keywords = {cross-entropy method,machine learning,Monte-Carlo simulation,randomized optimization,rare events}
}

@misc{DebuggingDeepModelbased,
  title = {Debugging {{Deep Model-based Reinforcement Learning Systems}}},
  urldate = {2023-07-09},
  abstract = {Things I have learned in 3 years of a young, and generally tricky research field.},
  howpublished = {https://www.natolambert.com/writing/debugging-mbrl},
  keywords = {Model-based RL},
  file = {/home/james/Zotero/storage/LB6EJNJ4/debugging-mbrl.html}
}

@article{dengDreamerProReconstructionFreeModelBased,
  title = {{{DreamerPro}}: {{Reconstruction-Free Model-Based Reinforcement Learning}} with {{Prototypical Representations}}},
  author = {Deng, Fei and Jang, Ingook and Ahn, Sungjin},
  abstract = {Reconstruction-based Model-Based Reinforcement Learning (MBRL) agents, such as Dreamer, often fail to discard task-irrelevant visual distractions that are prevalent in natural scenes. In this paper, we propose a reconstruction-free MBRL agent, called DreamerPro, that can enhance robustness to distractions. Motivated by the recent success of prototypical representations, a noncontrastive self-supervised learning approach in computer vision, DreamerPro combines Dreamer with prototypes. In order for the prototypes to benefit temporal dynamics learning in MBRL, we propose to additionally learn the prototypes from the recurrent states of the world model, thereby distilling temporal structures from past observations and actions into the prototypes. Experiments on the DeepMind Control suite show that DreamerPro achieves better overall performance than state-of-the-art contrastive MBRL agents when there are complex background distractions, and maintains similar performance as Dreamer in standard tasks where contrastive MBRL agents can perform much worse.},
  langid = {english},
  file = {/home/james/Documents/zotero/Deng et al_DreamerPro.pdf}
}

@misc{detoneSuperPointSelfSupervisedInterest2018,
  title = {{{SuperPoint}}: {{Self-Supervised Interest Point Detection}} and {{Description}}},
  shorttitle = {{{SuperPoint}}},
  author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2018},
  month = apr,
  number = {arXiv:1712.07629},
  eprint = {1712.07629},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.07629},
  urldate = {2023-06-08},
  abstract = {This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/DeTone et al_2018_SuperPoint.pdf;/home/james/Zotero/storage/C9YLBJ8L/1712.html}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-03-04},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,LMs,Transformers},
  file = {/home/james/Documents/zotero/Devlin et al_2019_BERT.pdf;/home/james/Zotero/storage/EGGQPIYZ/1810.html}
}

@inproceedings{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  urldate = {2024-11-24},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  file = {/home/james/Zotero/storage/CDNTKPCA/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf}
}

@misc{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  year = {2021},
  month = jun,
  number = {arXiv:2105.05233},
  eprint = {2105.05233},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.05233},
  urldate = {2023-03-04},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Dhariwal_Nichol_2021_Diffusion Models Beat GANs on Image Synthesis.pdf;/home/james/Zotero/storage/3XZ3MV5I/2105.html}
}

@misc{dohareMaintainingPlasticityDeep2024,
  title = {Maintaining {{Plasticity}} in {{Deep Continual Learning}}},
  author = {Dohare, Shibhansh and {Hernandez-Garcia}, J. Fernando and Rahman, Parash and Mahmood, A. Rupam and Sutton, Richard S.},
  year = {2024},
  month = apr,
  number = {arXiv:2306.13812},
  eprint = {2306.13812},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.13812},
  urldate = {2024-08-25},
  abstract = {Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to learn on new examples, a phenomenon called loss of plasticity. We provide direct demonstrations of loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89\% accuracy on an early task down to 77\%, about the level of a linear network, on the 2000th task. Loss of plasticity occurred with a wide range of deep network architectures, optimizers, activation functions, batch normalization, dropout, but was substantially eased by L2-regularization, particularly when combined with weight perturbation. Further, we introduce a new algorithm -- continual backpropagation -- which slightly modifies conventional backpropagation to reinitialize a small fraction of less-used units after each example and appears to maintain plasticity indefinitely.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/436VFTZ9/Dohare et al. - 2024 - Maintaining Plasticity in Deep Continual Learning.pdf;/home/james/Zotero/storage/CJZ2BYU6/2306.html}
}

@misc{dorkaDynamicUpdatetoDataRatio2023,
  title = {Dynamic {{Update-to-Data Ratio}}: {{Minimizing World Model Overfitting}}},
  shorttitle = {Dynamic {{Update-to-Data Ratio}}},
  author = {Dorka, Nicolai and Welschehold, Tim and Burgard, Wolfram},
  year = {2023},
  month = mar,
  number = {arXiv:2303.10144},
  eprint = {2303.10144},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.10144},
  urldate = {2024-10-28},
  abstract = {Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari \$100\$k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search which is not feasible for many applications. Our method eliminates the need to set the UTD hyperparameter by hand and even leads to a higher robustness with regard to other learning-related hyperparameters further reducing the amount of necessary tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/QYEQFJ65/Dorka et al. - 2023 - Dynamic Update-to-Data Ratio Minimizing World Model Overfitting.pdf;/home/james/Zotero/storage/W3H9549Z/2303.html}
}

@inproceedings{doroSampleEfficientReinforcementLearning2022,
  title = {Sample-{{Efficient Reinforcement Learning}} by {{Breaking}} the {{Replay Ratio Barrier}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {D'Oro, Pierluca and Schwarzer, Max and Nikishin, Evgenii and Bacon, Pierre-Luc and Bellemare, Marc G. and Courville, Aaron},
  year = {2022},
  month = sep,
  urldate = {2024-07-06},
  abstract = {Increasing the replay ratio, the number of updates of an agent's parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.},
  langid = {english},
  file = {/home/james/Zotero/storage/56KZMTMH/D'Oro et al. - 2022 - Sample-Efficient Reinforcement Learning by Breakin.pdf}
}

@misc{duGuidingPretrainingReinforcement2023,
  title = {Guiding {{Pretraining}} in {{Reinforcement Learning}} with {{Large Language Models}}},
  author = {Du, Yuqing and Watkins, Olivia and Wang, Zihan and Colas, C{\'e}dric and Darrell, Trevor and Abbeel, Pieter and Gupta, Abhishek and Andreas, Jacob},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06692},
  eprint = {2302.06692},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-22},
  abstract = {Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually match or improve performance on a range of downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Du et al_2023_Guiding Pretraining in Reinforcement Learning with Large Language Models.pdf;/home/james/Zotero/storage/WEU9I7SX/2302.html}
}

@misc{duSurveyVisionLanguagePreTrained2022,
  title = {A {{Survey}} of {{Vision-Language Pre-Trained Models}}},
  author = {Du, Yifan and Liu, Zikang and Li, Junyi and Zhao, Wayne Xin},
  year = {2022},
  month = jul,
  number = {arXiv:2202.10936},
  eprint = {2202.10936},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.10936},
  urldate = {2023-03-04},
  abstract = {As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,CV,Foundation Models},
  file = {/home/james/Documents/zotero/Du et al_2022_A Survey of Vision-Language Pre-Trained Models.pdf;/home/james/Zotero/storage/YRCDANSA/2202.html}
}

@misc{dwaracherlaEfficientExplorationLLMs2024,
  title = {Efficient {{Exploration}} for {{LLMs}}},
  author = {Dwaracherla, Vikranth and Asghari, Seyed Mohammad and Hao, Botao and Van Roy, Benjamin},
  year = {2024},
  month = jun,
  number = {arXiv:2402.00396},
  eprint = {2402.00396},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.00396},
  urldate = {2024-07-06},
  abstract = {We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/james/Zotero/storage/S5RAW6CN/Dwaracherla et al. - 2024 - Efficient Exploration for LLMs.pdf;/home/james/Zotero/storage/2SY3MXSW/2402.html}
}

@misc{ecoffetGoExploreNewApproach2021,
  title = {Go-{{Explore}}: A {{New Approach}} for {{Hard-Exploration Problems}}},
  shorttitle = {Go-{{Explore}}},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2021},
  month = feb,
  number = {arXiv:1901.10995},
  eprint = {1901.10995},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.10995},
  urldate = {2023-03-04},
  abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of "superhuman" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Ecoffet et al_2021_Go-Explore.pdf;/home/james/Zotero/storage/26P92VJ3/1901.html}
}

@misc{EfficientTransformersSurvey,
  title = {Efficient {{Transformers}}: {{A Survey}} {\textbar} {{ACM Computing Surveys}}},
  urldate = {2023-03-04},
  howpublished = {https://dl.acm.org/doi/full/10.1145/3530811?casa\_token=oHt4AHAX\_DUAAAAA\%3AH1T8xGV\_vC5Nu0nL8xA5rza60XWYyz7X3\_2R1-N1DpBk1fZ8eTCcnKJPdKRiShUuIVF1FR9LEb0}
}

@misc{eldanTinyStoriesHowSmall2023,
  title = {{{TinyStories}}: {{How Small Can Language Models Be}} and {{Still Speak Coherent English}}?},
  shorttitle = {{{TinyStories}}},
  author = {Eldan, Ronen and Li, Yuanzhi},
  year = {2023},
  month = may,
  number = {arXiv:2305.07759},
  eprint = {2305.07759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07759},
  urldate = {2023-07-16},
  abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Eldan_Li_2023_TinyStories.pdf;/home/james/Zotero/storage/KKU466I7/2305.html}
}

@misc{ellisDreamCoderGrowingGeneralizable2020,
  title = {{{DreamCoder}}: {{Growing}} Generalizable, Interpretable Knowledge with Wake-Sleep {{Bayesian}} Program Learning},
  shorttitle = {{{DreamCoder}}},
  author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and {Sable-Meyer}, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and {Solar-Lezama}, Armando and Tenenbaum, Joshua B.},
  year = {2020},
  month = jun,
  number = {arXiv:2006.08381},
  eprint = {2006.08381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.08381},
  urldate = {2023-03-04},
  abstract = {Expert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep'' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Ellis et al_2020_DreamCoder.pdf;/home/james/Zotero/storage/65NHH4PL/2006.html}
}

@misc{emmonsRvSWhatEssential2022,
  title = {{{RvS}}: {{What}} Is {{Essential}} for {{Offline RL}} via {{Supervised Learning}}?},
  shorttitle = {{{RvS}}},
  author = {Emmons, Scott and Eysenbach, Benjamin and Kostrikov, Ilya and Levine, Sergey},
  year = {2022},
  month = may,
  number = {arXiv:2112.10751},
  eprint = {2112.10751},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10751},
  urldate = {2023-03-08},
  abstract = {Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin "RvS learning"). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Emmons et al_2022_RvS.pdf;/home/james/Zotero/storage/ADDL8RSA/2112.html}
}

@misc{esserTamingTransformersHighResolution2021,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Bj{\"o}rn},
  year = {2021},
  month = jun,
  number = {arXiv:2012.09841},
  eprint = {2012.09841},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.09841},
  urldate = {2023-03-04},
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Esser et al_2021_Taming Transformers for High-Resolution Image Synthesis.pdf;/home/james/Zotero/storage/895X3SJ4/2012.html}
}

@misc{eysenbachDiversityAllYou2018,
  title = {Diversity Is {{All You Need}}: {{Learning Skills}} without a {{Reward Function}}},
  shorttitle = {Diversity Is {{All You Need}}},
  author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  year = {2018},
  month = oct,
  number = {arXiv:1802.06070},
  eprint = {1802.06070},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.06070},
  urldate = {2024-07-06},
  abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/james/Zotero/storage/3YR2UCKE/Eysenbach et al. - 2018 - Diversity is All You Need Learning Skills without.pdf;/home/james/Zotero/storage/YVN2I9K2/1802.html}
}

@misc{eysenbachMaximumEntropyRL2022,
  title = {Maximum {{Entropy RL}} ({{Provably}}) {{Solves Some Robust RL Problems}}},
  author = {Eysenbach, Benjamin and Levine, Sergey},
  year = {2022},
  month = may,
  number = {arXiv:2103.06257},
  eprint = {2103.06257},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.06257},
  urldate = {2023-03-04},
  abstract = {Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that maximum entropy (MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be used to learn policies that are robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL is a simple robust RL method with appealing formal guarantees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Eysenbach_Levine_2022_Maximum Entropy RL (Provably) Solves Some Robust RL Problems.pdf;/home/james/Zotero/storage/9CRJHI4B/2103.html}
}

@misc{farebrotherStopRegressingTraining2024,
  title = {Stop {{Regressing}}: {{Training Value Functions}} via {{Classification}} for {{Scalable Deep RL}}},
  shorttitle = {Stop {{Regressing}}},
  author = {Farebrother, Jesse and Orbay, Jordi and Vuong, Quan and Ta{\"i}ga, Adrien Ali and Chebotar, Yevgen and Xiao, Ted and Irpan, Alex and Levine, Sergey and Castro, Pablo Samuel and Faust, Aleksandra and Kumar, Aviral and Agarwal, Rishabh},
  year = {2024},
  month = mar,
  number = {arXiv:2403.03950},
  eprint = {2403.03950},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03950},
  urldate = {2024-07-06},
  abstract = {Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/8N8S5PPV/Farebrother et al. - 2024 - Stop Regressing Training Value Functions via Clas.pdf;/home/james/Zotero/storage/NPJTL2ZS/2403.html}
}

@misc{farquharTreeQNATreeCDifferentiable2018,
  title = {{{TreeQN}} and {{ATreeC}}: {{Differentiable Tree-Structured Models}} for {{Deep Reinforcement Learning}}},
  shorttitle = {{{TreeQN}} and {{ATreeC}}},
  author = {Farquhar, Gregory and Rockt{\"a}schel, Tim and Igl, Maximilian and Whiteson, Shimon},
  year = {2018},
  month = mar,
  number = {arXiv:1710.11417},
  eprint = {1710.11417},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.11417},
  urldate = {2023-04-02},
  abstract = {Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions. TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate Q-values. We also propose ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end, such that the learned model is optimised for its actual use in the tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a box-pushing task, as well as n-step DQN and value prediction networks (Oh et al. 2017) on multiple Atari games. Furthermore, we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Farquhar et al_2018_TreeQN and ATreeC.pdf;/home/james/Zotero/storage/4JU2SQAH/1710.html}
}

@misc{fortunatoNoisyNetworksExploration2019,
  title = {Noisy {{Networks}} for {{Exploration}}},
  author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
  year = {2019},
  month = jul,
  number = {arXiv:1706.10295},
  eprint = {1706.10295},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.10295},
  urldate = {2024-07-06},
  abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and \${\textbackslash}epsilon\$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/QK3I2G27/Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf;/home/james/Zotero/storage/VYV9II9Y/1706.html}
}

@misc{francois-lavetHowDiscountDeep2016,
  title = {How to {{Discount Deep Reinforcement Learning}}: {{Towards New Dynamic Strategies}}},
  shorttitle = {How to {{Discount Deep Reinforcement Learning}}},
  author = {{Fran{\c c}ois-Lavet}, Vincent and Fonteneau, Raphael and Ernst, Damien},
  year = {2016},
  month = jan,
  number = {arXiv:1512.02011},
  eprint = {1512.02011},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.02011},
  urldate = {2024-11-24},
  abstract = {Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/TWXWRW7G/François-Lavet et al. - 2016 - How to Discount Deep Reinforcement Learning Towards New Dynamic Strategies.pdf;/home/james/Zotero/storage/QP5YHIDA/1512.html}
}

@article{francois-lavetIntroductionDeepReinforcement2018,
  title = {An {{Introduction}} to {{Deep Reinforcement Learning}}},
  author = {{Francois-Lavet}, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  year = {2018},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {11},
  number = {3-4},
  eprint = {1811.12560},
  primaryclass = {cs, stat},
  pages = {219--354},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000071},
  urldate = {2023-03-04},
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,RL,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Francois-Lavet et al_2018_An Introduction to Deep Reinforcement Learning.pdf;/home/james/Zotero/storage/X6GDUDDX/1811.html}
}

@misc{fransPowderworldPlatformUnderstanding2022,
  title = {Powderworld: {{A Platform}} for {{Understanding Generalization}} via {{Rich Task Distributions}}},
  shorttitle = {Powderworld},
  author = {Frans, Kevin and Isola, Phillip},
  year = {2022},
  month = nov,
  number = {arXiv:2211.13051},
  eprint = {2211.13051},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.13051},
  urldate = {2023-03-04},
  abstract = {One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating challenges distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models and certain reinforcement learning agents, yet may inhibit learning in high-variance environments. Powderworld aims to support the study of generalization by providing a source of diverse tasks arising from the same core rules.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Frans_Isola_2022_Powderworld.pdf;/home/james/Zotero/storage/V797HRJC/2211.html}
}

@misc{FrontiersDexterousManipulation,
  title = {Frontiers {\textbar} {{Dexterous Manipulation}} for {{Multi-Fingered Robotic Hands With Reinforcement Learning}}: {{A Review}}},
  urldate = {2023-03-04},
  howpublished = {https://www.frontiersin.org/articles/10.3389/fnbot.2022.861825/full}
}

@misc{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and {van Hoof}, Herke and Meger, David},
  year = {2018},
  month = oct,
  number = {arXiv:1802.09477},
  eprint = {1802.09477},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-08-13},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf;/home/james/Zotero/storage/NACSBQUE/1802.html}
}

@misc{garcezNeurosymbolicAI3rd2020,
  title = {Neurosymbolic {{AI}}: {{The}} 3rd {{Wave}}},
  shorttitle = {Neurosymbolic {{AI}}},
  author = {d'Avila Garcez, Artur and Lamb, Luis C.},
  year = {2020},
  month = dec,
  number = {arXiv:2012.05876},
  eprint = {2012.05876},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.05876},
  urldate = {2023-03-04},
  abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.4,I.2.6},
  file = {/home/james/Documents/zotero/Garcez_Lamb_2020_Neurosymbolic AI.pdf;/home/james/Zotero/storage/4TQ5GT63/2012.html}
}

@misc{gargIQLearnInverseSoftQ2022,
  title = {{{IQ-Learn}}: {{Inverse}} Soft-{{Q Learning}} for {{Imitation}}},
  shorttitle = {{{IQ-Learn}}},
  author = {Garg, Divyansh and Chakraborty, Shuvam and Cundy, Chris and Song, Jiaming and Geist, Matthieu and Ermon, Stefano},
  year = {2022},
  month = nov,
  number = {arXiv:2106.12142},
  eprint = {2106.12142},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.12142},
  urldate = {2023-07-20},
  abstract = {In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment's dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, significantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Garg et al_2022_IQ-Learn.pdf;/home/james/Zotero/storage/PDYHHDZ6/2106.html}
}

@misc{geLongVideoGeneration2022,
  title = {Long {{Video Generation}} with {{Time-Agnostic VQGAN}} and {{Time-Sensitive Transformer}}},
  author = {Ge, Songwei and Hayes, Thomas and Yang, Harry and Yin, Xi and Pang, Guan and Jacobs, David and Huang, Jia-Bin and Parikh, Devi},
  year = {2022},
  month = sep,
  number = {arXiv:2204.03638},
  eprint = {2204.03638},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.03638},
  urldate = {2023-06-08},
  abstract = {Videos are created to express emotion, exchange information, and share experiences. Video synthesis has intrigued researchers for a long time. Despite the rapid progress driven by advances in visual synthesis, most existing studies focus on improving the frames' quality and the transitions between them, while little progress has been made in generating longer videos. In this paper, we present a method that builds on 3D-VQGAN and transformers to generate videos with thousands of frames. Our evaluation shows that our model trained on 16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse, and Taichi-HD datasets can generate diverse, coherent, and high-quality long videos. We also showcase conditional extensions of our approach for generating meaningful long videos by incorporating temporal information with text and audio. Videos and code can be found at https://songweige.github.io/projects/tats/index.html.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Ge et al_2022_Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer.pdf;/home/james/Zotero/storage/HGG46MCQ/2204.html}
}

@misc{gidarisUnsupervisedRepresentationLearning2018,
  title = {Unsupervised {{Representation Learning}} by {{Predicting Image Rotations}}},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  year = {2018},
  month = mar,
  number = {arXiv:1803.07728},
  eprint = {1803.07728},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.07728},
  urldate = {2024-11-24},
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/J5T2P75U/Gidaris et al. - 2018 - Unsupervised Representation Learning by Predicting Image Rotations.pdf;/home/james/Zotero/storage/M5YVEZ3L/1803.html}
}

@article{girinDynamicalVariationalAutoencoders2021,
  title = {Dynamical {{Variational Autoencoders}}: {{A Comprehensive Review}}},
  shorttitle = {Dynamical {{Variational Autoencoders}}},
  author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and {Alameda-Pineda}, Xavier},
  year = {2021},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {15},
  number = {1-2},
  eprint = {2008.12595},
  primaryclass = {cs, stat},
  pages = {1--175},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000089},
  urldate = {2024-06-04},
  abstract = {Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this paper, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimplemented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The paper concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/YS7MWJQ4/Girin et al. - 2021 - Dynamical Variational Autoencoders A Comprehensiv.pdf;/home/james/Zotero/storage/Y3G6TJRZ/2008.html}
}

@misc{gogianuSpectralNormalisationDeep2021,
  title = {Spectral {{Normalisation}} for {{Deep Reinforcement Learning}}: An {{Optimisation Perspective}}},
  shorttitle = {Spectral {{Normalisation}} for {{Deep Reinforcement Learning}}},
  author = {Gogianu, Florin and Berariu, Tudor and Rosca, Mihaela and Clopath, Claudia and Busoniu, Lucian and Pascanu, Razvan},
  year = {2021},
  month = may,
  number = {arXiv:2105.05246},
  eprint = {2105.05246},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.05246},
  urldate = {2024-07-06},
  abstract = {Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the performance of these developments not by changing the objective, but by regularising the value-function estimator. Constraining the Lipschitz constant of a single layer using spectral normalisation is sufficient to elevate the performance of a Categorical-DQN agent to that of a more elaborated {\textbackslash}rainbow\{\} agent on the challenging Atari domain. We conduct ablation studies to disentangle the various effects normalisation has on the learning dynamics and show that is sufficient to modulate the parameter updates to recover most of the performance of spectral normalisation. These findings hint towards the need to also focus on the neural component and its learning dynamics to tackle the peculiarities of Deep Reinforcement Learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/EZGPLL24/Gogianu et al. - 2021 - Spectral Normalisation for Deep Reinforcement Lear.pdf;/home/james/Zotero/storage/7HHY62SS/2105.html}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.2661},
  urldate = {2023-03-04},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Goodfellow et al_2014_Generative Adversarial Networks.pdf;/home/james/Zotero/storage/K65LTI8P/1406.html}
}

@misc{goyalCoordinationNeuralModules2022,
  title = {Coordination {{Among Neural Modules Through}} a {{Shared Global Workspace}}},
  author = {Goyal, Anirudh and Didolkar, Aniket and Lamb, Alex and Badola, Kartikeya and Ke, Nan Rosemary and Rahaman, Nasim and Binas, Jonathan and Blundell, Charles and Mozer, Michael and Bengio, Yoshua},
  year = {2022},
  month = mar,
  number = {arXiv:2103.01197},
  eprint = {2103.01197},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.01197},
  urldate = {2023-03-04},
  abstract = {Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions; object-centric architectures make use of graph neural networks to model interactions among entities. However, pairwise interactions may not achieve global coordination or a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Theory},
  file = {/home/james/Documents/zotero/Goyal et al_2022_Coordination Among Neural Modules Through a Shared Global Workspace.pdf;/home/james/Zotero/storage/4ZSCPIWD/2103.html}
}

@misc{greffBindingProblemArtificial2020,
  title = {On the {{Binding Problem}} in {{Artificial Neural Networks}}},
  author = {Greff, Klaus and {van Steenkiste}, Sjoerd and Schmidhuber, J{\"u}rgen},
  year = {2020},
  month = dec,
  number = {arXiv:2012.05208},
  eprint = {2012.05208},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.05208},
  urldate = {2023-03-04},
  abstract = {Contemporary neural networks still fall short of human-level generalization, which extends far beyond our direct experiences. In this paper, we argue that the underlying cause for this shortcoming is their inability to dynamically and flexibly bind information that is distributed throughout the network. This binding problem affects their capacity to acquire a compositional understanding of the world in terms of symbol-like entities (like objects), which is crucial for generalizing in predictable and systematic ways. To address this issue, we propose a unifying framework that revolves around forming meaningful entities from unstructured sensory inputs (segregation), maintaining this separation of information at a representational level (representation), and using these entities to construct new inferences, predictions, and behaviors (composition). Our analysis draws inspiration from a wealth of research in neuroscience and cognitive psychology, and surveys relevant mechanisms from the machine learning literature, to help identify a combination of inductive biases that allow symbolic information processing to emerge naturally in neural networks. We believe that a compositional approach to AI, in terms of grounded symbol-like representations, is of fundamental importance for realizing human-level generalization, and we hope that this paper may contribute towards that goal as a reference and inspiration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6},
  file = {/home/james/Documents/zotero/Greff et al_2020_On the Binding Problem in Artificial Neural Networks.pdf;/home/james/Zotero/storage/ACCHRHIC/2012.html}
}

@misc{grillBootstrapYourOwn2020,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  month = sep,
  number = {arXiv:2006.07733},
  eprint = {2006.07733},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.07733},
  urldate = {2024-11-24},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6{\textbackslash}\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/BR83I5BP/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-supervised Learning.pdf;/home/james/Zotero/storage/EF8GD23Y/2006.html}
}

@article{gronauerMultiagentDeepReinforcement2022,
  title = {Multi-Agent Deep Reinforcement Learning: A Survey},
  shorttitle = {Multi-Agent Deep Reinforcement Learning},
  author = {Gronauer, Sven and Diepold, Klaus},
  year = {2022},
  month = feb,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {2},
  pages = {895--943},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-021-09996-w},
  urldate = {2023-03-04},
  abstract = {The advances in reinforcement learning have recorded sublime success in various domains. Although the multi-agent domain has been overshadowed by its single-agent counterpart during this progress, multi-agent reinforcement learning gains rapid traction, and the latest accomplishments address problems with real-world complexity. This article provides an overview of the current developments in the field of multi-agent deep reinforcement learning. We focus primarily on literature from recent years that combines deep reinforcement learning methods with a multi-agent scenario. To survey the works that constitute the contemporary landscape, the main contents are divided into three parts. First, we analyze the structure of training schemes that are applied to train multiple agents. Second, we consider the emergent patterns of agent behavior in cooperative, competitive and mixed scenarios. Third, we systematically enumerate challenges that exclusively arise in the multi-agent domain and review methods that are leveraged to cope with these challenges. To conclude this survey, we discuss advances, identify trends, and outline possible directions for future work in this research area.},
  langid = {english},
  file = {/home/james/Documents/zotero/Gronauer_Diepold_2022_Multi-agent deep reinforcement learning.pdf}
}

@misc{guiSurveySelfsupervisedLearning2023,
  title = {A {{Survey}} on {{Self-supervised Learning}}: {{Algorithms}}, {{Applications}}, and {{Future Trends}}},
  shorttitle = {A {{Survey}} on {{Self-supervised Learning}}},
  author = {Gui, Jie and Chen, Tuo and Zhang, Jing and Cao, Qiong and Sun, Zhenan and Luo, Hao and Tao, Dacheng},
  year = {2023},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-09-12},
  abstract = {Deep supervised learning algorithms typically require a large volume of labeled data to achieve satisfactory performance. However, the process of collecting and labeling such data can be expensive and time-consuming. Self-supervised learning (SSL), a subset of unsupervised learning, aims to learn discriminative features from unlabeled data without relying on human-annotated labels. SSL has garnered significant attention recently, leading to the development of numerous related algorithms. However, there is a dearth of comprehensive studies that elucidate the connections and evolution of different SSL variants. This paper presents a review of diverse SSL methods, encompassing algorithmic aspects, application domains, three key trends, and open research questions. Firstly, we provide a detailed introduction to the motivations behind most SSL algorithms and compare their commonalities and differences. Secondly, we explore representative applications of SSL in domains such as image processing, computer vision, and natural language processing. Lastly, we discuss the three primary trends observed in SSL research and highlight the open questions that remain. A curated collection of valuable resources can be accessed at https://github.com/guijiejie/SSL.},
  howpublished = {https://arxiv.org/abs/2301.05712v4},
  langid = {english},
  file = {/home/james/Zotero/storage/WMIZP8ZQ/Gui et al. - 2023 - A Survey on Self-supervised Learning Algorithms, .pdf}
}

@misc{guoDirectedExplorationReinforcement2019,
  title = {Directed {{Exploration}} for {{Reinforcement Learning}}},
  author = {Guo, Zhaohan Daniel and Brunskill, Emma},
  year = {2019},
  month = jun,
  number = {arXiv:1906.07805},
  eprint = {1906.07805},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.07805},
  urldate = {2024-07-06},
  abstract = {Efficient exploration is necessary to achieve good sample efficiency for reinforcement learning in general. From small, tabular settings such as gridworlds to large, continuous and sparse reward settings such as robotic object manipulation tasks, exploration through adding an uncertainty bonus to the reward function has been shown to be effective when the uncertainty is able to accurately drive exploration towards promising states. However reward bonuses can still be inefficient since they are non-stationary, which means that we must wait for function approximators to catch up and converge again when uncertainties change. We propose the idea of directed exploration, that is learning a goal-conditioned policy where goals are simply other states, and using that to directly try to reach states with large uncertainty. The goal-conditioned policy is independent of uncertainty and is thus stationary. We show in our experiments how directed exploration is more efficient at exploration and more robust to how the uncertainty is computed than adding bonuses to rewards.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/9928WDWK/Guo and Brunskill - 2019 - Directed Exploration for Reinforcement Learning.pdf;/home/james/Zotero/storage/B895D842/1906.html}
}

@misc{haarnojaSoftActorCriticAlgorithms2019,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year = {2019},
  month = jan,
  number = {arXiv:1812.05905},
  eprint = {1812.05905},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.05905},
  urldate = {2023-08-24},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Haarnoja et al_2019_Soft Actor-Critic Algorithms and Applications.pdf;/home/james/Zotero/storage/LAQAN6WZ/1812.html}
}

@misc{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = aug,
  number = {arXiv:1801.01290},
  eprint = {1801.01290},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01290},
  urldate = {2023-06-14},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Haarnoja et al_2018_Soft Actor-Critic.pdf;/home/james/Zotero/storage/YYPH8Z7L/1801.html}
}

@misc{hafnerDeepHierarchicalPlanning2022,
  title = {Deep {{Hierarchical Planning}} from {{Pixels}}},
  author = {Hafner, Danijar and Lee, Kuang-Huei and Fischer, Ian and Abbeel, Pieter},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04114},
  eprint = {2206.04114},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.04114},
  urldate = {2023-03-04},
  abstract = {Intelligent agents need to select long sequences of actions to solve complex tasks. While humans easily break down tasks into subgoals and reach them through millions of muscle commands, current artificial intelligence is limited to tasks with horizons of a few hundred decisions, despite large compute budgets. Research on hierarchical reinforcement learning aims to overcome this limitation but has proven to be challenging, current methods rely on manually specified goal spaces or subtasks, and no general solution exists. We introduce Director, a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. The high-level policy maximizes task and exploration rewards by selecting latent goals and the low-level policy learns to achieve the goals. Despite operating in latent space, the decisions are interpretable because the world model can decode goals into images for visualization. Director outperforms exploration methods on tasks with sparse rewards, including 3D maze traversal with a quadruped robot from an egocentric camera and proprioception, without access to the global position or top-down view that was used by prior work. Director also learns successful behaviors across a wide range of environments, including visual control, Atari games, and DMLab levels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Hafner et al_2022_Deep Hierarchical Planning from Pixels.pdf;/home/james/Zotero/storage/FT6JRY3P/2206.html}
}

@misc{hafnerDreamControlLearning2020,
  title = {Dream to {{Control}}: {{Learning Behaviors}} by {{Latent Imagination}}},
  shorttitle = {Dream to {{Control}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  year = {2020},
  month = mar,
  number = {arXiv:1912.01603},
  eprint = {1912.01603},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.01603},
  urldate = {2023-03-04},
  abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Hafner et al_2020_Dream to Control.pdf;/home/james/Zotero/storage/95G88IHI/1912.html}
}

@misc{hafnerLearningLatentDynamics2019,
  title = {Learning {{Latent Dynamics}} for {{Planning}} from {{Pixels}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  year = {2019},
  month = jun,
  number = {arXiv:1811.04551},
  eprint = {1811.04551},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.04551},
  urldate = {2023-03-04},
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Hafner et al_2019_Learning Latent Dynamics for Planning from Pixels.pdf;/home/james/Zotero/storage/75LXKK22/1811.html}
}

@misc{hafnerMasteringAtariDiscrete2022,
  title = {Mastering {{Atari}} with {{Discrete World Models}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  year = {2022},
  month = feb,
  number = {arXiv:2010.02193},
  eprint = {2010.02193},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.02193},
  urldate = {2023-03-04},
  abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Hafner et al_2022_Mastering Atari with Discrete World Models.pdf;/home/james/Zotero/storage/UA2F47NY/2010.html}
}

@misc{hafnerMasteringDiverseDomains2024,
  title = {Mastering {{Diverse Domains}} through {{World Models}}},
  author = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  year = {2024},
  month = apr,
  number = {arXiv:2301.04104},
  eprint = {2301.04104},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.04104},
  urldate = {2024-11-24},
  abstract = {Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/HYS3S98G/Hafner et al. - 2024 - Mastering Diverse Domains through World Models.pdf;/home/james/Zotero/storage/R4M4U9NX/2301.html}
}

@misc{hansenTDMPC2ScalableRobust2023,
  title = {{{TD-MPC2}}: {{Scalable}}, {{Robust World Models}} for {{Continuous Control}}},
  shorttitle = {{{TD-MPC2}}},
  author = {Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16828},
  eprint = {2310.16828},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-26},
  abstract = {TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://nicklashansen.github.io/td-mpc2},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Hansen et al_2023_TD-MPC2.pdf;/home/james/Zotero/storage/HVI483GN/2310.html}
}

@misc{hansenTemporalDifferenceLearning2022,
  title = {Temporal {{Difference Learning}} for {{Model Predictive Control}}},
  author = {Hansen, Nicklas and Wang, Xiaolong and Su, Hao},
  year = {2022},
  month = jul,
  number = {arXiv:2203.04955},
  eprint = {2203.04955},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.04955},
  urldate = {2024-11-24},
  abstract = {Data-driven model predictive control has two key advantages over model-free methods: a potential for improved sample efficiency through model learning, and better performance as computational budget for planning increases. However, it is both costly to plan over long horizons and challenging to obtain an accurate model of the environment. In this work, we combine the strengths of model-free and model-based methods. We use a learned task-oriented latent dynamics model for local trajectory optimization over a short horizon, and use a learned terminal value function to estimate long-term return, both of which are learned jointly by temporal difference learning. Our method, TD-MPC, achieves superior sample efficiency and asymptotic performance over prior work on both state and image-based continuous control tasks from DMControl and Meta-World. Code and video results are available at https://nicklashansen.github.io/td-mpc.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Zotero/storage/U2WQ3L9N/Hansen et al. - 2022 - Temporal Difference Learning for Model Predictive Control.pdf;/home/james/Zotero/storage/PAND6WC8/2203.html}
}

@article{haoExplorationDeepReinforcement2023,
  title = {Exploration in {{Deep Reinforcement Learning}}: {{From Single-Agent}} to {{Multiagent Domain}}},
  shorttitle = {Exploration in {{Deep Reinforcement Learning}}},
  author = {Hao, Jianye and Yang, Tianpei and Tang, Hongyao and Bai, Chenjia and Liu, Jinyi and Meng, Zhaopeng and Liu, Peng and Wang, Zhen},
  year = {2023},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  eprint = {2109.06668},
  primaryclass = {cs},
  pages = {1--21},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2023.3236361},
  urldate = {2023-03-04},
  abstract = {Deep Reinforcement Learning (DRL) and Deep Multi-agent Reinforcement Learning (MARL) have achieved significant successes across a wide range of domains, including game AI, autonomous vehicles, robotics, and so on. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning towards the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and non-stationary co-learners. In this paper, we conduct a comprehensive survey on existing exploration methods for both single-agent and multi-agent RL. We start the survey by identifying several key challenges to efficient exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/james/Documents/zotero/Hao et al_2023_Exploration in Deep Reinforcement Learning.pdf;/home/james/Zotero/storage/DJZ8TR3T/2109.html}
}

@inproceedings{haRecurrentWorldModels2018,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-11-24},
  abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io},
  file = {/home/james/Zotero/storage/9YXWGU7I/Ha and Schmidhuber - 2018 - Recurrent World Models Facilitate Policy Evolution.pdf}
}

@misc{hasseltDeepReinforcementLearning2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q-learning}}},
  author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
  year = {2015},
  month = dec,
  number = {arXiv:1509.06461},
  eprint = {1509.06461},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.06461},
  urldate = {2024-11-24},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/KN8LT52B/Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf;/home/james/Zotero/storage/CERMKB9X/1509.html}
}

@misc{heLatentVideoDiffusion2022,
  title = {Latent {{Video Diffusion Models}} for {{High-Fidelity Video Generation}} with {{Arbitrary Lengths}}},
  author = {He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  year = {2022},
  month = nov,
  number = {arXiv:2211.13221},
  eprint = {2211.13221},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.13221},
  urldate = {2023-03-04},
  abstract = {AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models (DMs) are another class of deep generative models and have recently achieved remarkable performance on various image synthesis tasks. However, training image diffusion models usually requires substantial computational resources to achieve a high performance, which makes expanding diffusion models to high-dimensional video synthesis tasks more computationally expensive. To ease this problem while leveraging its advantages, we introduce lightweight video diffusion models that synthesize high-fidelity and arbitrary-long videos from pure noise. Specifically, we propose to perform diffusion and denoising in a low-dimensional 3D latent space, which significantly outperforms previous methods on 3D pixel space when under a limited computational budget. In addition, though trained on tens of frames, our models can generate videos with arbitrary lengths, i.e., thousands of frames, in an autoregressive way. Finally, conditional latent perturbation is further introduced to reduce performance degradation during synthesizing long-duration videos. Extensive experiments on various datasets and generated lengths suggest that our framework is able to sample much more realistic and longer videos than previous approaches, including GAN-based, autoregressive-based, and diffusion-based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/He et al_2022_Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary.pdf;/home/james/Zotero/storage/T7NNY382/2211.html}
}

@misc{heMaskedAutoencodersAre2021,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2021},
  month = dec,
  number = {arXiv:2111.06377},
  eprint = {2111.06377},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.06377},
  urldate = {2023-03-04},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/He et al_2021_Masked Autoencoders Are Scalable Vision Learners.pdf;/home/james/Zotero/storage/4EWC2432/2111.html}
}

@misc{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2020},
  month = mar,
  number = {arXiv:1911.05722},
  eprint = {1911.05722},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.05722},
  urldate = {2024-11-24},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Zotero/storage/PL8SZT3I/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Representation Learning.pdf;/home/james/Zotero/storage/UNCHHJ6N/1911.html}
}

@misc{hertzPrompttoPromptImageEditing2022,
  title = {Prompt-to-{{Prompt Image Editing}} with {{Cross Attention Control}}},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and {Cohen-Or}, Daniel},
  year = {2022},
  month = aug,
  number = {arXiv:2208.01626},
  eprint = {2208.01626},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.01626},
  urldate = {2023-03-04},
  abstract = {Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Hertz et al_2022_Prompt-to-Prompt Image Editing with Cross Attention Control.pdf;/home/james/Zotero/storage/47J55SQL/2208.html}
}

@misc{hesselMuesliCombiningImprovements2022,
  title = {Muesli: {{Combining Improvements}} in {{Policy Optimization}}},
  shorttitle = {Muesli},
  author = {Hessel, Matteo and Danihelka, Ivo and Viola, Fabio and Guez, Arthur and Schmitt, Simon and Sifre, Laurent and Weber, Theophane and Silver, David and {van Hasselt}, Hado},
  year = {2022},
  month = mar,
  number = {arXiv:2104.06159},
  eprint = {2104.06159},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.06159},
  urldate = {2023-03-04},
  abstract = {We propose a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. The update (henceforth Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli does so without using deep search: it acts directly with a policy network and has computation speed comparable to model-free baselines. The Atari results are complemented by extensive ablations, and by additional results on continuous control and 9x9 Go.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Hessel et al_2022_Muesli.pdf;/home/james/Zotero/storage/NANYK9GY/2104.html}
}

@misc{hesselRainbowCombiningImprovements2017,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2017},
  month = oct,
  number = {arXiv:1710.02298},
  eprint = {1710.02298},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.02298},
  urldate = {2023-03-04},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Hessel et al_2017_Rainbow.pdf;/home/james/Zotero/storage/R45S76E4/1710.html}
}

@misc{heSurveyOfflineModelBased2023,
  title = {A {{Survey}} on {{Offline Model-Based Reinforcement Learning}}},
  author = {He, Haoyang},
  year = {2023},
  month = may,
  number = {arXiv:2305.03360},
  eprint = {2305.03360},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.03360},
  urldate = {2024-07-07},
  abstract = {Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by the field, and suggest possible directions for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,I.2.6,I.2.8},
  file = {/home/james/Zotero/storage/UNYFHHXZ/He - 2023 - A Survey on Offline Model-Based Reinforcement Lear.pdf;/home/james/Zotero/storage/AJLCHVIR/2305.html}
}

@misc{HierarchicalReinforcementLearning,
  title = {Hierarchical {{Reinforcement Learning}}: {{A Comprehensive Survey}}: {{ACM Computing Surveys}}: {{Vol}} 54, {{No}} 5},
  urldate = {2023-03-04},
  howpublished = {https://dl.acm.org/doi/10.1145/3453160}
}

@misc{higginsDefinitionDisentangledRepresentations2018,
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  number = {arXiv:1812.02230},
  eprint = {1812.02230},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.02230},
  urldate = {2023-03-04},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Higgins et al_2018_Towards a Definition of Disentangled Representations.pdf;/home/james/Zotero/storage/SWJUGNHD/1812.html}
}

@incollection{hintonTransformingAutoEncoders2011,
  title = {Transforming {{Auto-Encoders}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2011},
  author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
  editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
  year = {2011},
  volume = {6791},
  pages = {44--51},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-21735-7_6},
  urldate = {2023-03-04},
  abstract = {The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the handengineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.},
  isbn = {978-3-642-21734-0 978-3-642-21735-7},
  langid = {english},
  file = {/home/james/Documents/zotero/Hinton et al_2011_Transforming Auto-Encoders.pdf}
}

@misc{hoVideoDiffusionModels2022,
  title = {Video {{Diffusion Models}}},
  author = {Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J.},
  year = {2022},
  month = jun,
  number = {arXiv:2204.03458},
  eprint = {2204.03458},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.03458},
  urldate = {2023-03-04},
  abstract = {Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Ho et al_2022_Video Diffusion Models.pdf;/home/james/Zotero/storage/7LDWEV5A/2204.html}
}

@article{huangGroundedDecodingGuiding,
  title = {Grounded {{Decoding}}: {{Guiding Text Generation}} with {{Grounded Models}} for {{Robot Control}}},
  author = {Huang, Wenlong and Xia, Fei and Shah, Dhruv and Driess, Danny and Zeng, Andy and Lu, Yao and Florence, Pete and Mordatch, Igor and Levine, Sergey and Hausman, Karol and Ichter, Brian},
  langid = {english},
  file = {/home/james/Documents/zotero/Huang et al_Grounded Decoding.pdf}
}

@inproceedings{huhStraighteningOutStraightThrough2023,
  title = {Straightening {{Out}} the {{Straight-Through Estimator}}: {{Overcoming Optimization Challenges}} in {{Vector Quantized Networks}}},
  shorttitle = {Straightening {{Out}} the {{Straight-Through Estimator}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Huh, Minyoung and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},
  year = {2023},
  month = jul,
  pages = {14096--14113},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-03-31},
  abstract = {This work examines the challenges of training neural networks using vector quantization using straight-through estimation. We find that the main cause of training instability is the discrepancy between the model embedding and the code-vector distribution. We identify the factors that contribute to this issue, including the codebook gradient sparsity and the asymmetric nature of the commitment loss, which leads to misaligned code-vector assignments. We propose to address this issue via affine re-parameterization of the code vectors. Additionally, we introduce an alternating optimization to reduce the gradient error introduced by the straight-through estimation. Moreover, we propose an improvement to the commitment loss to ensure better alignment between the codebook representation and the model embedding. These optimization methods improve the mathematical approximation of the straight-through estimation and, ultimately, the model performance. We demonstrate the effectiveness of our methods on several common model architectures, such as AlexNet, ResNet, and ViT, across various tasks, including image classification and generative modeling.},
  langid = {english},
  file = {/home/james/Zotero/storage/D5E3FHME/Huh et al. - 2023 - Straightening Out the Straight-Through Estimator .pdf}
}

@inproceedings{huPlanningGoalsExploration2022,
  title = {Planning {{Goals}} for {{Exploration}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Hu, Edward S. and Chang, Richard and Rybkin, Oleh and Jayaraman, Dinesh},
  year = {2022},
  month = sep,
  urldate = {2023-08-12},
  abstract = {Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose "Planning Exploratory Goals" (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to "plan goal commands". In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: https://sites.google.com/view/exploratory-goals},
  langid = {english},
  file = {/home/james/Documents/zotero/Hu et al_2022_Planning Goals for Exploration.pdf}
}

@article{hyvarinenEstimationNonNormalizedStatistical,
  title = {Estimation of {{Non-Normalized Statistical Models}} by {{Score Matching}}},
  author = {Hyvarinen, Aapo},
  abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
  langid = {english},
  file = {/home/james/Zotero/storage/6PZ4X5QZ/Hyvarinen - Estimation of Non-Normalized Statistical Models by.pdf}
}

@article{ibarzHowTrainYour2021,
  title = {How to Train Your Robot with Deep Reinforcement Learning: Lessons We Have Learned},
  shorttitle = {How to Train Your Robot with Deep Reinforcement Learning},
  author = {Ibarz, Julian and Tan, Jie and Finn, Chelsea and Kalakrishnan, Mrinal and Pastor, Peter and Levine, Sergey},
  year = {2021},
  month = apr,
  journal = {The International Journal of Robotics Research},
  volume = {40},
  number = {4-5},
  pages = {698--721},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364920987859},
  urldate = {2023-03-04},
  abstract = {Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low-level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time, realworld robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn: as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world.},
  langid = {english},
  file = {/home/james/Documents/zotero/Ibarz et al_2021_How to train your robot with deep reinforcement learning.pdf}
}

@misc{InductiveBiasesDeep,
  title = {Inductive Biases for Deep Learning of Higher-Level Cognition},
  doi = {10.1098/rspa.2021.0068},
  urldate = {2023-03-12},
  howpublished = {https://royalsocietypublishing.org/doi/epdf/10.1098/rspa.2021.0068},
  langid = {english},
  file = {/home/james/Documents/zotero/Inductive biases for deep learning of higher-level cognition.pdf}
}

@inproceedings{inproceedings,
  title = {Extracting and Composing Robust Features with Denoising Autoencoders},
  author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Y. and Manzagol, Pierre-Antoine},
  year = {2008},
  month = jan,
  series = {Proceedings of the 25th {{International Conference}} on {{Machine Learning}}},
  pages = {1096--1103},
  doi = {10.1145/1390156.1390294}
}

@misc{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  year = {2021},
  month = jun,
  number = {arXiv:2103.03206},
  eprint = {2103.03206},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.03206},
  urldate = {2023-03-04},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/james/Documents/zotero/Jaegle et al_2021_Perceiver.pdf;/home/james/Zotero/storage/36GXCMZ6/2103.html}
}

@article{jannerPlanningDiffusionFlexible,
  title = {Planning with {{Diffusion}} for {{Flexible Behavior Synthesis}}},
  author = {Janner, Michael and Du, Yilun and Tenenbaum, Joshua B and Levine, Sergey},
  abstract = {Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize longhorizon decision-making and test-time flexibility.},
  langid = {english},
  file = {/home/james/Zotero/storage/7YULH8XR/Janner et al_2021_When to Trust Your Model.pdf}
}

@misc{jannerWhenTrustYour2021,
  title = {When to {{Trust Your Model}}: {{Model-Based Policy Optimization}}},
  shorttitle = {When to {{Trust Your Model}}},
  author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  year = {2021},
  month = nov,
  number = {arXiv:1906.08253},
  eprint = {1906.08253},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.08253},
  urldate = {2023-03-04},
  abstract = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Janner et al_2021_When to Trust Your Model.pdf;/home/james/Zotero/storage/ZY3DI9HK/1906.html}
}

@misc{julianiStudyPlasticityLoss2024,
  title = {A {{Study}} of {{Plasticity Loss}} in {{On-Policy Deep Reinforcement Learning}}},
  author = {Juliani, Arthur and Ash, Jordan T.},
  year = {2024},
  month = may,
  number = {arXiv:2405.19153},
  eprint = {2405.19153},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.19153},
  urldate = {2024-08-29},
  abstract = {Continual learning with deep neural networks presents challenges distinct from both the fixed-dataset and convex continual learning regimes. One such challenge is plasticity loss, wherein a neural network trained in an online fashion displays a degraded ability to fit new tasks. This problem has been extensively studied in both supervised learning and off-policy reinforcement learning (RL), where a number of remedies have been proposed. Still, plasticity loss has received less attention in the on-policy deep RL setting. Here we perform an extensive set of experiments examining plasticity loss and a variety of mitigation methods in on-policy deep RL. We demonstrate that plasticity loss is pervasive under domain shift in this regime, and that a number of methods developed to resolve it in other settings fail, sometimes even resulting in performance that is worse than performing no intervention at all. In contrast, we find that a class of ``regenerative'' methods are able to consistently mitigate plasticity loss in a variety of contexts, including in gridworld tasks and more challenging environments like Montezuma's Revenge and ProcGen.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/RV6JPVD4/Juliani and Ash - 2024 - A Study of Plasticity Loss in On-Policy Deep Reinf.pdf;/home/james/Zotero/storage/TPXI24F4/2405.html}
}

@misc{kaiserModelBasedReinforcementLearning2024,
  title = {Model-{{Based Reinforcement Learning}} for {{Atari}}},
  author = {Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  year = {2024},
  month = apr,
  number = {arXiv:1903.00374},
  eprint = {1903.00374},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.00374},
  urldate = {2024-08-18},
  abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/DTUZ3VK7/Kaiser et al. - 2024 - Model-Based Reinforcement Learning for Atari.pdf;/home/james/Zotero/storage/5IXG6WQL/1903.html}
}

@misc{kapturowskiHumanlevelAtari200x2022,
  title = {Human-Level {{Atari}} 200x Faster},
  author = {Kapturowski, Steven and Campos, V{\'i}ctor and Jiang, Ray and Raki{\'c}evi{\'c}, Nemanja and {van Hasselt}, Hado and Blundell, Charles and Badia, Adri{\`a} Puigdom{\`e}nech},
  year = {2022},
  month = sep,
  number = {arXiv:2209.07550},
  eprint = {2209.07550},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.07550},
  urldate = {2023-04-02},
  abstract = {The task of building general agents that perform well over a wide range of tasks has been an important goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 billion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to out perform the human baseline. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization layers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy overtime.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Kapturowski et al_2022_Human-level Atari 200x faster.pdf;/home/james/Zotero/storage/TME8IA5L/2209.html}
}

@misc{karrasAliasFreeGenerativeAdversarial2021,
  title = {Alias-{{Free Generative Adversarial Networks}}},
  author = {Karras, Tero and Aittala, Miika and Laine, Samuli and H{\"a}rk{\"o}nen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  year = {2021},
  month = oct,
  number = {arXiv:2106.12423},
  eprint = {2106.12423},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.12423},
  urldate = {2023-03-04},
  abstract = {We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,GAN,Image Synthesis,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Karras et al_2021_Alias-Free Generative Adversarial Networks.pdf;/home/james/Zotero/storage/4348ZZE9/2106.html}
}

@misc{karrasAnalyzingImprovingImage2020,
  title = {Analyzing and {{Improving}} the {{Image Quality}} of {{StyleGAN}}},
  author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  year = {2020},
  month = mar,
  number = {arXiv:1912.04958},
  eprint = {1912.04958},
  primaryclass = {cs, eess, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.04958},
  urldate = {2023-03-04},
  abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Electrical Engineering and Systems Science - Image and Video Processing,GAN,Image Synthesis,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Karras et al_2020_Analyzing and Improving the Image Quality of StyleGAN.pdf;/home/james/Zotero/storage/KUVEF6NE/1912.html}
}

@misc{karrasStyleBasedGeneratorArchitecture2019,
  title = {A {{Style-Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year = {2019},
  month = mar,
  number = {arXiv:1812.04948},
  eprint = {1812.04948},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.04948},
  urldate = {2023-03-04},
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,GAN,Image Synthesis,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Karras et al_2019_A Style-Based Generator Architecture for Generative Adversarial Networks2.pdf;/home/james/Zotero/storage/Y2YLLJGL/1812.html}
}

@misc{karrasTrainingGenerativeAdversarial2020,
  title = {Training {{Generative Adversarial Networks}} with {{Limited Data}}},
  author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
  year = {2020},
  month = oct,
  number = {arXiv:2006.06676},
  eprint = {2006.06676},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.06676},
  urldate = {2023-03-05},
  abstract = {Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Karras et al_2020_Training Generative Adversarial Networks with Limited Data.pdf;/home/james/Zotero/storage/SYZL4WZS/2006.html}
}

@inproceedings{kearnsBiasVarianceErrorBounds2000,
  title = {Bias-{{Variance Error Bounds}} for {{Temporal Difference Updates}}},
  booktitle = {Proceedings of the {{Thirteenth Annual Conference}} on {{Computational Learning Theory}}},
  author = {Kearns, Michael J. and Singh, Satinder P.},
  year = {2000},
  month = jun,
  series = {{{COLT}} '00},
  pages = {142--147},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2024-11-24},
  isbn = {978-1-55860-703-3}
}

@misc{kharitonovEntropyMinimizationEmergent2020,
  title = {Entropy {{Minimization In Emergent Languages}}},
  author = {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
  year = {2020},
  month = jun,
  number = {arXiv:1905.13687},
  eprint = {1905.13687},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.13687},
  urldate = {2023-03-04},
  abstract = {There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent's inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Kharitonov et al_2020_Entropy Minimization In Emergent Languages.pdf;/home/james/Zotero/storage/MVSJTBM8/1905.html}
}

@article{kielakRecentAdvancementsModelbased2019,
  title = {Do Recent Advancements in Model-Based Deep Reinforcement Learning Really Improve Data Efficiency?},
  author = {Kielak, Kacper Piotr},
  year = {2019},
  month = sep,
  urldate = {2024-11-24},
  abstract = {Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.},
  langid = {english},
  file = {/home/james/Zotero/storage/8H9GUTBD/Kielak - 2019 - Do recent advancements in model-based deep reinforcement learning really improve data efficiency.pdf}
}

@misc{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2022},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6114},
  urldate = {2024-11-24},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/7MM8EYSG/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/home/james/Zotero/storage/8WEIQSBD/1312.html}
}

@misc{kolesnikovUViMUnifiedModeling2022,
  title = {{{UViM}}: {{A Unified Modeling Approach}} for {{Vision}} with {{Learned Guiding Codes}}},
  shorttitle = {{{UViM}}},
  author = {Kolesnikov, Alexander and Pinto, Andr{\'e} Susano and Beyer, Lucas and Zhai, Xiaohua and Harmsen, Jeremiah and Houlsby, Neil},
  year = {2022},
  month = oct,
  number = {arXiv:2205.10337},
  eprint = {2205.10337},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-28},
  abstract = {We introduce UViM, a unified approach capable of modeling a wide range of computer vision tasks. In contrast to previous models, UViM has the same functional form for all tasks; it requires no task-specific modifications which require extensive human expertise. The approach involves two components: (I) a base model (feed-forward) which is trained to directly predict raw vision outputs, guided by a learned discrete code and (II) a language model (autoregressive) that is trained to generate the guiding code. These components complement each other: the language model is well-suited to modeling structured interdependent data, while the base model is efficient at dealing with high-dimensional outputs. We demonstrate the effectiveness of UViM on three diverse and challenging vision tasks: panoptic segmentation, depth prediction and image colorization, where we achieve competitive and near state-of-the-art results. Our experimental results suggest that UViM is a promising candidate for a unified modeling approach in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Kolesnikov et al_2022_UViM.pdf;/home/james/Zotero/storage/7I5PZLPT/2205.html}
}

@misc{kostrikovImageAugmentationAll2021,
  title = {Image {{Augmentation Is All You Need}}: {{Regularizing Deep Reinforcement Learning}} from {{Pixels}}},
  shorttitle = {Image {{Augmentation Is All You Need}}},
  author = {Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  year = {2021},
  month = mar,
  number = {arXiv:2004.13649},
  eprint = {2004.13649},
  primaryclass = {cs, eess, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.13649},
  urldate = {2024-09-13},
  abstract = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/NT5GU8GQ/Kostrikov et al. - 2021 - Image Augmentation Is All You Need Regularizing D.pdf;/home/james/Zotero/storage/LZ9VZ6D2/2004.html}
}

@misc{kostrikovImageAugmentationAll2021a,
  title = {Image {{Augmentation Is All You Need}}: {{Regularizing Deep Reinforcement Learning}} from {{Pixels}}},
  shorttitle = {Image {{Augmentation Is All You Need}}},
  author = {Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  year = {2021},
  month = mar,
  number = {arXiv:2004.13649},
  eprint = {2004.13649},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-02},
  abstract = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to transform input examples, as well as regularizing the value function and policy. Existing model-free approaches, such as Soft Actor-Critic (SAC) [22], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based [23, 38, 24] methods and recently proposed contrastive learning [50]. Our approach, which we dub DrQ: Data-regularized Q, can be combined with any model-free reinforcement learning algorithm. We further demonstrate this by applying it to DQN [43] and significantly improve its data-efficiency on the Atari 100k [31] benchmark. An implementation can be found at https://sites. google.com/view/data-regularized-q.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/NILWSADJ/Kostrikov et al. - 2021 - Image Augmentation Is All You Need Regularizing Deep Reinforcement Learning from Pixels.pdf}
}

@misc{krzyzinskiSurvSHAPtTimedependentExplanations2022,
  title = {{{SurvSHAP}}(t): {{Time-dependent}} Explanations of Machine Learning Survival Models},
  shorttitle = {{{SurvSHAP}}(t)},
  author = {Krzyzi{\'n}ski, Mateusz and Spytek, Miko{\l}aj and Baniecki, Hubert and Biecek, Przemys{\l}aw},
  year = {2022},
  month = sep,
  number = {arXiv:2208.11080},
  eprint = {2208.11080},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.11080},
  urldate = {2023-03-04},
  abstract = {Machine and deep learning survival models demonstrate similar or even improved time-to-event prediction capabilities compared to classical statistical learning methods yet are too complex to be interpreted by humans. Several model-agnostic explanations are available to overcome this issue; however, none directly explain the survival function prediction. In this paper, we introduce SurvSHAP(t), the first time-dependent explanation that allows for interpreting survival black-box models. It is based on SHapley Additive exPlanations with solid theoretical foundations and a broad adoption among machine learning practitioners. The proposed methods aim to enhance precision diagnostics and support domain experts in making decisions. Experiments on synthetic and medical data confirm that SurvSHAP(t) can detect variables with a time-dependent effect, and its aggregation is a better determinant of the importance of variables for a prediction than SurvLIME. SurvSHAP(t) is model-agnostic and can be applied to all models with functional output. We provide an accessible implementation of time-dependent explanations in Python at http://github.com/MI2DataLab/survshap.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Krzyziński et al_2022_SurvSHAP(t).pdf;/home/james/Zotero/storage/S9TCR7UG/2208.html}
}

@misc{kumarConservativeQLearningOffline2020,
  title = {Conservative {{Q-Learning}} for {{Offline Reinforcement Learning}}},
  author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  year = {2020},
  month = aug,
  number = {arXiv:2006.04779},
  eprint = {2006.04779},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.04779},
  urldate = {2024-08-03},
  abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/H6VEYDN2/Kumar et al. - 2020 - Conservative Q-Learning for Offline Reinforcement .pdf;/home/james/Zotero/storage/TBGUXQA8/2006.html}
}

@misc{kurutachModelEnsembleTrustRegionPolicy2018,
  title = {Model-{{Ensemble Trust-Region Policy Optimization}}},
  author = {Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
  year = {2018},
  month = oct,
  number = {arXiv:1802.10592},
  eprint = {1802.10592},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.10592},
  urldate = {2024-11-26},
  abstract = {Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Zotero/storage/NPZPA3WL/Kurutach et al. - 2018 - Model-Ensemble Trust-Region Policy Optimization.pdf;/home/james/Zotero/storage/JVFZXSE7/1802.html}
}

@misc{lakshminarayananSimpleScalablePredictive2017,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  year = {2017},
  month = nov,
  number = {arXiv:1612.01474},
  eprint = {1612.01474},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-06-10},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Lakshminarayanan et al_2017_Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.pdf;/home/james/Zotero/storage/IYWPEGEP/1612.html}
}

@article{landajuelaDiscoveringSymbolicPolicies,
  title = {Discovering Symbolic Policies with Deep Reinforcement Learning},
  author = {Landajuela, Mikel and Petersen, Brenden K and Kim, Sookyung and Santiago, Claudio P and Glatt, Ruben and Mundhenk, T Nathan and Pettit, Jacob F and Faissol, Daniel M},
  abstract = {Deep reinforcement learning (DRL) has proven successful for many difficult control problems by learning policies represented by neural networks. However, the complexity of neural network-based policies---involving thousands of composed nonlinear operators---can render them problematic to understand, trust, and deploy. In contrast, simple policies comprising short symbolic expressions can facilitate human understanding, while also being transparent and exhibiting predictable behavior. To this end, we propose deep symbolic policy, a novel approach to directly search the space of symbolic policies. We use an autoregressive recurrent neural network to generate control policies represented by tractable mathematical expressions, employing a risk-seeking policy gradient to maximize performance of the generated policies. To scale to environments with multidimensional action spaces, we propose an ``anchoring'' algorithm that distills pre-trained neural network-based policies into fully symbolic policies, one action dimension at a time. We also introduce two novel methods to improve exploration in DRL-based combinatorial optimization, building on ideas of entropy regularization and distribution initialization. Despite their dramatically reduced complexity, we demonstrate that discovered symbolic policies outperform seven state-of-the-art DRL algorithms in terms of average rank and average normalized episodic reward across eight benchmark environments.},
  langid = {english},
  file = {/home/james/Documents/zotero/Landajuela et al_Discovering symbolic policies with deep reinforcement learning.pdf}
}

@misc{laskinIncontextReinforcementLearning2022,
  title = {In-Context {{Reinforcement Learning}} with {{Algorithm Distillation}}},
  author = {Laskin, Michael and Wang, Luyu and Oh, Junhyuk and Parisotto, Emilio and Spencer, Stephen and Steigerwald, Richie and Strouse, D. J. and Hansen, Steven and Filos, Angelos and Brooks, Ethan and Gazeau, Maxime and Sahni, Himanshu and Singh, Satinder and Mnih, Volodymyr},
  year = {2022},
  month = oct,
  number = {arXiv:2210.14215},
  eprint = {2210.14215},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.14215},
  urldate = {2023-03-12},
  abstract = {We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Laskin et al_2022_In-context Reinforcement Learning with Algorithm Distillation.pdf;/home/james/Zotero/storage/BE4LLGZ5/2210.html}
}

@inproceedings{laskinReinforcementLearningAugmented2020,
  title = {Reinforcement {{Learning}} with {{Augmented Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Laskin, Misha and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  year = {2020},
  volume = {33},
  pages = {19884--19895},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-11-24},
  abstract = {Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks.},
  file = {/home/james/Zotero/storage/RBDVETXJ/Laskin et al. - 2020 - Reinforcement Learning with Augmented Data.pdf}
}

@misc{lazaridouEmergenceLinguisticCommunication2018,
  title = {Emergence of {{Linguistic Communication}} from {{Referential Games}} with {{Symbolic}} and {{Pixel Input}}},
  author = {Lazaridou, Angeliki and Hermann, Karl Moritz and Tuyls, Karl and Clark, Stephen},
  year = {2018},
  month = apr,
  number = {arXiv:1804.03984},
  eprint = {1804.03984},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.03984},
  urldate = {2023-03-04},
  abstract = {The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/james/Documents/zotero/Lazaridou et al_2018_Emergence of Linguistic Communication from Referential Games with Symbolic and.pdf;/home/james/Zotero/storage/YDUVYJRE/1804.html}
}

@misc{lecarpentierNonStationaryMarkovDecision2020,
  title = {Non-{{Stationary Markov Decision Processes}}, a {{Worst-Case Approach}} Using {{Model-Based Reinforcement Learning}}, {{Extended}} Version},
  author = {Lecarpentier, Erwan and Rachelson, Emmanuel},
  year = {2020},
  month = jan,
  number = {arXiv:1904.10090},
  eprint = {1904.10090},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.10090},
  urldate = {2024-07-06},
  abstract = {This work tackles the problem of robust zero-shot planning in non-stationary stochastic environments. We study Markov Decision Processes (MDPs) evolving over time and consider Model-Based Reinforcement Learning algorithms in this setting. We make two hypotheses: 1) the environment evolves continuously with a bounded evolution rate; 2) a current model is known at each decision epoch but not its evolution. Our contribution can be presented in four points. 1) we define a specific class of MDPs that we call Non-Stationary MDPs (NSMDPs). We introduce the notion of regular evolution by making an hypothesis of Lipschitz-Continuity on the transition and reward functions w.r.t. time; 2) we consider a planning agent using the current model of the environment but unaware of its future evolution. This leads us to consider a worst-case method where the environment is seen as an adversarial agent; 3) following this approach, we propose the Risk-Averse Tree-Search (RATS) algorithm, a zero-shot Model-Based method similar to Minimax search; 4) we illustrate the benefits brought by RATS empirically and compare its performance with reference Model-Based algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/L5P6S2I9/Lecarpentier and Rachelson - 2020 - Non-Stationary Markov Decision Processes, a Worst-.pdf;/home/james/Zotero/storage/BEXJ8Z3X/1904.html}
}

@misc{leeMultiGameDecisionTransformers2022,
  title = {Multi-{{Game Decision Transformers}}},
  author = {Lee, Kuang-Huei and Nachum, Ofir and Yang, Mengjiao and Lee, Lisa and Freeman, Daniel and Xu, Winnie and Guadarrama, Sergio and Fischer, Ian and Jang, Eric and Michalewski, Henryk and Mordatch, Igor},
  year = {2022},
  month = oct,
  number = {arXiv:2205.15241},
  eprint = {2205.15241},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.15241},
  urldate = {2023-03-04},
  abstract = {A longstanding goal of the field of AI is a method for learning a highly capable, generalist agent from diverse experience. In the subfields of vision and language, this was largely achieved by scaling up transformer-based models and training them on large, diverse datasets. Motivated by this progress, we investigate whether the same strategy can be used to produce generalist reinforcement learning agents. Specifically, we show that a single transformer-based model - with a single set of weights - trained purely offline can play a suite of up to 46 Atari games simultaneously at close-to-human performance. When trained and evaluated appropriately, we find that the same trends observed in language and vision hold, including scaling of performance with model size and rapid adaptation to new games via fine-tuning. We compare several approaches in this multi-game setting, such as online and offline RL methods and behavioral cloning, and find that our Multi-Game Decision Transformer models offer the best scalability and performance. We release the pre-trained models and code to encourage further research in this direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Lee et al_2022_Multi-Game Decision Transformers.pdf;/home/james/Zotero/storage/ESL34KDT/2205.html}
}

@misc{levineReinforcementLearningControl2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  year = {2018},
  month = may,
  number = {arXiv:1805.00909},
  eprint = {1805.00909},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-10-21},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Levine_2018_Reinforcement Learning and Control as Probabilistic Inference.pdf;/home/james/Zotero/storage/7WK96DVV/1805.html}
}

@misc{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  number = {arXiv:1509.02971},
  eprint = {1509.02971},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.02971},
  urldate = {2024-11-24},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/WEB39L3B/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learning.pdf;/home/james/Zotero/storage/J3V4JQJJ/1509.html}
}

@article{linCloserLookLoss2021,
  title = {A {{Closer Look}} at {{Loss Weighting}} in {{Multi-Task Learning}}},
  author = {Lin, Baijiong and Ye, Feiyang and Zhang, Yu},
  year = {2021},
  month = oct,
  urldate = {2023-07-05},
  abstract = {Multi-Task Learning (MTL) has achieved great success in various fields, however, how to balance different tasks to avoid negative effects is still a key problem. To achieve the task balancing, there exist many works to balance task losses or gradients. In this paper, we unify eight representative task balancing methods from the perspective of loss weighting and provide a consistent experimental comparison. Moreover, we surprisingly find that training a MTL model with random weights sampled from a distribution can achieve comparable performance over state-of-the-art baselines. Based on this finding, we propose a simple yet effective weighting strategy called Random Loss Weighting (RLW), which can be implemented in only one additional line of code over existing works. Theoretically, we analyze the convergence of RLW and reveal that RLW has a higher probability to escape local minima than existing models with fixed task weights, resulting in a better generalization ability. Empirically, we extensively evaluate the proposed RLW method on six image datasets and four multilingual tasks from the XTREME benchmark to show the effectiveness of the proposed RLW strategy when compared with state-of-the-art strategies.},
  langid = {english},
  keywords = {Meta-Learning},
  file = {/home/james/Documents/zotero/Lin et al_2021_A Closer Look at Loss Weighting in Multi-Task Learning.pdf}
}

@misc{liSurveyTransformersReinforcement2023,
  title = {A {{Survey}} on {{Transformers}} in {{Reinforcement Learning}}},
  author = {Li, Wenzhe and Luo, Hao and Lin, Zichuan and Zhang, Chongjie and Lu, Zongqing and Ye, Deheng},
  year = {2023},
  month = jan,
  number = {arXiv:2301.03044},
  eprint = {2301.03044},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.03044},
  urldate = {2023-03-08},
  abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under a supervised setting. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. Hence, in this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,RL,Transformers},
  file = {/home/james/Documents/zotero/Li et al_2023_A Survey on Transformers in Reinforcement Learning.pdf;/home/james/Zotero/storage/9JEHLJXG/2301.html}
}

@misc{liuAPSActivePretraining2021,
  title = {{{APS}}: {{Active Pretraining}} with {{Successor Features}}},
  shorttitle = {{{APS}}},
  author = {Liu, Hao and Abbeel, Pieter},
  year = {2021},
  month = aug,
  number = {arXiv:2108.13956},
  eprint = {2108.13956},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.13956},
  urldate = {2023-03-17},
  abstract = {We introduce a new unsupervised pretraining objective for reinforcement learning. During the unsupervised reward-free pretraining phase, the agent maximizes mutual information between tasks and states induced by the policy. Our key contribution is a novel lower bound of this intractable quantity. We show that by reinterpreting and combining variational successor features{\textasciitilde}{\textbackslash}citep\{Hansen2020Fast\} with nonparametric entropy maximization{\textasciitilde}{\textbackslash}citep\{liu2021behavior\}, the intractable mutual information can be efficiently optimized. The proposed method Active Pretraining with Successor Feature (APS) explores the environment via nonparametric entropy maximization, and the explored data can be efficiently leveraged to learn behavior by variational successor features. APS addresses the limitations of existing mutual information maximization based and entropy maximization based unsupervised RL, and combines the best of both worlds. When evaluated on the Atari 100k data-efficiency benchmark, our approach significantly outperforms previous methods combining unsupervised pretraining with task-specific finetuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,RL},
  file = {/home/james/Documents/zotero/Liu_Abbeel_2021_APS.pdf;/home/james/Zotero/storage/4MW6HSYS/2108.html}
}

@article{liuBehaviorVoidUnsupervised,
  title = {Behavior {{From}} the {{Void}}: {{Unsupervised Active Pre-Training}}},
  author = {Liu, Hao and Abbeel, Pieter},
  abstract = {We introduce a new unsupervised pre-training method for reinforcement learning called APT, which stands for Active Pre-Training. APT learns behaviors and representations by actively searching for novel states in reward-free environments. The key novel idea is to explore the environment by maximizing a non-parametric entropy computed in an abstract representation space, which avoids challenging density modeling and consequently allows our approach to scale much better in environments that have high-dimensional observations (e.g., image observations). We empirically evaluate APT by exposing task-specific reward after a long unsupervised pre-training phase. In Atari games, APT achieves human-level performance on 12 games and obtains highly competitive performance compared to canonical fully supervised RL algorithms. On DMControl suite, APT beats all baselines in terms of asymptotic performance and data efficiency and dramatically improves performance on tasks that are extremely difficult to train from scratch.},
  langid = {english},
  keywords = {RL,Unsupervised RL},
  file = {/home/james/Documents/zotero/Liu_Abbeel_Behavior From the Void.pdf}
}

@misc{liuComputationalLanguageAcquisition2023,
  title = {Computational {{Language Acquisition}} with {{Theory}} of {{Mind}}},
  author = {Liu, Andy and Zhu, Hao and Liu, Emmy and Bisk, Yonatan and Neubig, Graham},
  year = {2023},
  month = mar,
  number = {arXiv:2303.01502},
  eprint = {2303.01502},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.01502},
  urldate = {2023-03-04},
  abstract = {Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack \& Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Theory},
  file = {/home/james/Documents/zotero/Liu et al_2023_Computational Language Acquisition with Theory of Mind.pdf;/home/james/Zotero/storage/5ML3ZKUZ/2303.html}
}

@article{liuDeepLearningProcedural2021,
  title = {Deep {{Learning}} for {{Procedural Content Generation}}},
  author = {Liu, Jialin and Snodgrass, Sam and Khalifa, Ahmed and Risi, Sebastian and Yannakakis, Georgios N. and Togelius, Julian},
  year = {2021},
  month = jan,
  journal = {Neural Computing and Applications},
  volume = {33},
  number = {1},
  eprint = {2010.04548},
  primaryclass = {cs},
  pages = {19--37},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-020-05383-8},
  urldate = {2024-07-06},
  abstract = {Procedural content generation in video games has a long history. Existing procedural content generation methods, such as search-based, solver-based, rule-based and grammar-based methods have been applied to various content types such as levels, maps, character models, and textures. A research field centered on content generation in games has existed for more than a decade. More recently, deep learning has powered a remarkable range of inventions in content production, which are applicable to games. While some cutting-edge deep learning methods are applied on their own, others are applied in combination with more traditional methods, or in an interactive setting. This article surveys the various deep learning methods that have been applied to generate game content directly or indirectly, discusses deep learning methods that could be used for content generation purposes but are rarely used today, and envisages some limitations and potential future directions of deep learning for procedural content generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/A5FA9PWP/Liu et al. - 2021 - Deep Learning for Procedural Content Generation.pdf;/home/james/Zotero/storage/ZQDYK56A/2010.html}
}

@misc{lockwoodReviewUncertaintyDeep2022,
  title = {A {{Review}} of {{Uncertainty}} for {{Deep Reinforcement Learning}}},
  author = {Lockwood, Owen and Si, Mei},
  year = {2022},
  month = aug,
  number = {arXiv:2208.09052},
  eprint = {2208.09052},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.09052},
  urldate = {2023-10-30},
  abstract = {Uncertainty is ubiquitous in games, both in the agents playing games and often in the games themselves. Working with uncertainty is therefore an important component of successful deep reinforcement learning agents. While there has been substantial effort and progress in understanding and working with uncertainty for supervised learning, the body of literature for uncertainty aware deep reinforcement learning is less developed. While many of the same problems regarding uncertainty in neural networks for supervised learning remain for reinforcement learning, there are additional sources of uncertainty due to the nature of an interactable environment. In this work, we provide an overview motivating and presenting existing techniques in uncertainty aware deep reinforcement learning. These works show empirical benefits on a variety of reinforcement learning tasks. This work serves to help to centralize the disparate results and promote future research in this area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Lockwood_Si_2022_A Review of Uncertainty for Deep Reinforcement Learning.pdf;/home/james/Zotero/storage/6WGVUQZI/2208.html}
}

@inproceedings{loshchilovDecoupledWeightDecay2018,
  title = {Decoupled {{Weight Decay Regularization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2018},
  month = sep,
  urldate = {2024-11-24},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at {\textbackslash}url\{https://github.com/loshchil/AdamW-and-SGDW\}},
  langid = {english},
  file = {/home/james/Zotero/storage/2FPW57V3/Loshchilov and Hutter - 2018 - Decoupled Weight Decay Regularization.pdf}
}

@misc{luoSurveyModelbasedReinforcement2022,
  title = {A {{Survey}} on {{Model-based Reinforcement Learning}}},
  author = {Luo, Fan-Ming and Xu, Tian and Lai, Hang and Chen, Xiong-Hui and Zhang, Weinan and Yu, Yang},
  year = {2022},
  month = jun,
  number = {arXiv:2206.09328},
  eprint = {2206.09328},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.09328},
  urldate = {2023-03-04},
  abstract = {Reinforcement learning (RL) solves sequential decision-making problems via a trial-and-error process interacting with the environment. While RL achieves outstanding success in playing complex video games that allow huge trial-and-error, making errors is always undesired in the real world. To improve the sample efficiency and thus reduce the errors, model-based reinforcement learning (MBRL) is believed to be a promising direction, which builds environment models in which the trial-and-errors can take place without real costs. In this survey, we take a review of MBRL with a focus on the recent progress in deep RL. For non-tabular environments, there is always a generalization error between the learned environment model and the real environment. As such, it is of great importance to analyze the discrepancy between policy training in the environment model and that in the real environment, which in turn guides the algorithm design for better model learning, model usage, and policy training. Besides, we also discuss the recent advances of model-based techniques in other forms of RL, including offline RL, goal-conditioned RL, multi-agent RL, and meta-RL. Moreover, we discuss the applicability and advantages of MBRL in real-world tasks. Finally, we end this survey by discussing the promising prospects for the future development of MBRL. We think that MBRL has great potential and advantages in real-world applications that were overlooked, and we hope this survey could attract more research on MBRL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Model-based RL,RL},
  file = {/home/james/Documents/zotero/Luo et al_2022_A Survey on Model-based Reinforcement Learning.pdf;/home/james/Zotero/storage/7KYBCJCF/2206.html}
}

@misc{luoUnderstandingDiffusionModels2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11970},
  eprint = {2208.11970},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.11970},
  urldate = {2023-03-04},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Luo_2022_Understanding Diffusion Models.pdf;/home/james/Zotero/storage/FEI8DUUP/2208.html}
}

@misc{luResetFreeLifelongLearning2021,
  title = {Reset-{{Free Lifelong Learning}} with {{Skill-Space Planning}}},
  author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  year = {2021},
  month = jun,
  number = {arXiv:2012.03548},
  eprint = {2012.03548},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.03548},
  urldate = {2023-03-04},
  abstract = {The objective of lifelong reinforcement learning (RL) is to optimize agents which can continuously adapt and interact in changing environments. However, current RL approaches fail drastically when environments are non-stationary and interactions are non-episodic. We propose Lifelong Skill Planning (LiSP), an algorithmic framework for non-episodic lifelong RL based on planning in an abstract space of higher-order skills. We learn the skills in an unsupervised manner using intrinsic rewards and plan over the learned skills using a learned dynamics model. Moreover, our framework permits skill discovery even from offline data, thereby reducing the need for excessive real-world interactions. We demonstrate empirically that LiSP successfully enables long-horizon planning and learns agents that can avoid catastrophic failures even in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Lu et al_2021_Reset-Free Lifelong Learning with Skill-Space Planning.pdf;/home/james/Zotero/storage/XTX9UR6K/2012.html}
}

@misc{lyleUnderstandingPlasticityNeural2023,
  title = {Understanding Plasticity in Neural Networks},
  author = {Lyle, Clare and Zheng, Zeyu and Nikishin, Evgenii and Pires, Bernardo Avila and Pascanu, Razvan and Dabney, Will},
  year = {2023},
  month = nov,
  number = {arXiv:2303.01486},
  eprint = {2303.01486},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it often occurs in the absence of saturated units. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these findings on larger-scale RL benchmarks in the Arcade Learning Environment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/H2KNQDDB/Lyle et al. - 2023 - Understanding plasticity in neural networks.pdf}
}

@article{macariobarrosComprehensiveSurveyVisual2022,
  title = {A {{Comprehensive Survey}} of {{Visual SLAM Algorithms}}},
  author = {Macario Barros, Andr{\'e}a and Michel, Maugan and Moline, Yoann and Corre, Gwenol{\'e} and Carrel, Fr{\'e}d{\'e}rick},
  year = {2022},
  month = feb,
  journal = {Robotics},
  volume = {11},
  number = {1},
  pages = {24},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2218-6581},
  doi = {10.3390/robotics11010024},
  urldate = {2023-06-08},
  abstract = {Simultaneous localization and mapping (SLAM) techniques are widely researched, since they allow the simultaneous creation of a map and the sensors' pose estimation in an unknown environment. Visual-based SLAM techniques play a significant role in this field, as they are based on a low-cost and small sensor system, which guarantees those advantages compared to other sensor-based SLAM techniques. The literature presents different approaches and methods to implement visual-based SLAM systems. Among this variety of publications, a beginner in this domain may find problems with identifying and analyzing the main algorithms and selecting the most appropriate one according to his or her project constraints. Therefore, we present the three main visual-based SLAM approaches (visual-only, visual-inertial, and RGB-D SLAM), providing a review of the main algorithms of each approach through diagrams and flowcharts, and highlighting the main advantages and disadvantages of each technique. Furthermore, we propose six criteria that ease the SLAM algorithm's analysis and consider both the software and hardware levels. In addition, we present some major issues and future directions on visual-SLAM field, and provide a general overview of some of the existing benchmark datasets. This work aims to be the first step for those initiating a SLAM project to have a good perspective of SLAM techniques' main elements and characteristics.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {3D reconstruction,embedded SLAM,evaluation criteria,RGB-D SLAM,SLAM,visual-inertial SLAM,visual-SLAM},
  file = {/home/james/Documents/zotero/Macario Barros et al_2022_A Comprehensive Survey of Visual SLAM Algorithms.pdf}
}

@misc{mandtStochasticGradientDescent2018,
  title = {Stochastic {{Gradient Descent}} as {{Approximate Bayesian Inference}}},
  author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  year = {2018},
  month = jan,
  number = {arXiv:1704.04289},
  eprint = {1704.04289},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.04289},
  urldate = {2024-07-06},
  abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/WVW55UYU/Mandt et al. - 2018 - Stochastic Gradient Descent as Approximate Bayesia.pdf;/home/james/Zotero/storage/EXAYMQF6/1704.html}
}

@article{mazziaEfficientCapsNetCapsuleNetwork2021,
  title = {Efficient-{{CapsNet}}: Capsule Network with Self-Attention Routing},
  shorttitle = {Efficient-{{CapsNet}}},
  author = {Mazzia, Vittorio and Salvetti, Francesco and Chiaberge, Marcello},
  year = {2021},
  month = jul,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {14634},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-93977-0},
  urldate = {2023-03-04},
  abstract = {Abstract             Deep convolutional neural networks, assisted by architectural design strategies, make extensive use of data augmentation techniques and layers with a high number of feature maps to embed object transformations. That is highly inefficient and for large datasets implies a massive redundancy of features detectors. Even though capsules networks are still in their infancy, they constitute a promising solution to extend current convolutional networks and endow artificial visual perception with a process to encode more efficiently all feature affine transformations. Indeed, a properly working capsule network should theoretically achieve higher results with a considerably lower number of parameters count due to intrinsic capability to generalize to novel viewpoints. Nevertheless, little attention has been given to this relevant aspect. In this paper, we investigate the efficiency of capsule networks and, pushing their capacity to the limits with an extreme architecture with barely 160 K parameters, we prove that the proposed architecture is still able to achieve state-of-the-art results on three different datasets with only 2\% of the original CapsNet parameters. Moreover, we replace dynamic routing with a novel non-iterative, highly parallelizable routing algorithm that can easily cope with a reduced number of capsules. Extensive experimentation with other capsule implementations has proved the effectiveness of our methodology and the capability of capsule networks to efficiently embed visual representations more prone to generalization.},
  langid = {english},
  file = {/home/james/Documents/zotero/Mazzia et al_2021_Efficient-CapsNet.pdf}
}

@inproceedings{meiSpeedyZeroMasteringAtari2022,
  title = {{{SpeedyZero}}: {{Mastering Atari}} with {{Limited Data}} and {{Time}}},
  shorttitle = {{{SpeedyZero}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Mei, Yixuan and Gao, Jiaxuan and Ye, Weirui and Liu, Shaohuai and Gao, Yang and Wu, Yi},
  year = {2022},
  month = sep,
  urldate = {2023-08-09},
  abstract = {Many recent breakthroughs of deep reinforcement learning (RL) are mainly built upon large-scale distributed training of model-free methods using millions to billions of samples. On the other hand, state-of-the-art model-based RL methods can achieve human-level sample efficiency but often take a much longer over all training time than model-free methods. However, high sample efficiency and fast training time are both important to many real-world applications. We develop SpeedyZero, a distributed RL system built upon a state-of-the-art model-based RL method, EfficientZero, with a dedicated system design for fast distributed computation. We also develop two novel algorithmic techniques, Priority Refresh and Clipped LARS, to stabilize training with massive parallelization and large batch size. SpeedyZero maintains on-par sample efficiency compared with EfficientZero while achieving a 14.5X speedup in wall-clock time, leading to human-level performances on the Atari benchmark within 35 minutes using only 300k samples. In addition, we also present an in-depth analysis on the fundamental challenges in further scaling our system to bring insights to the community.},
  langid = {english},
  file = {/home/james/Documents/zotero/Mei et al_2022_SpeedyZero.pdf}
}

@misc{mendoncaDiscoveringAchievingGoals2021,
  title = {Discovering and {{Achieving Goals}} via {{World Models}}},
  author = {Mendonca, Russell and Rybkin, Oleh and Daniilidis, Kostas and Hafner, Danijar and Pathak, Deepak},
  year = {2021},
  month = oct,
  number = {arXiv:2110.09514},
  eprint = {2110.09514},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.09514},
  urldate = {2024-08-05},
  abstract = {How can artificial agents learn to solve many diverse tasks in complex visual environments in the absence of any supervision? We decompose this question into two problems: discovering new goals and learning to reliably achieve them. We introduce Latent Explorer Achiever (LEXA), a unified solution to these that learns a world model from image inputs and uses it to train an explorer and an achiever policy from imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. After the unsupervised phase, LEXA solves tasks specified as goal images zero-shot without any additional learning. LEXA substantially outperforms previous approaches to unsupervised goal-reaching, both on prior benchmarks and on a new challenging benchmark with a total of 40 test tasks spanning across four standard robotic manipulation and locomotion domains. LEXA further achieves goals that require interacting with multiple objects in sequence. Finally, to demonstrate the scalability and generality of LEXA, we train a single general agent across four distinct environments. Code and videos at https://orybkin.github.io/lexa/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/9EZFZFJW/Mendonca et al. - 2021 - Discovering and Achieving Goals via World Models.pdf;/home/james/Zotero/storage/5VB4QCT2/2110.html}
}

@misc{mentzerFiniteScalarQuantization2023,
  title = {Finite {{Scalar Quantization}}: {{VQ-VAE Made Simple}}},
  shorttitle = {Finite {{Scalar Quantization}}},
  author = {Mentzer, Fabian and Minnen, David and Agustsson, Eirikur and Tschannen, Michael},
  year = {2023},
  month = sep,
  number = {arXiv:2309.15505},
  eprint = {2309.15505},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.15505},
  urldate = {2023-09-28},
  abstract = {We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Mentzer et al_2023_Finite Scalar Quantization.pdf;/home/james/Zotero/storage/ECRYHXDS/2309.html}
}

@misc{mialonAugmentedLanguageModels2023,
  title = {Augmented {{Language Models}}: A {{Survey}}},
  shorttitle = {Augmented {{Language Models}}},
  author = {Mialon, Gr{\'e}goire and Dess{\`i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and {Dwivedi-Yu}, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.07842},
  eprint = {2302.07842},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.07842},
  urldate = {2023-03-04},
  abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,LMs},
  file = {/home/james/Documents/zotero/Mialon et al_2023_Augmented Language Models.pdf;/home/james/Zotero/storage/YIHUTIQI/2302.html}
}

@misc{micheliTransformersAreSampleEfficient2023,
  title = {Transformers Are {{Sample-Efficient World Models}}},
  author = {Micheli, Vincent and Alonso, Eloi and Fleuret, Fran{\c c}ois},
  year = {2023},
  month = mar,
  number = {arXiv:2209.00588},
  eprint = {2209.00588},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.00588},
  urldate = {2023-03-04},
  abstract = {Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Micheli et al_2023_Transformers are Sample-Efficient World Models.pdf;/home/james/Zotero/storage/CFS39CM5/2209.html}
}

@misc{mikulikMetatrainedAgentsImplement2020,
  title = {Meta-Trained Agents Implement {{Bayes-optimal}} Agents},
  author = {Mikulik, Vladimir and Del{\'e}tang, Gr{\'e}goire and McGrath, Tom and Genewein, Tim and Martic, Miljan and Legg, Shane and Ortega, Pedro A.},
  year = {2020},
  month = oct,
  number = {arXiv:2010.11223},
  eprint = {2010.11223},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11223},
  urldate = {2024-07-06},
  abstract = {Memory-based meta-learning is a powerful technique to build agents that adapt fast to any task within a target distribution. A previous theoretical study has argued that this remarkable performance is because the meta-training protocol incentivises agents to behave Bayes-optimally. We empirically investigate this claim on a number of prediction and bandit tasks. Inspired by ideas from theoretical computer science, we show that meta-learned and Bayes-optimal agents not only behave alike, but they even share a similar computational structure, in the sense that one agent system can approximately simulate the other. Furthermore, we show that Bayes-optimal agents are fixed points of the meta-learning dynamics. Our results suggest that memory-based meta-learning might serve as a general technique for numerically approximating Bayes-optimal agents - that is, even for task distributions for which we currently don't possess tractable models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/james/Zotero/storage/TG3MM2M7/Mikulik et al. - 2020 - Meta-trained agents implement Bayes-optimal agents.pdf;/home/james/Zotero/storage/PK2QS49T/2010.html}
}

@misc{mindermannPrioritizedTrainingPoints2022,
  title = {Prioritized {{Training}} on {{Points}} That Are {{Learnable}}, {{Worth Learning}}, and {{Not Yet Learnt}}},
  author = {Mindermann, S{\"o}ren and Brauner, Jan and Razzak, Muhammed and Sharma, Mrinank and Kirsch, Andreas and Xu, Winnie and H{\"o}ltgen, Benedikt and Gomez, Aidan N. and Morisot, Adrien and Farquhar, Sebastian and Gal, Yarin},
  year = {2022},
  month = sep,
  number = {arXiv:2206.07137},
  eprint = {2206.07137},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.07137},
  urldate = {2023-03-04},
  abstract = {Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2\% higher final accuracy than uniform data shuffling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Mindermann et al_2022_Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet.pdf;/home/james/Zotero/storage/ALCEZS37/2206.html}
}

@misc{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  number = {arXiv:1602.01783},
  eprint = {1602.01783},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.01783},
  urldate = {2023-07-24},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Mnih et al_2016_Asynchronous Methods for Deep Reinforcement Learning.pdf;/home/james/Zotero/storage/PFJ2JYSL/1602.html}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2024-11-24},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science}
}

@misc{moerlandModelbasedReinforcementLearning2022,
  title = {Model-Based {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Model-Based {{Reinforcement Learning}}},
  author = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
  year = {2022},
  month = mar,
  number = {arXiv:2006.16712},
  eprint = {2006.16712},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.16712},
  urldate = {2023-03-04},
  abstract = {Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a important challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two sections, we also discuss implicit model-based RL as an end-to-end alternative for model learning and planning, and we cover the potential benefits of model-based RL. Along the way, the survey also draws connections to several related RL fields, like hierarchical RL and transfer learning. Altogether, the survey presents a broad conceptual overview of the combination of planning and learning for MDP optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Moerland et al_2022_Model-based Reinforcement Learning.pdf;/home/james/Zotero/storage/HQKG4HBE/2006.html}
}

@misc{mohanStructureReinforcementLearning2023,
  title = {Structure in {{Reinforcement Learning}}: {{A Survey}} and {{Open Problems}}},
  shorttitle = {Structure in {{Reinforcement Learning}}},
  author = {Mohan, Aditya and Zhang, Amy and Lindauer, Marius},
  year = {2023},
  month = aug,
  number = {arXiv:2306.16021},
  eprint = {2306.16021},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-15},
  abstract = {Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing various real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning problem, and classify these methods into distinct patterns of incorporating structure. By leveraging this comprehensive framework, we provide valuable insights into the challenges of structured RL and lay the groundwork for a design pattern perspective on RL research. This novel perspective paves the way for future advancements and aids in developing more effective and efficient RL algorithms that can potentially handle real-world scenarios better.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Mohan et al_2023_Structure in Reinforcement Learning.pdf;/home/james/Zotero/storage/AVLI49XS/2306.html}
}

@misc{moireLanguageModelsAre2021,
  title = {Language Models Are 0-Shot Interpreters},
  author = {{moire}},
  year = {2021},
  month = feb,
  urldate = {2023-03-04},
  abstract = {Mechanisms of meta-learning, beating few-shot benchmark performance with 0-shot prompts, and Bayesian analysis of prompt ablation},
  chapter = {GPT-3},
  howpublished = {https://generative.ink/posts/language-models-are-0-shot-interpreters/},
  langid = {english}
}

@misc{mordatchEmergenceGroundedCompositional2018,
  title = {Emergence of {{Grounded Compositional Language}} in {{Multi-Agent Populations}}},
  author = {Mordatch, Igor and Abbeel, Pieter},
  year = {2018},
  month = jul,
  number = {arXiv:1703.04908},
  eprint = {1703.04908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.04908},
  urldate = {2023-03-04},
  abstract = {By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/james/Documents/zotero/Mordatch_Abbeel_2018_Emergence of Grounded Compositional Language in Multi-Agent Populations.pdf;/home/james/Zotero/storage/E3HBUU77/1703.html}
}

@misc{MTIFreeFullText,
  title = {{{MTI}} {\textbar} {{Free Full-Text}} {\textbar} {{A Survey}} on the {{Procedural Generation}} of {{Virtual Worlds}}},
  urldate = {2023-05-14},
  howpublished = {https://www.mdpi.com/2414-4088/1/4/27},
  file = {/home/james/Zotero/storage/M7HE95LA/27.html}
}

@article{n/aModelbasedRLTagN/A,
  title = {Model-Based {{RL}} Tag},
  author = {N/A},
  year = {N/A},
  urldate = {2023-04-03},
  abstract = {Bibliography for tag {$<$}code{$>$}reinforcement-learning/\hspace{0pt}model{$<$}/code{$>$}, most recent first: 4 {$<$}a class='icon-not link-annotated-not' href='/doc/reinforcement-learning/model/index\#see-alsos'{$>$}related tags{$<$}/a{$>$}, 114 {$<$}a class='icon-not link-annotated-not' href='/doc/reinforcement-learning/model/index\#links'{$>$}annotations{$<$}/a{$>$}, \& 12 {$<$}a class='icon-not link-annotated-not' href='/doc/reinforcement-learning/model/index\#miscellaneous'{$>$}links{$<$}/a{$>$} ({$<$}a href='/doc/reinforcement-learning/index' class='link-page link-tag directory-indexes-upwards link-annotated link-annotated-partial' data-link-icon='arrow-up-left' data-link-icon-type='svg' rel='tag' title='Link to parent directory'{$>$}parent{$<$}/a{$>$}).},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/},
  langid = {american},
  file = {/home/james/Zotero/storage/66C6VKTS/index.html}
}

@misc{nachumBridgingGapValue2017,
  title = {Bridging the {{Gap Between Value}} and {{Policy Based Reinforcement Learning}}},
  author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  year = {2017},
  month = nov,
  number = {arXiv:1702.08892},
  eprint = {1702.08892},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.08892},
  urldate = {2024-07-06},
  abstract = {We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/EUFP7A46/Nachum et al. - 2017 - Bridging the Gap Between Value and Policy Based Re.pdf;/home/james/Zotero/storage/5E482LD7/1702.html}
}

@misc{nachumDataEfficientHierarchicalReinforcement2018,
  title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},
  author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  year = {2018},
  month = oct,
  number = {arXiv:1805.08296},
  eprint = {1805.08296},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.08296},
  urldate = {2023-03-04},
  abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Data-efficient RL,Hierarchical RL,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Nachum et al_2018_Data-Efficient Hierarchical Reinforcement Learning.pdf;/home/james/Zotero/storage/L8IBTRN9/1805.html}
}

@misc{nautaNeuralPrototypeTrees2021,
  title = {Neural {{Prototype Trees}} for {{Interpretable Fine-grained Image Recognition}}},
  author = {Nauta, Meike and {van Bree}, Ron and Seifert, Christin},
  year = {2021},
  month = apr,
  number = {arXiv:2012.02046},
  eprint = {2012.02046},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.02046},
  urldate = {2023-03-04},
  abstract = {Prototype-based methods use interpretable representations to address the black-box nature of deep learning models, in contrast to post-hoc explanation methods that only approximate such models. We propose the Neural Prototype Tree (ProtoTree), an intrinsically interpretable deep learning method for fine-grained image recognition. ProtoTree combines prototype learning with decision trees, and thus results in a globally interpretable model by design. Additionally, ProtoTree can locally explain a single prediction by outlining a decision path through the tree. Each node in our binary tree contains a trainable prototypical part. The presence or absence of this learned prototype in an image determines the routing through a node. Decision making is therefore similar to human reasoning: Does the bird have a red throat? And an elongated beak? Then it's a hummingbird! We tune the accuracy-interpretability trade-off using ensemble methods, pruning and binarizing. We apply pruning without sacrificing accuracy, resulting in a small tree with only 8 learned prototypes along a path to classify a bird from 200 species. An ensemble of 5 ProtoTrees achieves competitive accuracy on the CUB-200- 2011 and Stanford Cars data sets. Code is available at https://github.com/M-Nauta/ProtoTree},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Nauta et al_2021_Neural Prototype Trees for Interpretable Fine-grained Image Recognition.pdf;/home/james/Zotero/storage/FZU4FHSL/2012.html}
}

@misc{nikishinPrimacyBiasDeep2022,
  title = {The {{Primacy Bias}} in {{Deep Reinforcement Learning}}},
  author = {Nikishin, Evgenii and Schwarzer, Max and D'Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron},
  year = {2022},
  month = may,
  number = {arXiv:2205.07802},
  eprint = {2205.07802},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.07802},
  urldate = {2024-07-06},
  abstract = {This work identifies a common flaw of deep reinforcement learning (RL) algorithms: a tendency to rely on early interactions and ignore useful evidence encountered later. Because of training on progressively growing datasets, deep RL agents incur a risk of overfitting to earlier experiences, negatively affecting the rest of the learning process. Inspired by cognitive science, we refer to this effect as the primacy bias. Through a series of experiments, we dissect the algorithmic aspects of deep RL that exacerbate this bias. We then propose a simple yet generally-applicable mechanism that tackles the primacy bias by periodically resetting a part of the agent. We apply this mechanism to algorithms in both discrete (Atari 100k) and continuous action (DeepMind Control Suite) domains, consistently improving their performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/XMQV575X/Nikishin et al. - 2022 - The Primacy Bias in Deep Reinforcement Learning.pdf;/home/james/Zotero/storage/DAHTSBAK/2205.html}
}

@misc{norooziUnsupervisedLearningVisual2017,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  year = {2017},
  month = aug,
  number = {arXiv:1603.09246},
  eprint = {1603.09246},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.09246},
  urldate = {2024-11-24},
  abstract = {In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Zotero/storage/8QF5DCGM/Noroozi and Favaro - 2017 - Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.pdf;/home/james/Zotero/storage/2VRK26Q3/1603.html}
}

@misc{ohValuePredictionNetwork2017,
  title = {Value {{Prediction Network}}},
  author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
  year = {2017},
  month = nov,
  number = {arXiv:1707.03497},
  eprint = {1707.03497},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.03497},
  urldate = {2024-11-24},
  abstract = {This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/GTEJFKTW/Oh et al. - 2017 - Value Prediction Network.pdf;/home/james/Zotero/storage/RWADTCAL/1707.html}
}

@misc{oordConditionalImageGeneration2016,
  title = {Conditional {{Image Generation}} with {{PixelCNN Decoders}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  number = {arXiv:1606.05328},
  eprint = {1606.05328},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.05328},
  urldate = {2024-03-26},
  abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/XD23XXA5/Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf;/home/james/Zotero/storage/DIB7AADX/1606.html}
}

@misc{oordNeuralDiscreteRepresentation2018,
  title = {Neural {{Discrete Representation Learning}}},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2018},
  month = may,
  number = {arXiv:1711.00937},
  eprint = {1711.00937},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.00937},
  urldate = {2023-03-04},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Oord et al_2018_Neural Discrete Representation Learning.pdf;/home/james/Zotero/storage/JSPM23UP/1711.html}
}

@misc{oordPixelRecurrentNeural2016,
  title = {Pixel {{Recurrent Neural Networks}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  year = {2016},
  month = aug,
  number = {arXiv:1601.06759},
  eprint = {1601.06759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1601.06759},
  urldate = {2023-03-25},
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/james/Documents/zotero/Oord et al_2016_Pixel Recurrent Neural Networks.pdf;/home/james/Zotero/storage/UVJX5KWJ/1601.html}
}

@misc{openaiDota2Large2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k e}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  month = dec,
  number = {arXiv:1912.06680},
  eprint = {1912.06680},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.06680},
  urldate = {2023-03-04},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/OpenAI et al_2019_Dota 2 with Large Scale Deep Reinforcement Learning.pdf;/home/james/Zotero/storage/HFQR3FB9/1912.html}
}

@misc{openaiSolvingRubiksCube2019,
  title = {Solving {{Rubik}}'s {{Cube}} with a {{Robot Hand}}},
  author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
  year = {2019},
  month = oct,
  number = {arXiv:1910.07113},
  eprint = {1910.07113},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.07113},
  urldate = {2024-11-24},
  abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/9JFJ5HT9/OpenAI et al. - 2019 - Solving Rubik's Cube with a Robot Hand.pdf;/home/james/Zotero/storage/523N9S2W/1910.html}
}

@misc{openendedlearningteamOpenEndedLearningLeads2021,
  title = {Open-{{Ended Learning Leads}} to {{Generally Capable Agents}}},
  author = {Open Ended Learning Team and Stooke, Adam and Mahajan, Anuj and Barros, Catarina and Deck, Charlie and Bauer, Jakob and Sygnowski, Jakub and Trebacz, Maja and Jaderberg, Max and Mathieu, Michael and McAleese, Nat and {Bradley-Schmieg}, Nathalie and Wong, Nathaniel and Porcel, Nicolas and Raileanu, Roberta and {Hughes-Fitt}, Steph and Dalibard, Valentin and Czarnecki, Wojciech Marian},
  year = {2021},
  month = jul,
  number = {arXiv:2107.12808},
  eprint = {2107.12808},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.12808},
  urldate = {2023-05-05},
  abstract = {In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/james/Documents/zotero/Open Ended Learning Team et al_2021_Open-Ended Learning Leads to Generally Capable Agents.pdf;/home/james/Zotero/storage/CDLSTY95/2107.html}
}

@misc{ortegaMetalearningSequentialStrategies2019,
  title = {Meta-Learning of {{Sequential Strategies}}},
  author = {Ortega, Pedro A. and Wang, Jane X. and Rowland, Mark and Genewein, Tim and {Kurth-Nelson}, Zeb and Pascanu, Razvan and Heess, Nicolas and Veness, Joel and Pritzel, Alex and Sprechmann, Pablo and Jayakumar, Siddhant M. and McGrath, Tom and Miller, Kevin and Azar, Mohammad and Osband, Ian and Rabinowitz, Neil and Gy{\"o}rgy, Andr{\'a}s and Chiappa, Silvia and Osindero, Simon and Teh, Yee Whye and {van Hasselt}, Hado and {de Freitas}, Nando and Botvinick, Matthew and Legg, Shane},
  year = {2019},
  month = jul,
  number = {arXiv:1905.03030},
  eprint = {1905.03030},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.03030},
  urldate = {2024-07-06},
  abstract = {In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/GL7B2485/Ortega et al. - 2019 - Meta-learning of Sequential Strategies.pdf;/home/james/Zotero/storage/FBGT5IBG/1905.html}
}

@misc{osbandBehaviourSuiteReinforcement2020,
  title = {Behaviour {{Suite}} for {{Reinforcement Learning}}},
  author = {Osband, Ian and Doron, Yotam and Hessel, Matteo and Aslanides, John and Sezener, Eren and Saraiva, Andre and McKinney, Katrina and Lattimore, Tor and Szepesvari, Csaba and Singh, Satinder and Van Roy, Benjamin and Sutton, Richard and Silver, David and Van Hasselt, Hado},
  year = {2020},
  month = feb,
  number = {arXiv:1908.03568},
  eprint = {1908.03568},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.03568},
  urldate = {2023-03-04},
  abstract = {This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github.com/deepmind/bsuite, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Osband et al_2020_Behaviour Suite for Reinforcement Learning.pdf;/home/james/Zotero/storage/Y6L69IQ5/1908.html}
}

@misc{ostrovskiDifficultyPassiveLearning2021,
  title = {The {{Difficulty}} of {{Passive Learning}} in {{Deep Reinforcement Learning}}},
  author = {Ostrovski, Georg and Castro, Pablo Samuel and Dabney, Will},
  year = {2021},
  month = oct,
  number = {arXiv:2110.14020},
  eprint = {2110.14020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.14020},
  urldate = {2024-07-06},
  abstract = {Learning to act from observational data without active environmental interaction is a well-known challenge in Reinforcement Learning (RL). Recent approaches involve constraints on the learned policy or conservative updates, preventing strong deviations from the state-action distribution of the dataset. Although these methods are evaluated using non-linear function approximation, theoretical justifications are mostly limited to the tabular or linear cases. Given the impressive results of deep reinforcement learning, we argue for a need to more clearly understand the challenges in this setting. In the vein of Held \& Hein's classic 1963 experiment, we propose the "tandem learning" experimental paradigm which facilitates our empirical analysis of the difficulties in offline reinforcement learning. We identify function approximation in conjunction with fixed data distributions as the strongest factors, thereby extending but also challenging hypotheses stated in past work. Our results provide relevant insights for offline deep reinforcement learning, while also shedding new light on phenomena observed in the online case of learning control.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/TAYNXC9D/Ostrovski et al. - 2021 - The Difficulty of Passive Learning in Deep Reinfor.pdf;/home/james/Zotero/storage/Q8CP7I8C/2110.html}
}

@inproceedings{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Gray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = oct,
  urldate = {2024-11-24},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  langid = {english},
  file = {/home/james/Zotero/storage/KQCF6BYN/Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf}
}

@misc{panTrustModelWhen2020,
  title = {Trust the {{Model When It Is Confident}}: {{Masked Model-based Actor-Critic}}},
  shorttitle = {Trust the {{Model When It Is Confident}}},
  author = {Pan, Feiyang and He, Jia and Tu, Dandan and He, Qing},
  year = {2020},
  month = oct,
  number = {arXiv:2010.04893},
  eprint = {2010.04893},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.04893},
  urldate = {2023-08-15},
  abstract = {It is a popular belief that model-based Reinforcement Learning (RL) is more sample efficient than model-free RL, but in practice, it is not always true due to overweighed model errors. In complex and noisy settings, model-based RL tends to have trouble using the model if it does not know when to trust the model. In this work, we find that better model usage can make a huge difference. We show theoretically that if the use of model-generated data is restricted to state-action pairs where the model error is small, the performance gap between model and real rollouts can be reduced. It motivates us to use model rollouts only when the model is confident about its predictions. We propose Masked Model-based Actor-Critic (M2AC), a novel policy optimization algorithm that maximizes a model-based lower-bound of the true value function. M2AC implements a masking mechanism based on the model's uncertainty to decide whether its prediction should be used or not. Consequently, the new algorithm tends to give robust policy improvements. Experiments on continuous control benchmarks demonstrate that M2AC has strong performance even when using long model rollouts in very noisy environments, and it significantly outperforms previous state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Pan et al_2020_Trust the Model When It Is Confident.pdf;/home/james/Zotero/storage/5MN7SUX3/2010.html}
}

@misc{parisiUnsurprisingEffectivenessPreTrained2022,
  title = {The {{Unsurprising Effectiveness}} of {{Pre-Trained Vision Models}} for {{Control}}},
  author = {Parisi, Simone and Rajeswaran, Aravind and Purushwalkam, Senthil and Gupta, Abhinav},
  year = {2022},
  month = aug,
  number = {arXiv:2203.03580},
  eprint = {2203.03580},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-12},
  abstract = {Recent years have seen the emergence of pre-trained representations as a powerful abstraction for AI applications in computer vision, natural language, and speech. However, policy learning for control is still dominated by a tabula-rasa learning paradigm, with visuo-motor policies often trained from scratch using data from deployment environments. In this context, we revisit and study the role of pre-trained visual representations for control, and in particular representations trained on large-scale computer vision datasets. Through extensive empirical evaluation in diverse control domains (Habitat, DeepMind Control, Adroit, Franka Kitchen), we isolate and study the importance of different representation training methods, data augmentations, and feature hierarchies. Overall, we find that pre-trained visual representations can be competitive or even better than ground-truth state representations to train control policies. This is in spite of using only out-of-domain data from standard vision datasets, without any in-domain data from the deployment environments. Source code and more at https://sites.google.com/view/pvr-control.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Parisi et al_2022_The Unsurprising Effectiveness of Pre-Trained Vision Models for Control.pdf;/home/james/Zotero/storage/B5G2D3XL/2203.html}
}

@misc{parkNerfiesDeformableNeural2021,
  title = {Nerfies: {{Deformable Neural Radiance Fields}}},
  shorttitle = {Nerfies},
  author = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B. and Seitz, Steven M. and {Martin-Brualla}, Ricardo},
  year = {2021},
  month = sep,
  number = {arXiv:2011.12948},
  eprint = {2011.12948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.12948},
  urldate = {2023-05-14},
  abstract = {We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub "nerfies." We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/home/james/Documents/zotero/Park et al_2021_Nerfies.pdf;/home/james/Zotero/storage/LBMCR3B2/2011.html}
}

@misc{parmasPIPPSFlexibleModelBased2019,
  title = {{{PIPPS}}: {{Flexible Model-Based Policy Search Robust}} to the {{Curse}} of {{Chaos}}},
  shorttitle = {{{PIPPS}}},
  author = {Parmas, Paavo and Rasmussen, Carl Edward and Peters, Jan and Doya, Kenji},
  year = {2019},
  month = feb,
  number = {arXiv:1902.01240},
  eprint = {1902.01240},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-10-16},
  abstract = {Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimization. Our experiments in model-based reinforcement learning imply that the problem is not just a numerical issue, but it may be caused by a fundamental chaos-like nature of long chains of nonlinear computations. Not only do the magnitudes of the gradients become large, the direction of the gradients becomes essentially random. We show that reparameterization gradients suffer from the problem, while likelihood ratio gradients are robust. Using our insights, we develop a model-based policy search framework, Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is easily extensible, and allows for almost arbitrary models and policies, while simultaneously matching the performance of previous data-efficient learning algorithms. Finally, we invent the total propagation algorithm, which efficiently computes a union over all pathwise derivative depths during a single backwards pass, automatically giving greater weight to estimators with lower variance, sometimes improving over reparameterization gradients by \$10{\textasciicircum}6\$ times.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Parmas et al_2019_PIPPS.pdf;/home/james/Zotero/storage/C4WA4TB5/1902.html}
}

@misc{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  number = {arXiv:1912.01703},
  eprint = {1912.01703},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.01703},
  urldate = {2024-11-23},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/9NN95CNL/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf;/home/james/Zotero/storage/2R5WC9HL/1912.html}
}

@article{pateriaHierarchicalReinforcementLearning2021,
  title = {Hierarchical {{Reinforcement Learning}}: {{A Comprehensive Survey}}},
  shorttitle = {Hierarchical {{Reinforcement Learning}}},
  author = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  year = {2021},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {5},
  pages = {109:1--109:35},
  issn = {0360-0300},
  doi = {10.1145/3453160},
  urldate = {2023-03-04},
  abstract = {Hierarchical Reinforcement Learning (HRL) enables autonomous decomposition of challenging long-horizon decision-making tasks into simpler subtasks. During the past years, the landscape of HRL research has grown profoundly, resulting in copious approaches. A comprehensive overview of this vast landscape is necessary to study HRL in an organized manner. We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL. Furthermore, we outline a few suitable task domains for evaluating the HRL approaches and a few interesting examples of the practical applications of HRL in the Supplementary Material.},
  keywords = {Hierarchical reinforcement learning,hierarchical reinforcement learning survey,hierarchical reinforcement learning taxonomy,skill discovery,subtask discovery}
}

@misc{pathakCuriositydrivenExplorationSelfsupervised2017,
  title = {Curiosity-Driven {{Exploration}} by {{Self-supervised Prediction}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  month = may,
  number = {arXiv:1705.05363},
  eprint = {1705.05363},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.05363},
  urldate = {2023-03-04},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Exploration,RL,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Pathak et al_2017_Curiosity-driven Exploration by Self-supervised Prediction.pdf;/home/james/Zotero/storage/WXZTQ9N3/1705.html}
}

@misc{PDFAllElse,
  title = {({{PDF}}) {{All Else Being Equal Be Empowered}}},
  urldate = {2023-03-04},
  howpublished = {https://www.researchgate.net/publication/221531178\_All\_Else\_Being\_Equal\_Be\_Empowered},
  file = {/home/james/Zotero/storage/TD74WFAF/221531178_All_Else_Being_Equal_Be_Empowered.html}
}

@misc{pengOpenScene3DScene2023,
  title = {{{OpenScene}}: {{3D Scene Understanding}} with {{Open Vocabularies}}},
  shorttitle = {{{OpenScene}}},
  author = {Peng, Songyou and Genova, Kyle and Jiang, Chiyu "Max" and Tagliasacchi, Andrea and Pollefeys, Marc and Funkhouser, Thomas},
  year = {2023},
  month = apr,
  number = {arXiv:2211.15654},
  eprint = {2211.15654},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.15654},
  urldate = {2024-07-06},
  abstract = {Traditional 3D scene understanding approaches rely on labeled 3D datasets to train a model for a single task with supervision. We propose OpenScene, an alternative approach where a model predicts dense features for 3D scene points that are co-embedded with text and image pixels in CLIP feature space. This zero-shot approach enables task-agnostic training and open-vocabulary queries. For example, to perform SOTA zero-shot 3D semantic segmentation it first infers CLIP features for every 3D point and later classifies them based on similarities to embeddings of arbitrary class labels. More interestingly, it enables a suite of open-vocabulary scene understanding applications that have never been done before. For example, it allows a user to enter an arbitrary text query and then see a heat map indicating which parts of a scene match. Our approach is effective at identifying objects, materials, affordances, activities, and room types in complex 3D scenes, all using a single model trained without any labeled 3D data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Zotero/storage/V4Y2NIE2/Peng et al. - 2023 - OpenScene 3D Scene Understanding with Open Vocabu.pdf;/home/james/Zotero/storage/NQ2ZC575/2211.html}
}

@misc{peroneAppreciatingComplexityLarge2023,
  title = {Appreciating the Complexity of Large Language Models Data Pipelines {\textbar} {{Christian S}}. {{Perone}}},
  author = {Perone, Christian S.},
  year = {2023},
  month = jun,
  urldate = {2023-06-06},
  abstract = {This article provides a short introduction to the pipeline used to create the data to train large language models (LLMs) such as LLaMA using Common Crawl (CC).},
  langid = {american},
  keywords = {Infra,LMs},
  file = {/home/james/Zotero/storage/2Y2TLYZT/appreciating-llms-data-pipelines.html}
}

@misc{pertschLongHorizonVisualPlanning2020,
  title = {Long-{{Horizon Visual Planning}} with {{Goal-Conditioned Hierarchical Predictors}}},
  author = {Pertsch, Karl and Rybkin, Oleh and Ebert, Frederik and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  year = {2020},
  month = nov,
  number = {arXiv:2006.13205},
  eprint = {2006.13205},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.13205},
  urldate = {2023-03-04},
  abstract = {The ability to predict and plan into the future is fundamental for agents acting in the world. To reach a faraway goal, we predict trajectories at multiple timescales, first devising a coarse plan towards the goal and then gradually filling in details. In contrast, current learning approaches for visual prediction and planning fail on long-horizon tasks as they generate predictions (1) without considering goal information, and (2) at the finest temporal resolution, one step at a time. In this work we propose a framework for visual prediction and planning that is able to overcome both of these limitations. First, we formulate the problem of predicting towards a goal and propose the corresponding class of latent space goal-conditioned predictors (GCPs). GCPs significantly improve planning efficiency by constraining the search space to only those trajectories that reach the goal. Further, we show how GCPs can be naturally formulated as hierarchical models that, given two observations, predict an observation between them, and by recursively subdividing each part of the trajectory generate complete sequences. This divide-and-conquer strategy is effective at long-term prediction, and enables us to design an effective hierarchical planning algorithm that optimizes trajectories in a coarse-to-fine manner. We show that by using both goal-conditioning and hierarchical prediction, GCPs enable us to solve visual planning tasks with much longer horizon than previously possible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Pertsch et al_2020_Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors.pdf;/home/james/Zotero/storage/VZH7U2HM/2006.html}
}

@misc{pinedaMBRLLibModularLibrary2021,
  title = {{{MBRL-Lib}}: {{A Modular Library}} for {{Model-based Reinforcement Learning}}},
  shorttitle = {{{MBRL-Lib}}},
  author = {Pineda, Luis and Amos, Brandon and Zhang, Amy and Lambert, Nathan O. and Calandra, Roberto},
  year = {2021},
  month = apr,
  number = {arXiv:2104.10159},
  eprint = {2104.10159},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.10159},
  urldate = {2024-11-23},
  abstract = {Model-based reinforcement learning is a compelling framework for data-efficient learning of agents that interact with the world. This family of algorithms has many subcomponents that need to be carefully selected and tuned. As a result the entry-bar for researchers to approach the field and to deploy it in real-world tasks can be daunting. In this paper, we present MBRL-Lib -- a machine learning library for model-based reinforcement learning in continuous state-action spaces based on PyTorch. MBRL-Lib is designed as a platform for both researchers, to easily develop, debug and compare new algorithms, and non-expert user, to lower the entry-bar of deploying state-of-the-art algorithms. MBRL-Lib is open-source at https://github.com/facebookresearch/mbrl-lib.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/james/Zotero/storage/UAF48PXV/Pineda et al. - 2021 - MBRL-Lib A Modular Library for Model-based Reinforcement Learning.pdf;/home/james/Zotero/storage/UPVHFRVI/2104.html}
}

@misc{PromiseHierarchicalReinforcement,
  title = {The {{Promise}} of {{Hierarchical Reinforcement Learning}}},
  urldate = {2023-03-04},
  howpublished = {https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/}
}

@article{prudencioSurveyOfflineReinforcement2024,
  title = {A {{Survey}} on {{Offline Reinforcement Learning}}: {{Taxonomy}}, {{Review}}, and {{Open Problems}}},
  shorttitle = {A {{Survey}} on {{Offline Reinforcement Learning}}},
  author = {Prudencio, Rafael Figueiredo and Maximo, Marcos R. O. A. and Colombini, Esther Luna},
  year = {2024},
  month = aug,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {35},
  number = {8},
  eprint = {2203.01387},
  primaryclass = {cs, stat},
  pages = {10237--10257},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2023.3250269},
  urldate = {2024-10-03},
  abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks' properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/NBSEUGUD/Prudencio et al. - 2024 - A Survey on Offline Reinforcement Learning Taxono.pdf;/home/james/Zotero/storage/ISI3VR3T/2203.html}
}

@misc{qiaoPrimacyBiasModelbased2023,
  title = {The Primacy Bias in {{Model-based RL}}},
  author = {Qiao, Zhongjian and Lyu, Jiafei and Li, Xiu},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15017},
  eprint = {2310.15017},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.15017},
  urldate = {2024-07-06},
  abstract = {The primacy bias in deep reinforcement learning (DRL), which refers to the agent's tendency to overfit early data and lose the ability to learn from new data, can significantly decrease the performance of DRL algorithms. Previous studies have shown that employing simple techniques, such as resetting the agent's parameters, can substantially alleviate the primacy bias. However, we observe that resetting the agent's parameters harms its performance in the context of model-based reinforcement learning (MBRL). In fact, on further investigation, we find that the primacy bias in MBRL differs from that in model-free RL. In this work, we focus on investigating the primacy bias in MBRL and propose world model resetting, which works in MBRL. We apply our method to two different MBRL algorithms, MBPO and DreamerV2. We validate the effectiveness of our method on multiple continuous control tasks on MuJoCo and DeepMind Control Suite, as well as discrete control tasks on Atari 100k benchmark. The results show that world model resetting can significantly alleviate the primacy bias in model-based setting and improve algorithm's performance. We also give a guide on how to perform world model resetting effectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/MPWMEM9E/Qiao et al. - 2023 - The primacy bias in Model-based RL.pdf;/home/james/Zotero/storage/MTLGJGSQ/2310.html}
}

@misc{raistrickInfinitePhotorealisticWorlds2023,
  title = {Infinite {{Photorealistic Worlds}} Using {{Procedural Generation}}},
  author = {Raistrick, Alexander and Lipson, Lahav and Ma, Zeyu and Mei, Lingjie and Wang, Mingzhe and Zuo, Yiming and Kayan, Karhan and Wen, Hongyu and Han, Beining and Wang, Yihan and Newell, Alejandro and Law, Hei and Goyal, Ankit and Yang, Kaiyu and Deng, Jia},
  year = {2023},
  month = jun,
  number = {arXiv:2306.09310},
  eprint = {2306.09310},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.09310},
  urldate = {2024-09-06},
  abstract = {We introduce Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition. Infinigen offers broad coverage of objects and scenes in the natural world including plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and snow. Infinigen can be used to generate unlimited, diverse training data for a wide range of computer vision tasks including object detection, semantic segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a useful resource for computer vision research and beyond. Please visit https://infinigen.org for videos, code and pre-generated data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Zotero/storage/JARAQVRF/Raistrick et al. - 2023 - Infinite Photorealistic Worlds using Procedural Ge.pdf;/home/james/Zotero/storage/TBUCUYNR/2306.html}
}

@misc{raschkaUnderstandingLargeLanguage2023,
  title = {Understanding {{Large Language Models}}},
  author = {Raschka, Sebastian},
  year = {2023},
  month = apr,
  urldate = {2023-05-01},
  abstract = {A Cross-Section of the Most Relevant Literature To Get Up to Speed},
  howpublished = {https://magazine.sebastianraschka.com/p/understanding-large-language-models},
  langid = {english},
  file = {/home/james/Zotero/storage/PHCLQ65V/understanding-large-language-models.html}
}

@misc{reedGeneralistAgent2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and {Barth-Maron}, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and {de Freitas}, Nando},
  year = {2022},
  month = nov,
  number = {arXiv:2205.06175},
  eprint = {2205.06175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.06175},
  urldate = {2023-03-15},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics,Foundation Models,RL},
  file = {/home/james/Documents/zotero/Reed et al_2022_A Generalist Agent2.pdf;/home/james/Zotero/storage/3DZLWG66/2205.html}
}

@misc{reedNeuralProgrammerInterpreters2016,
  title = {Neural {{Programmer-Interpreters}}},
  author = {Reed, Scott and {de Freitas}, Nando},
  year = {2016},
  month = feb,
  number = {arXiv:1511.06279},
  eprint = {1511.06279},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06279},
  urldate = {2023-03-04},
  abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/james/Documents/zotero/Reed_de Freitas_2016_Neural Programmer-Interpreters.pdf;/home/james/Zotero/storage/FQT44SSY/1511.html}
}

@misc{RLExplorationTag,
  title = {{{RL}} Exploration Tag {$\cdot$} {{Gwern}}.Net},
  urldate = {2023-04-03},
  howpublished = {https://gwern.net/doc/reinforcement-learning/exploration/index}
}

@book{robertsPrinciplesDeepLearning2022,
  title = {The {{Principles}} of {{Deep Learning Theory}}},
  author = {Roberts, Daniel A. and Yaida, Sho and Hanin, Boris},
  year = {2022},
  month = may,
  eprint = {2106.10165},
  primaryclass = {hep-th, stat},
  doi = {10.1017/9781009023405},
  urldate = {2024-07-06},
  abstract = {This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,High Energy Physics - Theory,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/P2PW9IUG/Roberts et al. - 2022 - The Principles of Deep Learning Theory.pdf;/home/james/Zotero/storage/Y8PC9IUH/2106.html}
}

@misc{robineTransformerbasedWorldModels2023,
  title = {Transformer-Based {{World Models Are Happy With}} 100k {{Interactions}}},
  author = {Robine, Jan and H{\"o}ftmann, Marc and Uelwer, Tobias and Harmeling, Stefan},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07109},
  eprint = {2303.07109},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.07109},
  urldate = {2024-11-27},
  abstract = {Deep neural networks have been successful in many reinforcement learning settings. However, compared to human learners they are overly data hungry. To build a sample-efficient world model, we apply a transformer to real-world episodes in an autoregressive manner: not only the compact latent states and the taken actions but also the experienced or predicted rewards are fed into the transformer, so that it can attend flexibly to all three modalities at different time steps. The transformer allows our world model to access previous states directly, instead of viewing them through a compressed recurrent state. By utilizing the Transformer-XL architecture, it is able to learn long-term dependencies while staying computationally efficient. Our transformer-based world model (TWM) generates meaningful, new experience, which is used to train a policy that outperforms previous model-free and model-based reinforcement learning algorithms on the Atari 100k benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/GVIXCBHN/Robine et al. - 2023 - Transformer-based World Models Are Happy With 100k Interactions.pdf;/home/james/Zotero/storage/KHNZK8JZ/2303.html}
}

@misc{rodriguez-galvezRoleEntropyReconstruction2023,
  title = {The {{Role}} of {{Entropy}} and {{Reconstruction}} in {{Multi-View Self-Supervised Learning}}},
  author = {{Rodr{\'i}guez-G{\'a}lvez}, Borja and Blaas, Arno and Rodr{\'i}guez, Pau and Goli{\'n}ski, Adam and Suau, Xavier and Ramapuram, Jason and Busbridge, Dan and Zappella, Luca},
  year = {2023},
  month = jul,
  number = {arXiv:2307.10907},
  eprint = {2307.10907},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-23},
  abstract = {The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients. Github repo: https://github.com/apple/ml-entropy-reconstruction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Rodríguez-Gálvez et al_2023_The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning.pdf;/home/james/Zotero/storage/WNL9H9R5/2307.html}
}

@misc{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10752},
  urldate = {2023-03-04},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Rombach et al_2022_High-Resolution Image Synthesis with Latent Diffusion Models.pdf;/home/james/Zotero/storage/I4ZHS4F6/2112.html}
}

@article{sabourDynamicRoutingCapsules,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  langid = {english},
  file = {/home/james/Documents/zotero/Sabour et al_Dynamic Routing Between Capsules.pdf}
}

@misc{sajjadiObjectSceneRepresentation2022,
  title = {Object {{Scene Representation Transformer}}},
  author = {Sajjadi, Mehdi S. M. and Duckworth, Daniel and Mahendran, Aravindh and {van Steenkiste}, Sjoerd and Paveti{\'c}, Filip and Lu{\v c}i{\'c}, Mario and Guibas, Leonidas J. and Greff, Klaus and Kipf, Thomas},
  year = {2022},
  month = oct,
  number = {arXiv:2206.06922},
  eprint = {2206.06922},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.06922},
  urldate = {2023-03-08},
  abstract = {A compositional understanding of the world in terms of objects and their geometry in 3D space is considered a cornerstone of human cognition. Facilitating the learning of such a representation in neural networks holds promise for substantially improving labeled data efficiency. As a key step in this direction, we make progress on the problem of learning 3D-consistent decompositions of complex scenes into individual objects in an unsupervised fashion. We introduce Object Scene Representation Transformer (OSRT), a 3D-centric model in which individual object representations naturally emerge through novel view synthesis. OSRT scales to significantly more complex scenes with larger diversity of objects and backgrounds than existing methods. At the same time, it is multiple orders of magnitude faster at compositional rendering thanks to its light field parametrization and the novel Slot Mixer decoder. We believe this work will not only accelerate future architecture exploration and scaling efforts, but it will also serve as a useful tool for both object-centric as well as neural scene representation learning communities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Sajjadi et al_2022_Object Scene Representation Transformer.pdf;/home/james/Zotero/storage/GD5CKTHK/2206.html}
}

@misc{salimansPixelCNNImprovingPixelCNN2017,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  year = {2017},
  month = jan,
  number = {arXiv:1701.05517},
  eprint = {1701.05517},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.05517},
  urldate = {2024-05-26},
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/2ISSIB53/Salimans et al. - 2017 - PixelCNN++ Improving the PixelCNN with Discretize.pdf;/home/james/Zotero/storage/QAFGMSSB/1701.html}
}

@misc{sarlinSuperGlueLearningFeature2020,
  title = {{{SuperGlue}}: {{Learning Feature Matching}} with {{Graph Neural Networks}}},
  shorttitle = {{{SuperGlue}}},
  author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2020},
  month = mar,
  number = {arXiv:1911.11763},
  eprint = {1911.11763},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.11763},
  urldate = {2023-06-08},
  abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at https://github.com/magicleap/SuperGluePretrainedNetwork.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Sarlin et al_2020_SuperGlue.pdf;/home/james/Zotero/storage/LGKPH5P9/1911.html}
}

@misc{sauerProjectedGANsConverge2021,
  title = {Projected {{GANs Converge Faster}}},
  author = {Sauer, Axel and Chitta, Kashyap and M{\"u}ller, Jens and Geiger, Andreas},
  year = {2021},
  month = nov,
  number = {arXiv:2111.01007},
  eprint = {2111.01007},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.01007},
  urldate = {2023-03-18},
  abstract = {Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr{\textbackslash}'echet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Sauer et al_2021_Projected GANs Converge Faster.pdf;/home/james/Zotero/storage/SUX2ALSE/2111.html}
}

@misc{schaulPrioritizedExperienceReplay2016,
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  year = {2016},
  month = feb,
  number = {arXiv:1511.05952},
  eprint = {1511.05952},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.05952},
  urldate = {2024-11-24},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/CHFNRVNH/Schaul et al. - 2016 - Prioritized Experience Replay.pdf;/home/james/Zotero/storage/J4MBXBBX/1511.html}
}

@article{schaulUniversalValueFunction,
  title = {Universal {{Value Function Approximators}}},
  author = {Schaul, Tom and Horgan, Dan and Gregor, Karol and Silver, David},
  abstract = {Value functions are a core component of reinforcement learning systems. The main idea is to to construct a single function approximator V (s; {\texttheta}) that estimates the long-term reward from any state s, using parameters {\texttheta}. In this paper we introduce universal value function approximators (UVFAs) V (s, g; {\texttheta}) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.},
  langid = {english},
  file = {/home/james/Documents/zotero/Schaul et al_Universal Value Function Approximators.pdf}
}

@article{schmidStudentGamesUnified2023,
  title = {Student of {{Games}}: {{A}} Unified Learning Algorithm for Both Perfect and Imperfect Information Games},
  shorttitle = {Student of {{Games}}},
  author = {Schmid, Martin and Moravcik, Matej and Burch, Neil and Kadlec, Rudolf and Davidson, Josh and Waugh, Kevin and Bard, Nolan and Timbers, Finbarr and Lanctot, Marc and Holland, G. Zacharias and Davoodi, Elnaz and Christianson, Alden and Bowling, Michael},
  year = {2023},
  month = nov,
  journal = {Science Advances},
  volume = {9},
  number = {46},
  eprint = {2112.03178},
  primaryclass = {cs},
  pages = {eadg3256},
  issn = {2375-2548},
  doi = {10.1126/sciadv.adg3256},
  urldate = {2023-11-24},
  abstract = {Games have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games --- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/IWBZN9GB/Schmid et al. - 2023 - Student of Games A unified learning algorithm for.pdf}
}

@misc{schmidtFastDataEfficientTraining2021,
  title = {Fast and {{Data-Efficient Training}} of {{Rainbow}}: An {{Experimental Study}} on {{Atari}}},
  shorttitle = {Fast and {{Data-Efficient Training}} of {{Rainbow}}},
  author = {Schmidt, Dominik and Schmied, Thomas},
  year = {2021},
  month = nov,
  number = {arXiv:2111.10247},
  eprint = {2111.10247},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.10247},
  urldate = {2023-03-04},
  abstract = {Across the Arcade Learning Environment, Rainbow achieves a level of performance competitive with humans and modern RL algorithms. However, attaining this level of performance requires large amounts of data and hardware resources, making research in this area computationally expensive and use in practical applications often infeasible. This paper's contribution is threefold: We (1) propose an improved version of Rainbow, seeking to drastically reduce Rainbow's data, training time, and compute requirements while maintaining its competitive performance; (2) we empirically demonstrate the effectiveness of our approach through experiments on the Arcade Learning Environment, and (3) we conduct a number of ablation studies to investigate the effect of the individual proposed modifications. Our improved version of Rainbow reaches a median human normalized score close to classic Rainbow's, while using 20 times less data and requiring only 7.5 hours of training time on a single GPU. We also provide our full implementation including pre-trained models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Schmidt_Schmied_2021_Fast and Data-Efficient Training of Rainbow.pdf;/home/james/Zotero/storage/M74K5ABX/2111.html}
}

@article{schrittwieserMasteringAtariGo2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  eprint = {1911.08265},
  primaryclass = {cs, stat},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  urldate = {2023-03-04},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Schrittwieser et al_2020_Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf;/home/james/Zotero/storage/XRX7QTMG/1911.html}
}

@misc{schroffFaceNetUnifiedEmbedding2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  month = jun,
  number = {arXiv:1503.03832},
  eprint = {1503.03832},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.03832},
  urldate = {2024-11-24},
  abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Zotero/storage/SEE6IFHK/Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition and Clustering.pdf;/home/james/Zotero/storage/IPCU2YUA/1503.html}
}

@misc{schulmanHighDimensionalContinuousControl2018,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = {2018},
  month = oct,
  number = {arXiv:1506.02438},
  eprint = {1506.02438},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02438},
  urldate = {2023-07-23},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/james/Documents/zotero/Schulman et al_2018_High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf;/home/james/Zotero/storage/WVY9PNF9/1506.html}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2023-05-02},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Schulman et al_2017_Proximal Policy Optimization Algorithms.pdf;/home/james/Zotero/storage/4AW5IADX/1707.html}
}

@misc{schulmanTrustRegionPolicy2017,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2017},
  month = apr,
  number = {arXiv:1502.05477},
  eprint = {1502.05477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.05477},
  urldate = {2023-05-02},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Schulman et al_2017_Trust Region Policy Optimization.pdf;/home/james/Zotero/storage/Y28SL8PZ/1502.html}
}

@misc{schwarzerBiggerBetterFaster2023,
  title = {Bigger, {{Better}}, {{Faster}}: {{Human-level Atari}} with Human-Level Efficiency},
  shorttitle = {Bigger, {{Better}}, {{Faster}}},
  author = {Schwarzer, Max and {Obando-Ceron}, Johan and Courville, Aaron and Bellemare, Marc and Agarwal, Rishabh and Castro, Pablo Samuel},
  year = {2023},
  month = nov,
  number = {arXiv:2305.19452},
  eprint = {2305.19452},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.19452},
  urldate = {2024-07-01},
  abstract = {We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger\_better\_faster.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/4UIE9QDK/Schwarzer et al. - 2023 - Bigger, Better, Faster Human-level Atari with hum.pdf;/home/james/Zotero/storage/5F7DCQN3/2305.html}
}

@misc{schwarzerDataEfficientReinforcementLearning2021,
  title = {Data-{{Efficient Reinforcement Learning}} with {{Self-Predictive Representations}}},
  author = {Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R. Devon and Courville, Aaron and Bachman, Philip},
  year = {2021},
  month = may,
  number = {arXiv:2007.05929},
  eprint = {2007.05929},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.05929},
  urldate = {2023-03-04},
  abstract = {While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations(SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55\% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. The code associated with this work is available at https://github.com/mila-iqia/spr},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Data-efficient RL,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Schwarzer et al_2021_Data-Efficient Reinforcement Learning with Self-Predictive Representations.pdf;/home/james/Zotero/storage/CJ3N5GZP/2007.html}
}

@misc{sekarPlanningExploreSelfSupervised2020,
  title = {Planning to {{Explore}} via {{Self-Supervised World Models}}},
  author = {Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  year = {2020},
  month = jun,
  number = {arXiv:2005.05960},
  eprint = {2005.05960},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.05960},
  urldate = {2023-03-04},
  abstract = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Sekar et al_2020_Planning to Explore via Self-Supervised World Models.pdf;/home/james/Zotero/storage/4BR97L5S/2005.html}
}

@misc{seoMaskedWorldModels2022,
  title = {Masked {{World Models}} for {{Visual Control}}},
  author = {Seo, Younggyo and Hafner, Danijar and Liu, Hao and Liu, Fangchen and James, Stephen and Lee, Kimin and Abbeel, Pieter},
  year = {2022},
  month = nov,
  number = {arXiv:2206.14244},
  eprint = {2206.14244},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.14244},
  urldate = {2023-03-04},
  abstract = {Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling approach achieves state-of-the-art performance on a variety of visual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7\% success rate on 50 visual robotic manipulation tasks from Meta-world, while the baseline achieves 67.9\%. Code is available on the project website: https://sites.google.com/view/mwm-rl.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Seo et al_2022_Masked World Models for Visual Control.pdf;/home/james/Zotero/storage/PNWMPJVN/2206.html}
}

@misc{sharmaDynamicsAwareUnsupervisedDiscovery2020,
  title = {Dynamics-{{Aware Unsupervised Discovery}} of {{Skills}}},
  author = {Sharma, Archit and Gu, Shixiang and Levine, Sergey and Kumar, Vikash and Hausman, Karol},
  year = {2020},
  month = feb,
  number = {arXiv:1907.01657},
  eprint = {1907.01657},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.01657},
  urldate = {2023-03-04},
  abstract = {Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Sharma et al_2020_Dynamics-Aware Unsupervised Discovery of Skills.pdf;/home/james/Zotero/storage/HSAYRY9F/1907.html}
}

@misc{shiSkillbasedModelbasedReinforcement2022,
  title = {Skill-Based {{Model-based Reinforcement Learning}}},
  author = {Shi, Lucy Xiaoyang and Lim, Joseph J. and Lee, Youngwoon},
  year = {2022},
  month = dec,
  number = {arXiv:2207.07560},
  eprint = {2207.07560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.07560},
  urldate = {2023-03-04},
  abstract = {Model-based reinforcement learning (RL) is a sample-efficient way of learning complex behaviors by leveraging a learned single-step dynamics model to plan actions in imagination. However, planning every action for long-horizon tasks is not practical, akin to a human planning out every muscle movement. Instead, humans efficiently plan with high-level skills to solve complex tasks. From this intuition, we propose a Skill-based Model-based RL framework (SkiMo) that enables planning in the skill space using a skill dynamics model, which directly predicts the skill outcomes, rather than predicting all small details in the intermediate states, step by step. For accurate and efficient long-term planning, we jointly learn the skill dynamics model and a skill repertoire from prior experience. We then harness the learned skill dynamics model to accurately simulate and plan over long horizons in the skill space, which enables efficient downstream learning of long-horizon, sparse reward tasks. Experimental results in navigation and manipulation domains show that SkiMo extends the temporal horizon of model-based approaches and improves the sample efficiency for both model-based RL and skill-based RL. Code and videos are available at https://clvrai.com/skimo},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Shi et al_2022_Skill-based Model-based Reinforcement Learning.pdf;/home/james/Zotero/storage/7L2B5NK5/2207.html}
}

@misc{shyamModelBasedActiveExploration2019,
  title = {Model-{{Based Active Exploration}}},
  author = {Shyam, Pranav and Ja{\'s}kowski, Wojciech and Gomez, Faustino},
  year = {2019},
  month = jun,
  number = {arXiv:1810.12162},
  eprint = {1810.12162},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.12162},
  urldate = {2023-03-04},
  abstract = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Shyam et al_2019_Model-Based Active Exploration.pdf;/home/james/Zotero/storage/8L9MNK6S/1810.html}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  urldate = {2024-11-23},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  copyright = {2016 Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Computer science,Reward}
}

@misc{silverPredictronEndToEndLearning2017,
  title = {The {{Predictron}}: {{End-To-End Learning}} and {{Planning}}},
  shorttitle = {The {{Predictron}}},
  author = {Silver, David and van Hasselt, Hado and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and {Dulac-Arnold}, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
  year = {2017},
  month = jul,
  number = {arXiv:1612.08810},
  eprint = {1612.08810},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.08810},
  urldate = {2024-11-24},
  abstract = {One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/james/Zotero/storage/47Y5Q28Y/Silver et al. - 2017 - The Predictron End-To-End Learning and Planning.pdf;/home/james/Zotero/storage/RXLHAJZ3/1612.html}
}

@techreport{singhIntrinsicallyMotivatedReinforcement2005,
  title = {Intrinsically {{Motivated Reinforcement Learning}}:},
  shorttitle = {Intrinsically {{Motivated Reinforcement Learning}}},
  author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
  year = {2005},
  month = jan,
  address = {Fort Belvoir, VA},
  institution = {Defense Technical Information Center},
  doi = {10.21236/ADA440280},
  urldate = {2023-03-04},
  abstract = {Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.},
  langid = {english},
  file = {/home/james/Documents/zotero/Singh et al_2005_Intrinsically Motivated Reinforcement Learning.pdf}
}

@misc{skorokhodovStyleGANVContinuousVideo2022,
  title = {{{StyleGAN-V}}: {{A Continuous Video Generator}} with the {{Price}}, {{Image Quality}} and {{Perks}} of {{StyleGAN2}}},
  shorttitle = {{{StyleGAN-V}}},
  author = {Skorokhodov, Ivan and Tulyakov, Sergey and Elhoseiny, Mohamed},
  year = {2022},
  month = may,
  number = {arXiv:2112.14683},
  eprint = {2112.14683},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.14683},
  urldate = {2023-03-04},
  abstract = {Videos show continuous events, yet most \$-\$ if not all \$-\$ video synthesis frameworks treat them discretely in time. In this work, we think of videos of what they should be \$-\$ time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator. For this, we first design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demonstrate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image + video discriminators pair and design a holistic discriminator that aggregates temporal information by simply concatenating frames' features. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 1024\${\textasciicircum}2\$ videos for the first time. We build our model on top of StyleGAN2 and it is just \$\{{\textbackslash}approx\}5{\textbackslash}\%\$ more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spatial manipulations that our method can propagate in time. We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a fixed rate. Our model is tested on four modern 256\${\textasciicircum}2\$ and one 1024\${\textasciicircum}2\$-resolution video synthesis benchmarks. In terms of sheer metrics, it performs on average \$\{{\textbackslash}approx\}30{\textbackslash}\%\$ better than the closest runner-up. Project website: https://universome.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Skorokhodov et al_2022_StyleGAN-V.pdf;/home/james/Zotero/storage/M4XQLS8U/2112.html}
}

@misc{smithDontDecayLearning2018,
  title = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  year = {2018},
  month = feb,
  number = {arXiv:1711.00489},
  eprint = {1711.00489},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.00489},
  urldate = {2024-07-06},
  abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \${\textbackslash}epsilon\$ and scaling the batch size \$B {\textbackslash}propto {\textbackslash}epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B {\textbackslash}propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1{\textbackslash}\%\$ validation accuracy in under 30 minutes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/5JSDU4K8/Smith et al. - 2018 - Don't Decay the Learning Rate, Increase the Batch .pdf;/home/james/Zotero/storage/84WA7YAQ/1711.html}
}

@misc{songConsistencyModels2023,
  title = {Consistency {{Models}}},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  year = {2023},
  month = may,
  number = {arXiv:2303.01469},
  eprint = {2303.01469},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-07-04},
  abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,DMs,Image Synthesis,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Song et al_2023_Consistency Models.pdf;/home/james/Zotero/storage/HYJPIDDB/2303.html}
}

@misc{songMASSMaskedSequence2019,
  title = {{{MASS}}: {{Masked Sequence}} to {{Sequence Pre-training}} for {{Language Generation}}},
  shorttitle = {{{MASS}}},
  author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  year = {2019},
  month = jun,
  number = {arXiv:1905.02450},
  eprint = {1905.02450},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.02450},
  urldate = {2023-03-06},
  abstract = {Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Song et al_2019_MASS.pdf;/home/james/Zotero/storage/LZJETBZG/1905.html}
}

@misc{songScoreBasedGenerativeModeling2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = feb,
  number = {arXiv:2011.13456},
  eprint = {2011.13456},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.13456},
  urldate = {2023-03-04},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Song et al_2021_Score-Based Generative Modeling through Stochastic Differential Equations.pdf;/home/james/Zotero/storage/3YLPLFH3/2011.html}
}

@misc{sovianyCurriculumLearningSurvey2022,
  title = {Curriculum {{Learning}}: {{A Survey}}},
  shorttitle = {Curriculum {{Learning}}},
  author = {Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  year = {2022},
  month = apr,
  number = {arXiv:2101.10382},
  eprint = {2101.10382},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.10382},
  urldate = {2023-03-04},
  abstract = {Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Curriculum Learning},
  file = {/home/james/Documents/zotero/Gronauer_Diepold_2022_Multi-agent deep reinforcement learning.pdf;/home/james/Zotero/storage/VTGYHBTP/2101.html}
}

@misc{srinivasCURLContrastiveUnsupervised2020,
  title = {{{CURL}}: {{Contrastive Unsupervised Representations}} for {{Reinforcement Learning}}},
  shorttitle = {{{CURL}}},
  author = {Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
  year = {2020},
  month = sep,
  number = {arXiv:2004.04136},
  eprint = {2004.04136},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.04136},
  urldate = {2023-06-21},
  abstract = {We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://github.com/MishaLaskin/curl.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,RL,Statistics - Machine Learning,Unsupervised RL},
  file = {/home/james/Documents/zotero/Srinivas et al_2020_CURL.pdf;/home/james/Zotero/storage/V2B9BSM7/2004.html}
}

@misc{stanicLearningGeneralizeObjectcentric2022,
  title = {Learning to {{Generalize}} with {{Object-centric Agents}} in the {{Open World Survival Game Crafter}}},
  author = {Stani{\'c}, Aleksandar and Tang, Yujin and Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03374},
  eprint = {2208.03374},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.03374},
  urldate = {2023-06-20},
  abstract = {Reinforcement learning agents must generalize beyond their training experience. Prior work has focused mostly on identical training and evaluation environments. Starting from the recently introduced Crafter benchmark, a 2D open world survival game, we introduce a new set of environments suitable for evaluating some agent's ability to generalize on previously unseen (numbers of) objects and to adapt quickly (meta-learning). In Crafter, the agents are evaluated by the number of unlocked achievements (such as collecting resources) when trained for 1M steps. We show that current agents struggle to generalize, and introduce novel object-centric agents that improve over strong baselines. We also provide critical insights of general interest for future work on Crafter through several experiments. We show that careful hyper-parameter tuning improves the PPO baseline agent by a large margin and that even feedforward agents can unlock almost all achievements by relying on the inventory display. We achieve new state-of-the-art performance on the original Crafter environment. Additionally, when trained beyond 1M steps, our tuned agents can unlock almost all achievements. We show that the recurrent PPO agents improve over feedforward ones, even with the inventory information removed. We introduce CrafterOOD, a set of 15 new environments that evaluate OOD generalization. On CrafterOOD, we show that the current agents fail to generalize, whereas our novel object-centric agents achieve state-of-the-art OOD generalization while also being interpretable. Our code is public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6},
  file = {/home/james/Documents/zotero/Stanić et al_2022_Learning to Generalize with Object-centric Agents in the Open World Survival.pdf;/home/james/Zotero/storage/ZLZWTSFQ/2208.html}
}

@misc{StochasticDifferentialEquations,
  title = {Stochastic {{Differential Equations}} and {{Diffusion Models}} {\textbar} {{VanillaBug}}},
  urldate = {2023-03-04},
  howpublished = {https://www.vanillabug.com/posts/sde/}
}

@misc{stookeDecouplingRepresentationLearning2021,
  title = {Decoupling {{Representation Learning}} from {{Reinforcement Learning}}},
  author = {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  year = {2021},
  month = may,
  number = {arXiv:2009.08319},
  eprint = {2009.08319},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.08319},
  urldate = {2023-03-04},
  abstract = {In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at https://github.com/astooke/rlpyt/tree/master/rlpyt/ul.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Stooke et al_2021_Decoupling Representation Learning from Reinforcement Learning.pdf;/home/james/Zotero/storage/DRIDYHJB/2009.html}
}

@article{suttonLearningPredictMethods1988,
  title = {Learning to Predict by the Methods of Temporal Differences},
  author = {Sutton, Richard S.},
  year = {1988},
  month = aug,
  journal = {Machine Learning},
  volume = {3},
  number = {1},
  pages = {9--44},
  issn = {1573-0565},
  doi = {10.1007/BF00115009},
  urldate = {2024-11-24},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  langid = {english},
  keywords = {Artificial Intelligence,connectionism,credit assignment,evaluation functions,Incremental learning,prediction},
  file = {/home/james/Zotero/storage/SIJVVH67/Sutton - 1988 - Learning to predict by the methods of temporal differences.pdf}
}

@inproceedings{suttonPolicyGradientMethods1999,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  editor = {Solla, S. and Leen, T. and M{\"u}ller, K.},
  year = {1999},
  volume = {12},
  publisher = {MIT Press}
}

@inproceedings{suttonPolicyGradientMethods1999a,
  title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
  year = {1999},
  month = nov,
  series = {{{NIPS}}'99},
  pages = {1057--1063},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  urldate = {2024-11-24},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.}
}

@misc{takidaSQVAEVariationalBayes2022,
  title = {{{SQ-VAE}}: {{Variational Bayes}} on {{Discrete Representation}} with {{Self-annealed Stochastic Quantization}}},
  shorttitle = {{{SQ-VAE}}},
  author = {Takida, Yuhta and Shibuya, Takashi and Liao, WeiHsiang and Lai, Chieh-Hsin and Ohmura, Junki and Uesaka, Toshimitsu and Murata, Naoki and Takahashi, Shusuke and Kumakura, Toshiyuki and Mitsufuji, Yuki},
  year = {2022},
  month = jun,
  number = {arXiv:2205.07547},
  eprint = {2205.07547},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-03},
  abstract = {One noted issue of vector-quantized variational autoencoder (VQ-VAE) is that the learned discrete representation uses only a fraction of the full capacity of the codebook, also known as codebook collapse. We hypothesize that the training scheme of VQ-VAE, which involves some carefully designed heuristics, underlies this issue. In this paper, we propose a new training scheme that extends the standard VAE via novel stochastic dequantization and quantization, called stochastically quantized variational autoencoder (SQ-VAE). In SQ-VAE, we observe a trend that the quantization is stochastic at the initial stage of the training but gradually converges toward a deterministic quantization, which we call self-annealing. Our experiments show that SQ-VAE improves codebook utilization without using common heuristics. Furthermore, we empirically show that SQ-VAE is superior to VAE and VQ-VAE in vision- and speech-related tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/2Y2ZV7U6/Takida et al. - 2022 - SQ-VAE Variational Bayes on Discrete Representati.pdf}
}

@misc{tassaDeepMindControlSuite2018,
  title = {{{DeepMind Control Suite}}},
  author = {Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and Lillicrap, Timothy and Riedmiller, Martin},
  year = {2018},
  month = jan,
  number = {arXiv:1801.00690},
  eprint = {1801.00690},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.00690},
  urldate = {2024-11-24},
  abstract = {The DeepMind Control Suite is a set of continuous control tasks with a standardised structure and interpretable rewards, intended to serve as performance benchmarks for reinforcement learning agents. The tasks are written in Python and powered by the MuJoCo physics engine, making them easy to use and modify. We include benchmarks for several learning algorithms. The Control Suite is publicly available at https://www.github.com/deepmind/dm\_control . A video summary of all tasks is available at http://youtu.be/rAai4QzcYbs .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/Zotero/storage/WCCR5876/Tassa et al. - 2018 - DeepMind Control Suite.pdf;/home/james/Zotero/storage/7WQI6APV/1801.html}
}

@misc{tishbyDeepLearningInformation2015,
  title = {Deep {{Learning}} and the {{Information Bottleneck Principle}}},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  year = {2015},
  month = mar,
  number = {arXiv:1503.02406},
  eprint = {1503.02406},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.02406},
  urldate = {2023-03-04},
  abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Tishby_Zaslavsky_2015_Deep Learning and the Information Bottleneck Principle.pdf;/home/james/Zotero/storage/GT726SEX/1503.html}
}

@misc{tobinDomainRandomizationTransferring2017,
  title = {Domain {{Randomization}} for {{Transferring Deep Neural Networks}} from {{Simulation}} to the {{Real World}}},
  author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  year = {2017},
  month = mar,
  number = {arXiv:1703.06907},
  eprint = {1703.06907},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.06907},
  urldate = {2024-11-24},
  abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to \$1.5\$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Zotero/storage/ZRMJ52LT/Tobin et al. - 2017 - Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.pdf;/home/james/Zotero/storage/6LWYXQFS/1703.html}
}

@misc{tongSimulationfreeSchrodingerBridges2023,
  title = {Simulation-Free {{Schr}}{\textbackslash}"odinger Bridges via Score and Flow Matching},
  author = {Tong, Alexander and Malkin, Nikolay and Fatras, Kilian and Atanackovic, Lazar and Zhang, Yanlei and Huguet, Guillaume and Wolf, Guy and Bengio, Yoshua},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03672},
  eprint = {2307.03672},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-10},
  abstract = {We present simulation-free score and flow matching ([SF]\${\textasciicircum}2\$M), a simulation-free objective for inferring stochastic dynamics given unpaired source and target samples drawn from arbitrary distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]\${\textasciicircum}2\$M interprets continuous-time stochastic generative modeling as a Schr{\textbackslash}"odinger bridge (SB) problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]\${\textasciicircum}2\$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]\${\textasciicircum}2\$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]\${\textasciicircum}2\$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Tong et al_2023_Simulation-free Schr-odinger bridges via score and flow matching.pdf;/home/james/Zotero/storage/ERCDQ85W/2307.html}
}

@misc{touatiDoesZeroShotReinforcement2023,
  title = {Does {{Zero-Shot Reinforcement Learning Exist}}?},
  author = {Touati, Ahmed and Rapin, J{\'e}r{\'e}my and Ollivier, Yann},
  year = {2023},
  month = mar,
  number = {arXiv:2209.14935},
  eprint = {2209.14935},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.14935},
  urldate = {2023-03-16},
  abstract = {A zero-shot RL agent is an agent that can solve any RL task in a given environment, instantly with no additional planning or learning, after an initial reward-free learning phase. This marks a shift from the reward-centric RL paradigm towards "controllable" agents that can follow arbitrary instructions in an environment. Current RL agents can solve families of related tasks at best, or require planning anew for each task. Strategies for approximate zero-shot RL ave been suggested using successor features (SFs) [BBQ+ 18] or forward-backward (FB) representations [TO21], but testing has been limited. After clarifying the relationships between these schemes, we introduce improved losses and new SF models, and test the viability of zero-shot RL schemes systematically on tasks from the Unsupervised RL benchmark [LYL+21]. To disentangle universal representation learning from exploration, we work in an offline setting and repeat the tests on several existing replay buffers. SFs appear to suffer from the choice of the elementary state features. SFs with Laplacian eigenfunctions do well, while SFs based on auto-encoders, inverse curiosity, transition models, low-rank transition matrix, contrastive learning, or diversity (APS), perform unconsistently. In contrast, FB representations jointly learn the elementary and successor features from a single, principled criterion. They perform best and consistently across the board, reaching 85\% of supervised RL performance with a good replay buffer, in a zero-shot manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Touati et al_2023_Does Zero-Shot Reinforcement Learning Exist.pdf;/home/james/Zotero/storage/DZ8YWWQU/2209.html}
}

@misc{touvronTrainingDataefficientImage2021,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = jan,
  number = {arXiv:2012.12877},
  eprint = {2012.12877},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.12877},
  urldate = {2023-03-04},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf;/home/james/Zotero/storage/SHH8GAUY/2012.html}
}

@misc{UpperConfidenceBound2016,
  title = {The {{Upper Confidence Bound Algorithm}}},
  year = {2016},
  month = sep,
  journal = {Bandit Algorithms},
  urldate = {2024-02-02},
  abstract = {We now describe the celebrated Upper Confidence Bound (UCB) algorithm that overcomes all of the limitations of strategies based on exploration followed by commitment, including the need to know the{\dots}},
  langid = {american},
  file = {/home/james/Zotero/storage/GXYC8AJJ/the-upper-confidence-bound-algorithm.html}
}

@misc{vanhasseltWhenUseParametric2019,
  title = {When to Use Parametric Models in Reinforcement Learning?},
  author = {{van Hasselt}, Hado and Hessel, Matteo and Aslanides, John},
  year = {2019},
  month = jun,
  number = {arXiv:1906.05243},
  eprint = {1906.05243},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.05243},
  urldate = {2023-03-04},
  abstract = {We examine the question of when and how parametric models are most useful in reinforcement learning. In particular, we look at commonalities and differences between parametric models and experience replay. Replay-based learning algorithms share important traits with model-based approaches, including the ability to plan: to use more computation without additional data to improve predictions and behaviour. We discuss when to expect benefits from either approach, and interpret prior work in this context. We hypothesise that, under suitable conditions, replay-based algorithms should be competitive to or better than model-based algorithms if the model is used only to generate fictional transitions from observed states for an update rule that is otherwise model-free. We validated this hypothesis on Atari 2600 video games. The replay-based algorithm attained state-of-the-art data efficiency, improving over prior results with parametric models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/van Hasselt et al_2019_When to use parametric models in reinforcement learning.pdf;/home/james/Zotero/storage/LT5GCZ6M/1906.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-03-04},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Transformers},
  file = {/home/james/Documents/zotero/Vaswani et al_2017_Attention Is All You Need.pdf;/home/james/Zotero/storage/MIYXNHJH/1706.html}
}

@misc{veeriahDiscoveryOptionsMetaLearned2021,
  title = {Discovery of {{Options}} via {{Meta-Learned Subgoals}}},
  author = {Veeriah, Vivek and Zahavy, Tom and Hessel, Matteo and Xu, Zhongwen and Oh, Junhyuk and Kemaev, Iurii and {van Hasselt}, Hado and Silver, David and Singh, Satinder},
  year = {2021},
  month = feb,
  number = {arXiv:2102.06741},
  eprint = {2102.06741},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.06741},
  urldate = {2023-03-04},
  abstract = {Temporal abstractions in the form of options have been shown to help reinforcement learning (RL) agents learn faster. However, despite prior work on this topic, the problem of discovering options through interaction with an environment remains a challenge. In this paper, we introduce a novel meta-gradient approach for discovering useful options in multi-task RL environments. Our approach is based on a manager-worker decomposition of the RL agent, in which a manager maximises rewards from the environment by learning a task-dependent policy over both a set of task-independent discovered-options and primitive actions. The option-reward and termination functions that define a subgoal for each option are parameterised as neural networks and trained via meta-gradients to maximise their usefulness. Empirical analysis on gridworld and DeepMind Lab tasks show that: (1) our approach can discover meaningful and diverse temporally-extended options in multi-task RL domains, (2) the discovered options are frequently used by the agent while learning to solve the training tasks, and (3) that the discovered options help a randomly initialised manager learn faster in completely new tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Veeriah et al_2021_Discovery of Options via Meta-Learned Subgoals.pdf;/home/james/Zotero/storage/GR4JJPQP/2102.html}
}

@misc{venugopalBilevelLatentVariable2023,
  title = {Bi-Level {{Latent Variable Model}} for {{Sample-Efficient Multi-Agent Reinforcement Learning}}},
  author = {Venugopal, Aravind and Milani, Stephanie and Fang, Fei and Ravindran, Balaraman},
  year = {2023},
  month = apr,
  number = {arXiv:2304.06011},
  eprint = {2304.06011},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-13},
  abstract = {Despite their potential in real-world applications, multi-agent reinforcement learning (MARL) algorithms often suffer from high sample complexity. To address this issue, we present a novel modelbased MARL algorithm, BiLL (Bi-Level Latent Variable Model-based Learning), that learns a bilevel latent variable model from high-dimensional inputs. At the top level, the model learns latent representations of the global state, which encode global information relevant to behavior learning. At the bottom level, it learns latent representations for each agent, given the global latent representations from the top level. The model generates latent trajectories to use for policy learning. We evaluate our algorithm on complex multi-agent tasks in the challenging SMAC and Flatland environments. Our algorithm outperforms state-ofthe-art model-free and model-based baselines in sample efficiency, including on two extremely challenging Super Hard SMAC maps.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Multi-agent RL,RL},
  file = {/home/james/Documents/zotero/Venugopal et al_2023_Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement.pdf}
}

@misc{vezhnevetsFeUdalNetworksHierarchical2017,
  title = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},
  author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  year = {2017},
  month = mar,
  number = {arXiv:1703.01161},
  eprint = {1703.01161},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.01161},
  urldate = {2023-03-04},
  abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/Documents/zotero/Vezhnevets et al_2017_FeUdal Networks for Hierarchical Reinforcement Learning.pdf;/home/james/Zotero/storage/VFDN3UDV/1703.html}
}

@article{vinyalsGrandmasterLevelStarCraft2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  urldate = {2024-11-24},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Statistics}
}

@misc{wangComprehensiveSurveyContinual2023,
  title = {A {{Comprehensive Survey}} of {{Continual Learning}}: {{Theory}}, {{Method}} and {{Application}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Continual Learning}}},
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  year = {2023},
  month = jun,
  number = {arXiv:2302.00487},
  eprint = {2302.00487},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00487},
  urldate = {2023-07-18},
  abstract = {To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Continual Learning},
  file = {/home/james/Documents/zotero/Wang et al_2023_A Comprehensive Survey of Continual Learning.pdf;/home/james/Zotero/storage/DBHH3CUA/2302.html}
}

@misc{wangDescribeExplainPlan2023,
  title = {Describe, {{Explain}}, {{Plan}} and {{Select}}: {{Interactive Planning}} with {{Large Language Models Enables Open-World Multi-Task Agents}}},
  shorttitle = {Describe, {{Explain}}, {{Plan}} and {{Select}}},
  author = {Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  year = {2023},
  month = feb,
  number = {arXiv:2302.01560},
  eprint = {2302.01560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.01560},
  urldate = {2023-03-04},
  abstract = {In this paper, we study the problem of planning in Minecraft, a popular, democratized yet challenging open-ended environment for developing multi-task embodied agents. We've found two primary challenges of empowering such agents with planning: 1) planning in an open-ended world like Minecraft requires precise and multi-step reasoning due to the long-term nature of the tasks, and 2) as vanilla planners do not consider the proximity to the current agent when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient. To this end, we propose "Describe, Explain, Plan and Select" (DEPS), an interactive planning approach based on Large Language Models (LLMs). Our approach helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goal Selector, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly. Our experiments mark the milestone of the first multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly doubles the overall performances. Finally, the ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the \${\textbackslash}texttt\{ObtainDiamond\}\$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/Documents/zotero/Wang et al_2023_Describe, Explain, Plan and Select.pdf;/home/james/Zotero/storage/7LXGZYPA/2302.html}
}

@misc{wangDuelingNetworkArchitectures2016,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
  year = {2016},
  month = apr,
  number = {arXiv:1511.06581},
  eprint = {1511.06581},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06581},
  urldate = {2024-11-24},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/B4T6P73C/Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforcement Learning.pdf;/home/james/Zotero/storage/BS84858L/1511.html}
}

@inproceedings{wangG3ANDisentanglingAppearance2020,
  title = {{{G3AN}}: {{Disentangling Appearance}} and {{Motion}} for {{Video Generation}}},
  shorttitle = {{{G3AN}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Yaohui and Bilinski, Piotr and Bremond, Francois and Dantcheva, Antitza},
  year = {2020},
  month = jun,
  pages = {5263--5272},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00531},
  urldate = {2023-03-04},
  abstract = {Creating realistic human videos entails the challenge of being able to simultaneously generate both appearance, as well as motion. To tackle this challenge, we introduce G3AN, a novel spatio-temporal generative model, which seeks to capture the distribution of high dimensional video data and to model appearance and motion in disentangled manner. The latter is achieved by decomposing appearance and motion in a three-stream Generator, where the main stream aims to model spatio-temporal consistency, whereas the two auxiliary streams augment the main stream with multi-scale appearance and motion features, respectively. An extensive quantitative and qualitative analysis shows that our model systematically and significantly outperforms state-of-the-art methods on the facial expression datasets MUG and UvA-NEMO, as well as the Weizmann and UCF101 datasets on human action. Additional analysis on the learned latent representations confirms the successful decomposition of appearance and motion. Source code and pre-trained models are publicly available 1.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/home/james/Documents/zotero/Wang et al_2020_G3AN.pdf}
}

@misc{wangLanguageModelingStochastic2022,
  title = {Language Modeling via Stochastic Processes},
  author = {Wang, Rose E. and Durmus, Esin and Goodman, Noah and Hashimoto, Tatsunori},
  year = {2022},
  month = mar,
  number = {arXiv:2203.11370},
  eprint = {2203.11370},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.11370},
  urldate = {2023-03-04},
  abstract = {Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40\% better) and text length consistency (up to +17\% better). Human evaluators also prefer TC's output 28.6\% more than the baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Wang et al_2022_Language modeling via stochastic processes.pdf;/home/james/Zotero/storage/TBIS2LUS/2203.html}
}

@misc{wangPairedOpenEndedTrailblazer2019,
  title = {Paired {{Open-Ended Trailblazer}} ({{POET}}): {{Endlessly Generating Increasingly Complex}} and {{Diverse Learning Environments}} and {{Their Solutions}}},
  shorttitle = {Paired {{Open-Ended Trailblazer}} ({{POET}})},
  author = {Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O.},
  year = {2019},
  month = feb,
  number = {arXiv:1901.01753},
  eprint = {1901.01753},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-13},
  abstract = {While the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. We test POET in a 2-D bipedal-walking obstacle-course domain in which POET can modify the types of challenges and their difficulty. At the same time, a neural network controlling a biped walker is optimized for each environment. The results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/james/Documents/zotero/Wang et al_2019_Paired Open-Ended Trailblazer (POET).pdf}
}

@misc{wengEnvPoolHighlyParallel2022,
  title = {{{EnvPool}}: {{A Highly Parallel Reinforcement Learning Environment Execution Engine}}},
  shorttitle = {{{EnvPool}}},
  author = {Weng, Jiayi and Lin, Min and Huang, Shengyi and Liu, Bo and Makoviichuk, Denys and Makoviychuk, Viktor and Liu, Zichen and Song, Yufan and Luo, Ting and Jiang, Yukun and Xu, Zhongwen and Yan, Shuicheng},
  year = {2022},
  month = oct,
  number = {arXiv:2206.10558},
  eprint = {2206.10558},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.10558},
  urldate = {2024-11-24},
  abstract = {There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the system's overall throughput. In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop and a modest workstation, to a high-end machine such as NVIDIA DGX-A100. On a high-end machine, EnvPool achieves one million frames per second for the environment execution on Atari environments and three million frames per second on MuJoCo environments. When running EnvPool on a laptop, the speed is 2.8x that of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl\_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has great potential to become the de facto RL environment execution engine. Example runs show that it only takes five minutes to train agents to play Atari Pong and MuJoCo Ant on a laptop. EnvPool is open-sourced at https://github.com/sail-sg/envpool.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance,Computer Science - Robotics},
  file = {/home/james/Zotero/storage/HH87KM5I/Weng et al. - 2022 - EnvPool A Highly Parallel Reinforcement Learning Environment Execution Engine.pdf;/home/james/Zotero/storage/4G6HQWQR/2206.html}
}

@misc{wengLilLog,
  title = {Lil'{{Log}}},
  author = {Weng, Lilian},
  urldate = {2023-07-04},
  abstract = {Document my learning notes.},
  howpublished = {https://lilianweng.github.io/},
  langid = {english},
  file = {/home/james/Zotero/storage/U57FPYPT/lilianweng.github.io.html}
}

@misc{wengSelfSupervisedRepresentationLearning2019,
  title = {Self-{{Supervised Representation Learning}}},
  author = {Weng, Lilian},
  year = {2019},
  month = nov,
  urldate = {2023-06-08},
  abstract = {[Updated on 2020-01-09: add a new section on Contrastive Predictive Coding].  [Updated on 2020-04-13: add a ``Momentum Contrast'' section on MoCo, SimCLR and CURL.]  [Updated on 2020-07-08: add a ``Bisimulation'' section on DeepMDP and DBC.]  [Updated on 2020-09-12: add MoCo V2 and BYOL in the ``Momentum Contrast'' section.]  [Updated on 2021-05-31: remove section on ``Momentum Contrast'' and add a pointer to a full post on ``Contrastive Representation Learning'']},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2019-11-10-self-supervised/},
  langid = {english},
  file = {/home/james/Zotero/storage/8HJ5AASV/2019-11-10-self-supervised.html}
}

@misc{wengTransformerFamily2020,
  title = {The {{Transformer Family}}},
  author = {Weng, Lilian},
  year = {2020},
  month = apr,
  urldate = {2023-07-04},
  abstract = {[Updated on 2023-01-27: After almost three years, I did a big refactoring update of this post to incorporate a bunch of new Transformer models since 2020. The enhanced version of this post is here: The Transformer Family Version 2.0. Please refer to that post on this topic.]  It has been almost two years since my last post on attention. Recent progress on new and enhanced versions of Transformer motivates me to write another post on this specific topic, focusing on how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving and more.},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/},
  langid = {english},
  file = {/home/james/Zotero/storage/ILPPZLBI/2020-04-07-the-transformer-family.html}
}

@misc{wengTransformerFamilyVersion2023,
  title = {The {{Transformer Family Version}} 2.0},
  author = {Weng, Lilian},
  year = {2023},
  month = jan,
  urldate = {2023-07-04},
  abstract = {Many new Transformer architecture improvements have been proposed since my last post on ``The Transformer Family'' about three years ago. Here I did a big refactoring and enrichment of that 2020 post --- restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length. Notations    Symbol Meaning     \$d\$ The model size / hidden state dimension / positional encoding size.},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/},
  langid = {english},
  file = {/home/james/Zotero/storage/3K52IV9U/2023-01-27-the-transformer-family-v2.html}
}

@misc{WhatAreDiffusion,
  title = {What Are {{Diffusion Models}}? {\textbar} {{Lil}}'{{Log}}},
  urldate = {2023-03-04},
  howpublished = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}
}

@misc{wilsonCaseBayesianDeep2020,
  title = {The {{Case}} for {{Bayesian Deep Learning}}},
  author = {Wilson, Andrew Gordon},
  year = {2020},
  month = jan,
  number = {arXiv:2001.10995},
  eprint = {2001.10995},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.10995},
  urldate = {2023-06-11},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization instead of optimization, not the prior, or Bayes rule. Bayesian inference is especially compelling for deep neural networks. (1) Neural networks are typically underspecified by the data, and can represent many different but high performing models corresponding to different settings of parameters, which is exactly when marginalization will make the biggest difference for both calibration and accuracy. (2) Deep ensembles have been mistaken as competing approaches to Bayesian methods, but can be seen as approximate Bayesian marginalization. (3) The structure of neural networks gives rise to a structured prior in function space, which reflects the inductive biases of neural networks that help them generalize. (4) The observed correlation between parameters in flat regions of the loss and a diversity of solutions that provide good generalization is further conducive to Bayesian marginalization, as flat regions occupy a large volume in a high dimensional space, and each different solution will make a good contribution to a Bayesian model average. (5) Recent practical advances for Bayesian deep learning provide improvements in accuracy and calibration compared to standard training, while retaining scalability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Wilson_2020_The Case for Bayesian Deep Learning.pdf;/home/james/Zotero/storage/YHA26WUC/2001.html}
}

@misc{wuDayDreamerWorldModels2022,
  title = {{{DayDreamer}}: {{World Models}} for {{Physical Robot Learning}}},
  shorttitle = {{{DayDreamer}}},
  author = {Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Goldberg, Ken and Abbeel, Pieter},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14176},
  eprint = {2206.14176},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.14176},
  urldate = {2023-03-04},
  abstract = {To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place multiple objects directly from camera images and sparse rewards, approaching human performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, establishing a strong baseline. We release our infrastructure for future applications of world models to robot learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Model-based RL,RL,Robotics},
  file = {/home/james/Documents/zotero/Wu et al_2022_DayDreamer.pdf;/home/james/Zotero/storage/2M78XBP5/2206.html}
}

@misc{wuMaskedTrajectoryModels2023,
  title = {Masked {{Trajectory Models}} for {{Prediction}}, {{Representation}}, and {{Control}}},
  author = {Wu, Philipp and Majumdar, Arjun and Stone, Kevin and Lin, Yixin and Mordatch, Igor and Abbeel, Pieter and Rajeswaran, Aravind},
  year = {2023},
  month = may,
  number = {arXiv:2305.02968},
  eprint = {2305.02968},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.02968},
  urldate = {2024-07-08},
  abstract = {We introduce Masked Trajectory Models (MTM) as a generic abstraction for sequential decision making. MTM takes a trajectory, such as a state-action sequence, and aims to reconstruct the trajectory conditioned on random subsets of the same trajectory. By training with a highly randomized masking pattern, MTM learns versatile networks that can take on different roles or capabilities, by simply choosing appropriate masks at inference time. For example, the same MTM network can be used as a forward dynamics model, inverse dynamics model, or even an offline RL agent. Through extensive experiments in several continuous control tasks, we show that the same MTM network -- i.e. same weights -- can match or outperform specialized networks trained for the aforementioned capabilities. Additionally, we find that state representations learned by MTM can significantly accelerate the learning speed of traditional RL algorithms. Finally, in offline RL benchmarks, we find that MTM is competitive with specialized offline RL algorithms, despite MTM being a generic self-supervised learning method without any explicit RL components. Code is available at https://github.com/facebookresearch/mtm},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/349GFS5M/Wu et al. - 2023 - Masked Trajectory Models for Prediction, Represent.pdf;/home/james/Zotero/storage/M9CDXC26/2305.html}
}

@misc{wuSPRINGGPT4Outperforms2023,
  title = {{{SPRING}}: {{GPT-4 Out-performs RL Algorithms}} by {{Studying Papers}} and {{Reasoning}}},
  shorttitle = {{{SPRING}}},
  author = {Wu, Yue and Min, So Yeon and Prabhumoye, Shrimai and Bisk, Yonatan and Salakhutdinov, Ruslan and Azaria, Amos and Mitchell, Tom and Li, Yuanzhi},
  year = {2023},
  month = may,
  number = {arXiv:2305.15486},
  eprint = {2305.15486},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15486},
  urldate = {2023-05-29},
  abstract = {Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context "reasoning" induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Wu et al_2023_SPRING.pdf;/home/james/Zotero/storage/XRX6QE8F/2305.html}
}

@misc{xieLatentSkillPlanning2021,
  title = {Latent {{Skill Planning}} for {{Exploration}} and {{Transfer}}},
  author = {Xie, Kevin and Bharadhwaj, Homanga and Hafner, Danijar and Garg, Animesh and Shkurti, Florian},
  year = {2021},
  month = may,
  number = {arXiv:2011.13897},
  eprint = {2011.13897},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.13897},
  urldate = {2023-03-04},
  abstract = {To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: https://sites.google.com/view/latent-skill-planning/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Documents/zotero/Xie et al_2021_Latent Skill Planning for Exploration and Transfer.pdf;/home/james/Zotero/storage/B2ZR5WEL/2011.html}
}

@book{yampolskiy2018artificial,
  title = {Artificial Intelligence Safety and Security},
  author = {Yampolskiy, R.V.},
  year = {2018},
  series = {Chapman \& {{Hall}}/{{CRC}} Artificial Intelligence and Robotics Series},
  publisher = {CRC Press/Taylor \& Francis Group},
  isbn = {978-0-8153-6982-0},
  lccn = {2018015502}
}

@misc{yangDiffusionModelsComprehensive2022,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  year = {2022},
  month = oct,
  number = {arXiv:2209.00796},
  eprint = {2209.00796},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.00796},
  urldate = {2023-03-04},
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Yang et al_2022_Diffusion Models.pdf;/home/james/Zotero/storage/YN6XKXII/2209.html}
}

@misc{yaratsImprovingSampleEfficiency2020,
  title = {Improving {{Sample Efficiency}} in {{Model-Free Reinforcement Learning}} from {{Images}}},
  author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
  year = {2020},
  month = jul,
  number = {arXiv:1910.01741},
  eprint = {1910.01741},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.01741},
  urldate = {2024-11-24},
  abstract = {Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/Zotero/storage/7BR7M7E7/Yarats et al. - 2020 - Improving Sample Efficiency in Model-Free Reinforcement Learning from Images.pdf;/home/james/Zotero/storage/RDIRVITD/1910.html}
}

@misc{yaratsMasteringVisualContinuous2021,
  title = {Mastering {{Visual Continuous Control}}: {{Improved Data-Augmented Reinforcement Learning}}},
  shorttitle = {Mastering {{Visual Continuous Control}}},
  author = {Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
  year = {2021},
  month = jul,
  number = {arXiv:2107.09645},
  eprint = {2107.09645},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.09645},
  urldate = {2024-07-06},
  abstract = {We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel observations, previously unattained by model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides significantly better computational footprint compared to prior work, with the majority of tasks taking just 8 hours to train on a single GPU. Finally, we publicly release DrQ-v2's implementation to provide RL practitioners with a strong and computationally efficient baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/WUJBSACI/Yarats et al. - 2021 - Mastering Visual Continuous Control Improved Data.pdf;/home/james/Zotero/storage/WXJX786X/2107.html}
}

@misc{yaratsReinforcementLearningPrototypical2021,
  title = {Reinforcement {{Learning}} with {{Prototypical Representations}}},
  author = {Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
  year = {2021},
  month = jul,
  number = {arXiv:2102.11271},
  eprint = {2102.11271},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-30},
  abstract = {Learning effective representations in image-based environments is crucial for sample efficient Reinforcement Learning (RL). Unfortunately, in RL, representation learning is confounded with the exploratory experience of the agent -- learning a useful representation requires diverse data, while effective exploration is only possible with coherent representations. Furthermore, we would like to learn representations that not only generalize across tasks but also accelerate downstream exploration for efficient task-specific training. To address these challenges we propose Proto-RL, a self-supervised framework that ties representation learning with exploration through prototypical representations. These prototypes simultaneously serve as a summarization of the exploratory experience of an agent as well as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information. This enables state-of-the-art downstream policy learning on a set of difficult continuous control tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Yarats et al_2021_Reinforcement Learning with Prototypical Representations.pdf;/home/james/Zotero/storage/5EUGENYF/2102.html}
}

@misc{yeMasteringAtariGames2021,
  title = {Mastering {{Atari Games}} with {{Limited Data}}},
  author = {Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  year = {2021},
  month = dec,
  number = {arXiv:2111.00210},
  eprint = {2111.00210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.00210},
  urldate = {2023-03-04},
  abstract = {Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3\% mean human performance and 109.0\% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/james/Documents/zotero/Ye et al_2021_Mastering Atari Games with Limited Data.pdf;/home/james/Zotero/storage/G2MEEKN4/2111.html}
}

@misc{yuanIntrinsicallyMotivatedReinforcementLearning2022,
  title = {Intrinsically-{{Motivated Reinforcement Learning}}: {{A Brief Introduction}}},
  shorttitle = {Intrinsically-{{Motivated Reinforcement Learning}}},
  author = {Yuan, Mingqi},
  year = {2022},
  month = jun,
  number = {arXiv:2203.02298},
  eprint = {2203.02298},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.02298},
  urldate = {2023-03-04},
  abstract = {Reinforcement learning (RL) is one of the three basic paradigms of machine learning. It has demonstrated impressive performance in many complex tasks like Go and StarCraft, which is increasingly involved in smart manufacturing and autonomous driving. However, RL consistently suffers from the exploration-exploitation dilemma. In this paper, we investigated the problem of improving exploration in RL and introduced the intrinsically-motivated RL. In sharp contrast to the classic exploration strategies, intrinsically-motivated RL utilizes the intrinsic learning motivation to provide sustainable exploration incentives. We carefully classified the existing intrinsic reward methods and analyzed their practical drawbacks. Moreover, we proposed a new intrinsic reward method via R{\textbackslash}'enyi state entropy maximization, which overcomes the drawbacks of the preceding methods and provides powerful exploration incentives. Finally, extensive simulation demonstrated that the proposed module achieve superior performance with higher efficiency and robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Yuan_2022_Intrinsically-Motivated Reinforcement Learning.pdf;/home/james/Zotero/storage/B5759XU5/2203.html}
}

@misc{yuMAGVITMaskedGenerative2022,
  title = {{{MAGVIT}}: {{Masked Generative Video Transformer}}},
  shorttitle = {{{MAGVIT}}},
  author = {Yu, Lijun and Cheng, Yong and Sohn, Kihyuk and Lezama, Jos{\'e} and Zhang, Han and Chang, Huiwen and Hauptmann, Alexander G. and Yang, Ming-Hsuan and Hao, Yuan and Essa, Irfan and Jiang, Lu},
  year = {2022},
  month = dec,
  number = {arXiv:2212.05199},
  eprint = {2212.05199},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.05199},
  urldate = {2023-03-04},
  abstract = {We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle various video synthesis tasks with a single model. We introduce a 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning. We conduct extensive experiments to demonstrate the quality, efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT performs favorably against state-of-the-art approaches and establishes the best-published FVD on three video generation benchmarks, including the challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference time by two orders of magnitude against diffusion models and by 60x against autoregressive models. (iii) A single MAGVIT model supports ten diverse generation tasks and generalizes across videos from different visual domains. The source code and trained models will be released to the public at https://magvit.cs.cmu.edu.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Yu et al_2022_MAGVIT.pdf;/home/james/Zotero/storage/JVVPLQED/2212.html}
}

@misc{zhangColorfulImageColorization2016,
  title = {Colorful {{Image Colorization}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
  year = {2016},
  month = oct,
  number = {arXiv:1603.08511},
  eprint = {1603.08511},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.08511},
  urldate = {2024-11-24},
  abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test," asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32\% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Zotero/storage/UAKXPHDC/Zhang et al. - 2016 - Colorful Image Colorization.pdf;/home/james/Zotero/storage/7WRG6FX3/1603.html}
}

@article{zhangHowFinetuneModel,
  title = {How to {{Fine-tune}} the {{Model}}: {{Unified Model Shift}} and {{Model Bias Policy Optimization}}},
  author = {Zhang, Hai and Zhao, Hang Yu Junqiao and Zhang, Di and Zhou, Chang Huang Hongtu and Ye, Xiao Zhang Chen},
  abstract = {Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward algorithm USB-PO2 (Unified model Shift and model Bias Policy Optimization). Empirical results show that USB-PO achieves state-of-the-art performance on several challenging benchmark tasks.},
  langid = {english},
  file = {/home/james/Zotero/storage/LSKPWZBN/Zhang et al. - How to Fine-tune the Model Uniﬁed Model Shift and Model Bias Policy Optimization.pdf}
}

@misc{zhangSmoothVideoComposition2022,
  title = {Towards {{Smooth Video Composition}}},
  author = {Zhang, Qihang and Yang, Ceyuan and Shen, Yujun and Xu, Yinghao and Zhou, Bolei},
  year = {2022},
  month = dec,
  number = {arXiv:2212.07413},
  eprint = {2212.07413},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.07413},
  urldate = {2023-03-04},
  abstract = {Video generation requires synthesizing consistent and persistent frames with dynamic content over time. This work investigates modeling the temporal relations for composing video with arbitrary length, from a few frames to even infinite, using generative adversarial networks (GANs). First, towards composing adjacent frames, we show that the alias-free operation for single image generation, together with adequately pre-learned knowledge, brings a smooth frame transition without compromising the per-frame quality. Second, by incorporating the temporal shift module (TSM), originally designed for video understanding, into the discriminator, we manage to advance the generator in synthesizing more consistent dynamics. Third, we develop a novel B-Spline based motion representation to ensure temporal smoothness to achieve infinite-length video generation. It can go beyond the frame number used in training. A low-rank temporal modulation is also proposed to alleviate repeating contents for long video generation. We evaluate our approach on various datasets and show substantial improvements over video generation baselines. Code and models will be publicly available at https://genforce.github.io/StyleSV.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/james/Documents/zotero/Zhang et al_2022_Towards Smooth Video Composition.pdf;/home/james/Zotero/storage/IH2UAJ7F/2212.html}
}

@misc{zhangSTORMEfficientStochastic2023,
  title = {{{STORM}}: {{Efficient Stochastic Transformer}} Based {{World Models}} for {{Reinforcement Learning}}},
  shorttitle = {{{STORM}}},
  author = {Zhang, Weipu and Wang, Gang and Sun, Jian and Yuan, Yetian and Huang, Gao},
  year = {2023},
  month = oct,
  number = {arXiv:2310.09615},
  eprint = {2310.09615},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.09615},
  urldate = {2024-07-06},
  abstract = {Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial. In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of \$126.7{\textbackslash}\%\$ on the Atari \$100\$k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with \$1.85\$ hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only \$4.3\$ hours, showcasing improved efficiency compared to previous methodologies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/6PG9VAR6/Zhang et al. - 2023 - STORM Efficient Stochastic Transformer based Worl.pdf;/home/james/Zotero/storage/QAWNGGP3/2310.html}
}

@misc{zhaoSurveyLargeLanguage2023,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = jun,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-10},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LMs},
  file = {/home/james/Documents/zotero/Zhao et al_2023_A Survey of Large Language Models.pdf;/home/james/Zotero/storage/SK5RUTKC/2303.html}
}

@misc{zhengModelEnsembleNecessary2023,
  title = {Is {{Model Ensemble Necessary}}? {{Model-based RL}} via a {{Single Model}} with {{Lipschitz Regularized Value Function}}},
  shorttitle = {Is {{Model Ensemble Necessary}}?},
  author = {Zheng, Ruijie and Wang, Xiyao and Xu, Huazhe and Huang, Furong},
  year = {2023},
  month = feb,
  number = {arXiv:2302.01244},
  eprint = {2302.01244},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.01244},
  urldate = {2024-07-06},
  abstract = {Probabilistic dynamics model ensemble is widely used in existing model-based reinforcement learning methods as it outperforms a single dynamics model in both asymptotic performance and sample efficiency. In this paper, we provide both practical and theoretical insights on the empirical success of the probabilistic dynamics model ensemble through the lens of Lipschitz continuity. We find that, for a value function, the stronger the Lipschitz condition is, the smaller the gap between the true dynamics- and learned dynamics-induced Bellman operators is, thus enabling the converged value function to be closer to the optimal value function. Hence, we hypothesize that the key functionality of the probabilistic dynamics model ensemble is to regularize the Lipschitz condition of the value function using generated samples. To test this hypothesis, we devise two practical robust training mechanisms through computing the adversarial noise and regularizing the value network's spectral norm to directly regularize the Lipschitz condition of the value functions. Empirical results show that combined with our mechanisms, model-based RL algorithms with a single dynamics model outperform those with an ensemble of probabilistic dynamics models. These findings not only support the theoretical insight, but also provide a practical solution for developing computationally efficient model-based RL algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/Zotero/storage/K2UTWD6X/Zheng et al. - 2023 - Is Model Ensemble Necessary Model-based RL via a .pdf;/home/james/Zotero/storage/T3Z3CRI7/2302.html}
}

@misc{zhengOnlineDecisionTransformer2022,
  title = {Online {{Decision Transformer}}},
  author = {Zheng, Qinqing and Zhang, Amy and Grover, Aditya},
  year = {2022},
  month = jul,
  number = {arXiv:2202.05607},
  eprint = {2202.05607},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.05607},
  urldate = {2023-03-04},
  abstract = {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/Documents/zotero/Zheng et al_2022_Online Decision Transformer.pdf;/home/james/Zotero/storage/2M4QCQMX/2202.html}
}
