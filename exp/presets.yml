thesis:
  base:
    $extends: [dreamer.atari, atari.base]
    # For all experiments (save the sanity check) we reserve 15% of the episodes for validation purposes.
    data.val_frac: 0.15

  sanity_check:
    # Check if the reference scores for base DreamerV2 are correct
    $extends: [base]
    data.val_frac: 0.0
    stages:
      - prefill:
          until: 200e3
      - train_loop:
          until: 6e6
          tasks:
            - do_val_epoch: ~
              every: 250e3
            - do_opt_step: ~
              every:
                n: 64
                accumulate: true
            - do_env_step
      - do_val_epoch

  no_sticky:
    # Check what the scores are (at 6m horizon) with Atari-100k env setup (i.e. without sticky actions.)
    $extends: [base, atari.v4]
    data.val_frac: 0.0
    stages:
      - prefill:
          until: 200e3
      - train_loop:
          until: 6e6
          tasks:
            - do_val_epoch: ~
              every: 250e3
            - do_opt_step: ~
              every:
                n: 64
                accumulate: true
            - do_env_step
      - do_val_epoch

  baseline:
    # A baseline test - run the loop for 400k steps, with variable update freq
    $extends: [base]
    _ratio: 64 # Ratio of env steps to model/agent opt steps
    _prefill: 20e3
    stages:
      - prefill:
          until: ${_prefill}
      - train_loop:
          until: 400e3
          tasks:
            - do_val_epoch: ~
              every: 20e3
            - do_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_opt_step: ~
              every:
                n: ${_ratio}
                accumulate: true
            - do_env_step
      - do_val_epoch
      - save_ckpt:
          full: false
          tag: final

  split_ratios:
    # Like the baseline test, but the model and agent update freqs are different
    $extends: [base]
    _wm_ratio: 64
    _rl_ratio: 64
    _prefill: 20e3
    stages:
      - prefill:
          until: ${_prefill}
      - train_loop:
          until: 400e3
          tasks:
            - do_val_epoch: ~
              every: 20e3
            - do_wm_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_rl_val_step: ~
              every: { n: 16, of: rl_opt_step }
            - do_wm_opt_step: ~
              every:
                n: ${_wm_ratio}
                accumulate: true
            - do_rl_opt_step: ~
              every:
                n: ${_rl_ratio}
                accumulate: true
            - do_env_step
      - do_val_epoch
      - save_ckpt:
          full: false
          tag: final

  pretrain:
    # Perform offline pretraining stage after the prefill stage
    $extends: [base]
    _wm_ratio: 64
    _rl_ratio: 64
    _prefill: 20e3
    _rl_freq: 0.0
    stages:
      - prefill:
          until: ${_prefill}
      - pretrain:
          stop_criteria:
            rel_patience: 0.5
            margin: 0.0
            min_steps: 1024
          val_on_loss_improv: 0.2
          val_every: 1024
          max_val_batches: 128
          rl_opt_freq: ${_rl_freq}
      - train_loop:
          until: 400e3
          tasks:
            - do_val_epoch: ~
              every: 20e3
            - do_wm_val_step: ~
              every: { n: 16, of: wm_opt_step }
            - do_rl_val_step: ~
              every: { n: 16, of: rl_opt_step }
            - do_wm_opt_step: ~
              every:
                n: ${_wm_ratio}
                accumulate: true
            - do_rl_opt_step: ~
              every:
                n: ${_rl_ratio}
                accumulate: true
            - do_env_step
      - do_val_epoch
      - save_ckpt:
          full: false
          tag: final

  adaptive_ratio:
    base:
      # A setup, where an adaptive model/RL module update frequencies is used
      $extends: [thesis.base]
      _prefill: 20e3
      _rl_ratio: 0.0
      _update_period: 2e3
      _adaptive_setup: {}
      stages:
        - prefill:
            until: ${_prefill}
        - adaptive_setup: ${_adaptive_setup}
        - train_loop:
            until: 400e3
            tasks:
              - do_val_epoch: ~
                every: 20e3
              - update_adaptive_opt: ~
                every: ${_update_period}
              - do_adaptive_opt_step
              - do_rl_opt_step: ~
                every:
                  n: ${_rl_ratio}
                  accumulate: true
              - do_env_step
        - do_val_epoch
        - save_ckpt:
            full: false
            tag: final

    v1_0:
      $extends: [base]
      _rl_ratio: 0.0
      _v1_variant: "0"
      _adaptive_setup:
        rl_to_wm_ratio: 1.0
        type: v1
        v1:
          variant: ${_v1_variant}
          ratio_range: [1, 128]
          update_mult: 1.3
          initial_value: 64

    wm_v1_0:
      $extends: [v1_0]
      _rl_ratio: 4.0
      _adaptive_setup:
        rl_to_wm_ratio: 0.0

  warm_start:
    # Like `sanity_check`, except that we load the parameters from another run to check if it impacts the performance. We use the same base env ID, but "randomize" it (swap channels, flip axes, permute actions etc.) to simulate continual setting.
    $extends: [base]
    _ckpt_path: ~
    env:
      type: atari
      atari.randomize: true
    data.val_frac: 0.0
    stages:
      - load_ckpt:
          path: "${_ckpt_path}"
      - prefill:
          until: 200e3
      - train_loop:
          until: 6e6
          tasks:
            - do_val_epoch: ~
              every: 250e3
            - do_opt_step: ~
              every:
                n: 64
                accumulate: true
            - do_env_step
      - do_val_epoch

  data_aug:
    v1:
      $extends: [adaptive_ratio.wm_v1]
      _rl_ratio: 4.0
      data.loaders.dreamer_wm:
        augment:
          type: drq
          drq:
            max_shift: 4

    v2:
      $extends: [baseline]
      data.loaders.dreamer_wm:
        augment:
          type: drq
          drq:
            max_shift: 4

    smaller_shift:
      $extends: [baseline]
      data.loaders.dreamer_wm:
        augment:
          type: drq
          drq:
            max_shift: 2

    sweep:
      $extends: [baseline]
      _drq_config: {}
      data.loaders.dreamer_wm:
        augment:
          type: drq
          drq: ${_drq_config}

  warm_start_actor:
    $extends: [base]
    _ckpt_path: ~
    env:
      type: atari
      atari.randomize: true
    data.val_frac: 0.0
    stages:
      - load_ckpt:
          path: "${_ckpt_path}"
          only: [actor]
      - prefill:
          until: 200e3
      - train_loop:
          until: 6e6
          tasks:
            - do_val_epoch: ~
              every: 250e3
            - do_opt_step: ~
              every:
                n: 64
                accumulate: true
            - do_env_step
      - do_val_epoch

  sanity_check_rand:
    $extends: [sanity_check]
    env:
      type: atari
      atari.randomize: true

  sac:
    base:
      $extends: [baseline]
      data.loaders.dreamer_rl:
        keep_first_reward: false
      rl:
        loader: dreamer_rl
        type: sac
        sac:
          num_qf: 2
          gamma: 0.999
          gae_lambda: 0.95
          alpha:
            adaptive: false
            value: 1e-3
          clip_grad: 1e2
          actor:
            opt:
              type: adam_w
              lr: 4e-5
              weight_decay: "${1e-6/lr}"
              eps: 1e-5
            encoder:
              type: dreamer_box
              dreamer_box: ${_.mlp}
            dist:
              type: auto
              box:
                min_std: 0.1
                init: tf
              one_hot:
                init: tf
          qf:
            opt:
              type: adam_w
              lr: 1e-4
              weight_decay: "${1e-6/lr}"
              eps: 1e-5
            polyak:
              every: 100
              tau: sync
            encoder:
              type: dreamer_box
              dreamer_box: ${_.mlp}

    alpha_search:
      $extends: [base]
      _alpha: 1e-3
      rl.sac.alpha:
        adaptive: false
        value: ${_alpha}

    adaptive_ent:
      $extends: [base]
      _target: ~
      _mode: ~
      rl.sac.alpha:
        adaptive: true
        value: 1.0
        target: ${_target}
        mode: ${_mode}
        opt:
          type: adam
          lr: 2.5e-3
          eps: 1e-5

    ent_sched:
      $extends: [adaptive_ent]
      _target:
        value: linear((0, 0.75), (400e3, 1e-2))
        of: env_step
      _mode: eps

    fixed_ent:
      $extends: [adaptive_ent]
      _target: 0.1
      _mode: eps

    ent_sched_exp:
      $extends: [adaptive_ent]
      _target:
        value: exp((20e3, 0.99), (400e3, 2.5e-2), 25e3)
        of: env_step
      _mode: rel

    no_min_q:
      $extends: [ent_sched_exp]
      rl.sac.num_qf: 1

    ent_sched2:
      $extends: [adaptive_ent]
      _target:
        value: linear((20e3, 0.99), (50e3, 0.1), (200e3, 5e-2))
        of: env_step
      _mode: rel

  ent_sched:
    $extends: [baseline]
    _ratio: 4
    rl.a2c.alpha:
      adaptive: true
      value: 1.0
      target:
        value: exp((20e3, 0.99), (400e3, 2.5e-2), 25e3)
        of: env_step
      mode: rel
      opt:
        type: adam
        lr: 2.5e-3
        eps: 1e-5

  ppo:
    base:
      $extends: [split_ratios]
      _wm_ratio: 4
      _rl_ratio: 8
      data.loaders.dreamer_rl:
        keep_first_reward: false
      rl:
        type: ppo
        loader: dreamer_rl
        ppo:
          actor: ${a2c.actor}
          critic: ${a2c.critic}
          gamma: ${a2c.gamma}
          gae_lambda: ${a2c.gae_lambda}
          alpha: ${a2c.alpha}
          target_critic: ${a2c.target_critic}
          update_epochs: 4
          num_minibatches: 1
          adv_norm: true
          clip_vloss: true
          vf_coef: 0.5
          clip_grad: 0.5
          rew_fn: id
          clip_coef: 0.1

  final:
    prelim:
      $extends: [ppo.base, adaptive_ratio.wm_v1_0]
      _rl_ratio: 4
      _update_epochs: 8
      rl:
        type: ppo
        loader: dreamer_rl
        ppo:
          update_epochs: ${_update_epochs}
          num_minibatches: 1

    benchmark:
      $extends: [prelim]
      _rl_ratio: 8
      _update_epochs: 8
      env:
        type: atari
        atari:
          repeat_action_probability: 0.0

    dreamerv2:
      $extends: [dreamer.atari, atari.base]
      env:
        type: atari
        atari:
          repeat_action_probability: 0.0
      stages:
        - prefill:
            until: 20e3
        - train_loop:
            until: 400e3
            tasks:
              - do_val_epoch: ~
                every: 20e3
              - do_opt_step: ~
                every:
                  n: 64
                  accumulate: true
              - do_env_step
        - do_val_epoch
        - save_ckpt:
            full: false
            tag: final

  mpc:
    base:
      $extends: [atari.base]
      _ratio: 4
      wm:
        type: dreamer
        loader: dreamer_wm
      rl:
        type: cem
        loader: ~
        cem:
          lookahead: 10
          num_samples: 128
          num_elites: 16
          num_iters: 8
      stages:
        - prefill:
            until: 20e3
        - train_loop:
            until: 400e3
            tasks:
              - do_wm_opt_step: ~
                every: ${_ratio}
              - do_wm_val_step: ~
                every: { n: 16, of: wm_opt_step }
              - do_env_step
        - do_val_epoch
        - save_ckpt:
            full: false
            tag: final
