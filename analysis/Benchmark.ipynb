{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df, scalars = final_benchmark_loader()\n",
    "ref_df = reference_loader()\n",
    "paper_scores, paper_stats = papers_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = res_df.groupby(\"env\")\n",
    "scores = g[\"score\"].mean().reset_index()\n",
    "scores = scores.rename(columns={\"env\": \"Environment\", \"score\": \"Ours\"})\n",
    "scores = paper_scores.merge(scores, on=\"Environment\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res_df.merge(ref_df, left_on=\"env\", right_on=\"task\")\n",
    "df[\"norm\"] = (df[\"score\"] - df[\"random\"]) / (df[\"human_gamer\"] - df[\"random\"])\n",
    "ours_df = pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\"Statistic\": \"Median\", \"Ours\": df[\"norm\"].median()},\n",
    "        {\"Statistic\": \"Mean\", \"Ours\": df[\"norm\"].mean()},\n",
    "    ]\n",
    ")\n",
    "stats = paper_stats.merge(ours_df, on=\"Statistic\")\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = [\n",
    "    \"Random\",\n",
    "    \"Human\",\n",
    "    \"SimPLe\",\n",
    "    \"TWM\",\n",
    "    \"IRIS\",\n",
    "    \"DreamerV3\",\n",
    "    \"SR-SPR\",\n",
    "    \"EfficientZero\",\n",
    "    \"BBF\",\n",
    "    \"DreamerV2\",\n",
    "    \"Ours\",\n",
    "]\n",
    "\n",
    "sel_scores = scores[[\"Environment\", *selection]]\n",
    "sel_scores = sel_scores.rename(columns={\"EfficientZero\": \"EffZero\"})\n",
    "print(sel_scores.to_latex(index=False, float_format=\"%.0f\"))\n",
    "\n",
    "sel_stats = stats[[\"Statistic\", *selection]]\n",
    "sel_stats = sel_stats.rename(columns={\"EfficientZero\": \"EffZero\"})\n",
    "print(sel_stats.to_latex(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for _, row in res_df.iterrows():\n",
    "    task_df = scalars.read(row[\"path\"])\n",
    "    val_scores = task_df[task_df[\"tag\"] == \"val/mean_ep_ret\"]\n",
    "    for _, row2 in val_scores.iterrows():\n",
    "        records.append(\n",
    "            {\n",
    "                \"task\": row[\"env\"],\n",
    "                \"seed\": row[\"seed\"],\n",
    "                \"time\": row2[\"step\"],\n",
    "                \"score\": row2[\"value\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "val_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"time2\"] = int(20e3) * (val_df[\"time\"] // int(20e3))\n",
    "g = val_df.groupby([\"task\", \"time2\"])\n",
    "avg_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"score_mean\": g[\"score\"].mean(),\n",
    "        \"score_std\": g[\"score\"].std(),\n",
    "    }\n",
    ").reset_index()\n",
    "avg_df = avg_df.rename(columns={\"time2\": \"time\"})\n",
    "avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 4\n",
    "tasks = sorted(res_df[\"env\"].unique())\n",
    "rows = (len(tasks) + cols - 1) // cols\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=rows,\n",
    "    cols=cols,\n",
    "    subplot_titles=[*tasks],\n",
    "    vertical_spacing=0.05,\n",
    ")\n",
    "pos = np.stack(np.mgrid[:rows, :cols], -1).reshape(-1, 2) + 1\n",
    "\n",
    "color = next(make_color_iter())\n",
    "axis = 1\n",
    "for (row, col), task in zip(pos, tasks):\n",
    "    task_df = avg_df[avg_df[\"task\"] == task]\n",
    "\n",
    "    x, y = task_df[\"time\"], task_df[\"score_mean\"]\n",
    "    y_lower = task_df[\"score_mean\"] - task_df[\"score_std\"]\n",
    "    y_upper = task_df[\"score_mean\"] + task_df[\"score_std\"]\n",
    "\n",
    "    traces = [\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=color),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=[*x, *x[::-1]],\n",
    "            y=[*y_upper, *y_lower[::-1]],\n",
    "            fill=\"tozerox\",\n",
    "            fillcolor=to_rgba(color),\n",
    "            line=dict(color=\"rgba(255, 255, 255, 0)\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    ]\n",
    "    for trace in traces:\n",
    "        fig.add_trace(trace, row=row, col=col)\n",
    "\n",
    "    if col == 1:\n",
    "        fig.update_layout(**{f\"yaxis{axis}\": dict(title=\"Score\")})\n",
    "    axis += 1\n",
    "\n",
    "\n",
    "fig.update_layout(width=800, height=1100)\n",
    "\n",
    "fig.write_image(\"../tex/assets/atari_100k.curves.pdf\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df2 = res_df.copy()\n",
    "score2 = []\n",
    "for idx, row in res_df2.iterrows():\n",
    "    df = scalars.read(row[\"path\"])\n",
    "    train_ep_ret = df[df[\"tag\"] == \"train/ep_ret\"]\n",
    "    best_step = train_ep_ret[\"step\"].iloc[train_ep_ret[\"value\"].argmax()]\n",
    "    val_scores = df[df[\"tag\"] == \"val/mean_ep_ret\"]\n",
    "    score2.append(\n",
    "        np.interp(\n",
    "            best_step, val_scores[\"step\"].to_numpy(), val_scores[\"value\"].to_numpy()\n",
    "        )\n",
    "    )\n",
    "res_df2[\"score2\"] = score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res_df2.merge(ref_df, left_on=\"env\", right_on=\"task\")\n",
    "df[\"norm\"] = (df[\"score2\"] - df[\"random\"]) / (df[\"human_gamer\"] - df[\"random\"])\n",
    "ours_df = pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\"Statistic\": \"Median\", \"Ours\": df[\"norm\"].median()},\n",
    "        {\"Statistic\": \"Mean\", \"Ours\": df[\"norm\"].mean()},\n",
    "    ]\n",
    ")\n",
    "ours_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsrch-8qRRKNqI-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
