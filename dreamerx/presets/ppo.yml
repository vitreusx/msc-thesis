ppo:
  base:
    $extends: [no_model]
    rl:
      type: ppo
      loader: on_policy
      ppo:
        actor:
          encoder:
            type: ppo
        critic:
          encoder: ~
          dist:
            type: mse
            mse: { init: cleanrl_ppo }
          opt: ${actor.opt}
        adv_norm: true
        clip_vloss: true
        vf_coef: 0.25
        clip_grad: 0.5
        gamma: 0.99
        gae_lambda: 0.95
        alpha:
          adaptive: false
        rew_fn: sign
        target_critic: ~
    data.loaders.on_policy:
      min_seq_len: 16
  mujoco:
    $extends: [base]
    env:
      type: gym
      gym.env_id: Humanoid-v4
    rl.ppo:
      update_epochs: 10
      num_minibatches: 32
      clip_coef: 0.2
      alpha:
        value: 0.0
      actor:
        dist:
          type: trunc_normal
          trunc_normal:
            init: cleanrl_ppo
        opt:
          type: adam
          lr: 3e-4
    train.num_envs: 1
    data.loaders.on_policy:
      steps_per_batch: 2048
    stages:
      - train_loop:
          until: 1e6
          tasks: [do_rl_opt_step]
  atari:
    $extends: [base]
    env:
      type: atari
      atari:
        env_id: Pong
        screen_size: 84
        stack_num: 4
        term_on_life_loss: true
    rl.ppo:
      update_epochs: 4
      num_minibatches: 4
      actor:
        dist:
          type: cat
          cat:
            init: cleanrl_ppo
        opt:
          type: adam
          lr: 2.5e-4
          eps: 1e-5
      clip_coef: 0.1
      alpha.value: 1e-2
    train.num_envs: 8
    data.loaders.on_policy:
      steps_per_batch: 1024
    stages:
      - train_loop:
          until: 40e6
          tasks: [do_rl_opt_step]
